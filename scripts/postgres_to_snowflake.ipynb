{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dac2340-210c-4df7-beaf-3dcf375b779e",
   "metadata": {},
   "source": [
    "## Postgres to Snowflake Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e8b9b6-718e-4103-8072-2935f2c7f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone, timedelta, date\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from psycopg2.extensions import connection as Psycopg2Connection\n",
    "from psycopg2 import sql\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import subprocess\n",
    "import shlex\n",
    "from dateutil import parser\n",
    "from snowflake.ingest import SimpleIngestManager, StagedFile\n",
    "import json\n",
    "import uuid\n",
    "import socket\n",
    "import requests\n",
    "from cryptography.hazmat.primitives import serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5aa29-a707-4ce8-b78e-f263219f6d48",
   "metadata": {},
   "source": [
    "## 1) Config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b55f7-e86f-4282-b8f9-aece8fdebc46",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5876705-caa6-44b5-abf4-12c35b63add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            return config or {}\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML in config file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03388238-9938-4cec-9896-10579339b808",
   "metadata": {},
   "source": [
    "### Validate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b687fc-21cf-4a34-a860-7a5c1de3acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config(cfg: Dict[str, Any]) -> None:\n",
    "    # Top-level keys\n",
    "    required_top_keys = {\"version\", \"source\", \"export\", \"tables\"}\n",
    "    missing = required_top_keys - cfg.keys()\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing top-level keys: {', '.join(sorted(missing))}\")\n",
    "\n",
    "    # Validate source\n",
    "    if not isinstance(cfg[\"source\"], dict) or not cfg[\"source\"]:\n",
    "        raise ValueError(\"'source' must be a non-empty dictionary\")\n",
    "\n",
    "    for source_name, conn in cfg[\"source\"].items():\n",
    "        if not isinstance(conn, dict):\n",
    "            raise TypeError(f\"Source '{source_name}' must be a dictionary\")\n",
    "        for key in (\"host\", \"port\", \"database\", \"user\", \"password_env\", \"schema\"):\n",
    "            if key not in conn:\n",
    "                raise KeyError(f\"Missing '{key}' in source '{source_name}'\")\n",
    "                \n",
    "    # Validate target\n",
    "    if not isinstance(cfg[\"target\"], dict) or not cfg[\"target\"]:\n",
    "        raise ValueError(\"'target' must be a non-empty dictionary\")\n",
    "\n",
    "    for target_name, conn in cfg[\"target\"].items():\n",
    "        if not isinstance(conn, dict):\n",
    "            raise TypeError(f\"Target '{target_name}' must be a dictionary\")\n",
    "        for key in (\"account\", \"user\", \"password_env\", \"role\", \"database\", \"schema\"):\n",
    "            if key not in conn:\n",
    "                raise KeyError(f\"Missing '{key}' in target '{target_name}'\")\n",
    "\n",
    "    # Validate export\n",
    "    export = cfg[\"export\"]\n",
    "    if not isinstance(export, dict):\n",
    "        raise TypeError(\"'export' must be a dictionary\")\n",
    "    for key in (\"output_dir\", \"format\"):\n",
    "        if key not in export:\n",
    "            raise KeyError(f\"Missing '{key}' in 'export'\")\n",
    "\n",
    "    # Validate tables\n",
    "    tables = cfg[\"tables\"]\n",
    "    if not isinstance(tables, dict) or not tables:\n",
    "        raise ValueError(\"'tables' must be a non-empty list\")\n",
    "    for i, table in enumerate(tables):\n",
    "        t=tables[table][0]\n",
    "        if not isinstance(t, dict):\n",
    "            raise TypeError(f\"Table at index {i} must be a dictionary\")\n",
    "        if \"name\" not in t or \"mode\" not in t or \"order_by\" not in t:\n",
    "            raise KeyError(f\"Table {t} missing required keys\")\n",
    "        if t[\"mode\"] == \"table\" and \"table\" not in t:\n",
    "            raise KeyError(f\"Table {t['name']} missing 'table' key for mode 'table'\")\n",
    "        if t[\"mode\"] == \"query\" and \"query\" not in t:\n",
    "            raise KeyError(f\"Table {t['name']} missing 'query' key for mode 'query'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c259d8b-84b7-4064-875c-5655e2c9be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_yaml_file(path: str) -> Dict[str, Any]:\n",
    "    # Load YAML\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"YAML file not found: {path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML syntax: {e}\")\n",
    "\n",
    "    if not isinstance(cfg, dict):\n",
    "        raise ValueError(\"YAML root must be a dictionary\")\n",
    "\n",
    "    validate_config(cfg)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fdd40-b424-4217-80d1-93c2da378179",
   "metadata": {},
   "source": [
    "## 2) Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feab731-5144-411b-8418-98e13b64e300",
   "metadata": {},
   "source": [
    "### Create postgres database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4fd0b7-6ce3-4748-840f-4d8391271ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PostgresCreds:\n",
    "    host: str\n",
    "    port: str\n",
    "    dbname: str\n",
    "    user: str\n",
    "    password: str # use password_env in YAML (recommended)\n",
    "    schema: Optional[str]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4220613f-0772-472a-b382-aca8ea865a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pg_connection(creds: PostgresCreds) -> Psycopg2Connection:\n",
    "    \"\"\"\n",
    "    Create a PostgreSQL connection using psycopg2.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=creds.host,\n",
    "        port=creds.port,\n",
    "        dbname=creds.dbname,\n",
    "        user=creds.user,\n",
    "        password=creds.password,\n",
    "    )\n",
    "\n",
    "    # Optionally set search_path if schema is provided\n",
    "    if creds.schema:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                sql.SQL(\"SET search_path TO {}\").format(\n",
    "                    sql.Identifier(creds.schema)\n",
    "                )\n",
    "            )\n",
    "            conn.commit()\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17550cc-0bb0-4283-b462-9d441fd03851",
   "metadata": {},
   "source": [
    "### Create Snowflake connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b540cf-37cc-4c7f-a339-6a1e98ccdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SnowflakeCreds:\n",
    "    account: str\n",
    "    user: str\n",
    "    role: Optional[str]\n",
    "    warehouse: Optional[str]\n",
    "    database: Optional[str]\n",
    "    schema: Optional[str]\n",
    "    password: Optional[str]\n",
    "    sf_landing_stage: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b66c96-bae9-4389-b723-db531ad9bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _snowsql_base_cmd(creds: SnowflakeCreds, snowsql_path: str = \"snowsql\") -> list[str]:\n",
    "    cmd = [\n",
    "        snowsql_path,\n",
    "        \"-a\", creds.account,\n",
    "        \"-u\", creds.user,\n",
    "        \"-o\", \"exit_on_error=true\",\n",
    "        \"-o\", \"friendly=false\",\n",
    "        \"-o\", \"quiet=false\",\n",
    "    ]\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0a0d02-672c-4658-879b-a1f2a647b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_snowsql(creds: SnowflakeCreds, sql: str, *, snowsql_path: str = \"snowsql\", timeout_sec: int = 300) -> str:\n",
    "    env = os.environ.copy()\n",
    "    if creds.password:\n",
    "        env[\"SNOWSQL_PWD\"] = creds.password\n",
    "\n",
    "    cmd = _snowsql_base_cmd(creds, snowsql_path=snowsql_path) + [\"-q\", sql]\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        cmd,\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=timeout_sec,\n",
    "    )\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            \"SnowSQL command failed.\\n\"\n",
    "            f\"Command: {shlex.join(cmd)}\\n\"\n",
    "            f\"STDOUT:\\n{proc.stdout}\\n\"\n",
    "            f\"STDERR:\\n{proc.stderr}\"\n",
    "        )\n",
    "    return proc.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a367bc9f-c646-4764-9002-02f6e675c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_network_for_snowflake(\n",
    "    proxy_url: str = \"http://proxy.mycorp.com:8080\",\n",
    "    snowflake_host: str = \"account_name\",\n",
    "    timeout_s: int = 8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Makes Snowflake + stage (S3) uploads work without manual toggling.\n",
    "\n",
    "    Strategy:\n",
    "    - If proxy hostname resolves (likely on VPN/corp network):\n",
    "        Use proxy for general internet (so S3 works),\n",
    "        but bypass proxy for *.snowflakecomputing.com (so Snowflake doesn't get stuck on proxy DNS rules).\n",
    "    - If proxy hostname does NOT resolve (likely off VPN/home network):\n",
    "        Disable proxy env vars entirely and go direct.\n",
    "    - Then run quick connectivity checks for Snowflake host DNS + S3 reachability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def can_resolve(host: str) -> bool:\n",
    "        try:\n",
    "            socket.getaddrinfo(host, 80)\n",
    "            return True\n",
    "        except OSError:\n",
    "            return False\n",
    "\n",
    "    proxy_host = proxy_url.replace(\"http://\", \"\").replace(\"https://\", \"\").split(\":\")[0]\n",
    "    proxy_resolves = can_resolve(proxy_host)\n",
    "\n",
    "    if proxy_resolves:\n",
    "        # Proxy is usable (VPN/corp). Keep it for S3, but bypass for Snowflake.\n",
    "        for k in [\"HTTP_PROXY\", \"HTTPS_PROXY\", \"http_proxy\", \"https_proxy\"]:\n",
    "            os.environ[k] = proxy_url\n",
    "\n",
    "        no_proxy = \",\".join([\n",
    "            \"localhost\", \"127.0.0.1\",\n",
    "            snowflake_host,\n",
    "            \".snowflakecomputing.com\",\n",
    "        ])\n",
    "        os.environ[\"NO_PROXY\"] = no_proxy\n",
    "        os.environ[\"no_proxy\"] = no_proxy\n",
    "        mode = \"PROXY_FOR_S3__BYPASS_FOR_SNOWFLAKE\"\n",
    "    else:\n",
    "        # Proxy not resolvable (home/off VPN). Go fully direct.\n",
    "        for k in [\"HTTP_PROXY\",\"HTTPS_PROXY\",\"ALL_PROXY\",\"http_proxy\",\"https_proxy\",\"all_proxy\"]:\n",
    "            os.environ.pop(k, None)\n",
    "        os.environ[\"NO_PROXY\"] = \",\".join([\"localhost\",\"127.0.0.1\",snowflake_host,\".snowflakecomputing.com\"])\n",
    "        os.environ[\"no_proxy\"] = os.environ[\"NO_PROXY\"]\n",
    "        mode = \"DIRECT_NO_PROXY\"\n",
    "\n",
    "    # --- Fast checks ---\n",
    "    # 1) Snowflake DNS\n",
    "    try:\n",
    "        socket.getaddrinfo(snowflake_host, 443)\n",
    "        snowflake_dns_ok = True\n",
    "    except OSError:\n",
    "        snowflake_dns_ok = False\n",
    "\n",
    "    # 2) S3 reachability (HEAD is enough)\n",
    "    s3_ok = None\n",
    "    try:\n",
    "        r = requests.head(\"https://s3.amazonaws.com\", timeout=timeout_s)\n",
    "        s3_ok = (r.status_code < 500)\n",
    "    except Exception:\n",
    "        s3_ok = False\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"proxy_resolves\": proxy_resolves,\n",
    "        \"snowflake_dns_ok\": snowflake_dns_ok,\n",
    "        \"s3_ok\": s3_ok,\n",
    "        \"http_proxy\": os.environ.get(\"HTTP_PROXY\"),\n",
    "        \"no_proxy\": os.environ.get(\"NO_PROXY\"),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c207a3-6b6b-4975-b388-b885051848ed",
   "metadata": {},
   "source": [
    "## 3) Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f48ae-1708-4cb1-b145-e95e40670e1b",
   "metadata": {},
   "source": [
    "### Extract data from postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ed8538-581a-4d38-a852-636265b86ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_columns(conn: psycopg2.extensions.connection, schema: str, table: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch column names for a given schema.table in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        schema: schema name (e.g., 'public')\n",
    "        table: table name (e.g., 'hotel')\n",
    "\n",
    "    Returns:\n",
    "        List of column names in order.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = %s AND table_name = %s\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (schema, table))\n",
    "        columns = [row[0] for row in cur.fetchall()]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f9a4983-f67f-4fab-9b5b-4f513c5945ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_pk(conn: psycopg2.extensions.connection, schema: str, table: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Fetch the primary key column names for a given schema.table in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        schema: schema name (e.g., 'public')\n",
    "        table: table name (e.g., 'hotel')\n",
    "\n",
    "    Returns:\n",
    "        List of primary key column names in order, or None if table has no PK.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT kcu.column_name\n",
    "    FROM information_schema.table_constraints tc\n",
    "    JOIN information_schema.key_column_usage kcu\n",
    "      ON tc.constraint_name = kcu.constraint_name\n",
    "      AND tc.table_schema = kcu.table_schema\n",
    "    WHERE tc.table_schema = %s\n",
    "      AND tc.table_name = %s\n",
    "      AND tc.constraint_type = 'PRIMARY KEY'\n",
    "    ORDER BY kcu.ordinal_position;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (schema, table))\n",
    "        pk_columns = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    return pk_columns if pk_columns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a58347-0b42-4bf6-98ea-064a7e981a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rowcount(conn: psycopg2.extensions.connection, sql: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the number of rows a SQL query will return.\n",
    "    Uses COUNT(*) wrapped around the query.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        sql: SQL query (string)\n",
    "\n",
    "    Returns:\n",
    "        Estimated row count (int)\n",
    "    \"\"\"\n",
    "    # Wrap the original query as a subquery\n",
    "    count_sql = f\"SELECT COUNT(*) FROM ({sql}) AS subquery\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(count_sql)\n",
    "            rowcount = cur.fetchone()[0]\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    return rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f22e569-56da-47cb-b1b0-0ef04accb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_query(table_cfg: Dict, schema_default: str, columns: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Build the base SQL query for a table export.\n",
    "\n",
    "    Args:\n",
    "        table_cfg: dictionary containing table config from YAML\n",
    "                   Must include 'mode' and either 'table' or 'query'\n",
    "        schema_default: default schema to use if table_cfg does not specify one\n",
    "        columns: optional list of columns to select (for mode='table')\n",
    "\n",
    "    Returns:\n",
    "        SQL string (no trailing semicolon)\n",
    "    \"\"\"\n",
    "    mode = table_cfg.get(\"mode\")\n",
    "    \n",
    "    if mode == \"table\":\n",
    "        table_name = table_cfg.get(\"table\")\n",
    "        if not table_name:\n",
    "            raise KeyError(f\"Table config '{table_cfg.get('name')}' missing 'table' key for mode='table'\")\n",
    "        \n",
    "        schema = table_cfg.get(\"schema\", schema_default)\n",
    "        \n",
    "        # Use all columns if not specified\n",
    "        cols_sql = \", \".join(columns) if columns else \"*\"\n",
    "        \n",
    "        sql = f\"SELECT {cols_sql} FROM {schema}.{table_name}\"\n",
    "        return sql\n",
    "\n",
    "    elif mode == \"query\":\n",
    "        query = table_cfg.get(\"query\")\n",
    "        if not query:\n",
    "            raise KeyError(f\"Table config '{table_cfg.get('name')}' missing 'query' key for mode='query'\")\n",
    "        return query.rstrip().rstrip(\";\")  # strip trailing semicolon\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode '{mode}' for table '{table_cfg.get('name')}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96211b57-2c85-4f35-a8aa-3b03db7c7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_partition_clause(\n",
    "    base_sql: str,\n",
    "    partition_spec: Dict\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Partition base_sql across [start_date, end_date) and return a list of SQL strings,\n",
    "    one per partition window.\n",
    "\n",
    "    Supported:\n",
    "      partition_spec = {\"type\": \"date_range\", \"column\": \"checkin_date\", \"granularity\": \"month\"|\"day\"}\n",
    "\n",
    "    Notes:\n",
    "      - end_date is EXCLUSIVE (so end_date=\"2026-01-01\" includes all of 2025).\n",
    "      - If base_sql already contains a WHERE (case-insensitive), appends with AND.\n",
    "    \"\"\"\n",
    "    column = partition_spec.get(\"column\")\n",
    "    start_date=partition_spec.get(\"start\")\n",
    "    end_date=partition_spec.get(\"end\")\n",
    "    if not column:\n",
    "        raise KeyError(\"partition_spec must have a 'column' key\")\n",
    "\n",
    "    if partition_spec.get(\"type\") != \"date_range\":\n",
    "        raise ValueError(f\"Unsupported partition type '{partition_spec.get('type')}'\")\n",
    "\n",
    "    granularity = partition_spec.get(\"granularity\", \"month\").lower()\n",
    "    if granularity not in {\"month\", \"day\"}:\n",
    "        raise ValueError(f\"Unsupported granularity '{granularity}' (use 'month' or 'day')\")\n",
    "\n",
    "    # Parse dates\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    if start >= end:\n",
    "        raise ValueError(f\"start_date must be < end_date. Got {start_date} .. {end_date}\")\n",
    "\n",
    "    # Helper: next month boundary from a given date\n",
    "    def next_month(d: date) -> date:\n",
    "        if d.month == 12:\n",
    "            return date(d.year + 1, 1, 1)\n",
    "        return date(d.year, d.month + 1, 1)\n",
    "\n",
    "    # Helper: attach WHERE/AND\n",
    "    has_where = \" where \" in base_sql.lower()\n",
    "\n",
    "    sqls: List[str] = []\n",
    "    cur = start\n",
    "\n",
    "    while cur < end:\n",
    "        if granularity == \"day\":\n",
    "            nxt = cur + timedelta(days=1)\n",
    "        else:  # month\n",
    "            # advance to first of next month; works best when cur is 1st-of-month (your case)\n",
    "            nxt = next_month(cur)\n",
    "\n",
    "        window_end = nxt if nxt < end else end\n",
    "\n",
    "        clause = (\n",
    "            f\"{column} >= '{cur.strftime('%Y-%m-%d')}' \"\n",
    "            f\"AND {column} < '{window_end.strftime('%Y-%m-%d')}'\"\n",
    "        )\n",
    "\n",
    "        if has_where:\n",
    "            sqls.append(f\"{base_sql} AND {clause}\")\n",
    "        else:\n",
    "            sqls.append(f\"{base_sql} WHERE {clause}\")\n",
    "\n",
    "        cur = window_end\n",
    "\n",
    "    return sqls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2039f321-5011-4f93-9197-58be8fb1f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_file_splits(rowcount: int, max_rows_per_file: int) -> List[Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Plan file splits for a given rowcount and max rows per file.\n",
    "\n",
    "    Args:\n",
    "        rowcount: total number of rows in the partition\n",
    "        max_rows_per_file: maximum rows per output file\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys:\n",
    "            - start_row: 0-based inclusive start\n",
    "            - end_row: exclusive end\n",
    "    \"\"\"\n",
    "    if rowcount <= 0:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < rowcount:\n",
    "        end = min(start + max_rows_per_file, rowcount)\n",
    "        chunks.append({\"start_row\": start, \"end_row\": end})\n",
    "        start = end\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "affdb183-d88c-4218-8970-b4a5a635de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chunk_query(\n",
    "    sql: str,\n",
    "    order_by: Optional[str],\n",
    "    chunk: Dict[str, int]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a SQL query for a specific chunk of rows.\n",
    "\n",
    "    Args:\n",
    "        sql: base SQL (should include WHERE/filters/partition/order)\n",
    "        order_by: column(s) used for deterministic ordering\n",
    "        chunk: dict with 'start_row' (inclusive) and 'end_row' (exclusive)\n",
    "\n",
    "    Returns:\n",
    "        SQL string with ORDER BY + OFFSET/LIMIT applied\n",
    "    \"\"\"\n",
    "    if not order_by:\n",
    "        raise ValueError(\"order_by must be specified for chunking\")\n",
    "\n",
    "    start = chunk[\"start_row\"]\n",
    "    limit = chunk[\"end_row\"] - chunk[\"start_row\"]\n",
    "    \n",
    "    # Ensure SQL has ORDER BY\n",
    "    sql_ordered = sql if \" order by \" in sql.lower() else f\"{sql} ORDER BY {order_by}\"\n",
    "\n",
    "    return f\"{sql_ordered} OFFSET {start} LIMIT {limit}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544632f1-e853-4fc8-ac77-856970c63a7e",
   "metadata": {},
   "source": [
    "#### Load data into snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cadedf1-d3df-4c09-bae4-e530cd5d88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_query_to_snowflake_table(\n",
    "    pg: PostgresCreds,\n",
    "    sf: SnowflakeCreds,\n",
    "    data_dir: str,\n",
    "    sql: str,                               # your dynamic SELECT\n",
    "    stage_fqn: str,                         # e.g. \"@HOTEL_ANALYTICS.RAW.LANDING_STAGE\"\n",
    "    file_format_fqn: str,                   # e.g. \"HOTEL_ANALYTICS.RAW.CSV_FMT\" (CSV, SKIP_HEADER=1, COMPRESSION=GZIP)\n",
    "    target_table_fqn: str,                  # e.g. \"HOTEL_ANALYTICS.RAW.BOOKING_SLICE\"\n",
    "    overwrite_table: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Executes SELECT on Postgres, stages results as gzipped CSV, then loads into Snowflake.\n",
    "    - Requires Snowflake INTERNAL stage (PUT must work).\n",
    "    - Creates/overwrites target table with inferred column types as VARCHAR by default.\n",
    "      (Good enough for a project slice; you can tighten types later.)\n",
    "\n",
    "    Returns basic run stats.\n",
    "    \"\"\"\n",
    "    run_id = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "    # --- Connect Postgres ---\n",
    "    pg_conn = psycopg2.connect(\n",
    "        host=pg.host, port=pg.port, dbname=pg.dbname, user=pg.user, password=pg.password\n",
    "    )\n",
    "    pg_cur = pg_conn.cursor()\n",
    "\n",
    "    pg_cur.execute(sql)\n",
    "    cols: List[str] = [d.name for d in pg_cur.description]\n",
    "\n",
    "    # --- Write gzipped CSV ---\n",
    "    tmp_dir = Path(data_dir) / \"tmp\"\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local_file = os.path.join(tmp_dir, f\"extract__{run_id}.csv.gz\")\n",
    "    file_uri= \"file://\" + Path(local_file).resolve().as_posix()\n",
    "    \n",
    "    rows_extracted = 0\n",
    "    with gzip.open(local_file, \"wt\", newline=\"\", encoding=\"utf-8\") as gz:\n",
    "        w = csv.writer(gz)\n",
    "        w.writerow(cols)  # header\n",
    "        for row in pg_cur:\n",
    "            w.writerow(row)\n",
    "            rows_extracted += 1\n",
    "\n",
    "    pg_cur.close()\n",
    "    pg_conn.close()\n",
    "\n",
    "    if rows_extracted == 0:\n",
    "        return {\"status\": \"SUCCESS\", \"rows_extracted\": 0, \"rows_loaded\": 0, \"target_table\": target_table_fqn}\n",
    "\n",
    "\n",
    "        # --- Connect Snowflake ---\n",
    "    sf_conn = snowflake.connector.connect(\n",
    "        account=sf.account,\n",
    "        user=sf.user,\n",
    "        password=sf.password,\n",
    "        role=sf.role,\n",
    "        warehouse=sf.warehouse,\n",
    "        database=sf.database,\n",
    "        schema=sf.schema,\n",
    "        autocommit=True,\n",
    "    )\n",
    "    sf_cur = sf_conn.cursor()\n",
    "\n",
    "    # Put into a deterministic folder under the stage\n",
    "    stage_path = f\"{stage_fqn}/pg_extract/{run_id}\"\n",
    "    sf_cur.execute(f\"PUT '{file_uri}' {stage_path} AUTO_COMPRESS=FALSE OVERWRITE=TRUE\")\n",
    "\n",
    "    # Optionally drop and re-create the table (simple project-friendly behavior)\n",
    "    if overwrite_table:\n",
    "        sf_cur.execute(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
    "\n",
    "    # Create a table with simple VARCHAR columns (fast + robust for a demo)\n",
    "    col_ddl = \", \".join([f'\"{c.upper()}\" VARCHAR' for c in cols])\n",
    "    sf_cur.execute(f\"CREATE TABLE IF NOT EXISTS {target_table_fqn} ({col_ddl})\")\n",
    "    \n",
    "        # 2) COPY into RAW TABLE\n",
    "    sf_cur.execute(f\"\"\"\n",
    "        COPY INTO {target_table_fqn}\n",
    "        FROM {stage_path}\n",
    "        FILE_FORMAT = (FORMAT_NAME = {file_format_fqn})\n",
    "        PATTERN = '.*\\\\.csv\\\\.gz'\n",
    "        ON_ERROR = 'ABORT_STATEMENT'\n",
    "    \"\"\")\n",
    "    results = sf_cur.fetchall()\n",
    "    loaded = sum(int(r[3]) for r in results if len(r) > 3)\n",
    "\n",
    "    sf_cur.close()\n",
    "    sf_cur.close()\n",
    "\n",
    "    return {\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"rows_extracted\": rows_extracted,\n",
    "        \"rows_loaded\": loaded,\n",
    "        \"local_file\": str(local_file),\n",
    "        \"stage_path\": stage_path,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092742f-0d7f-4e16-87ef-944b7c1b2e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOTEL_ANALYTICS.RAW.hotel_RAW\n",
      "HOTEL_ANALYTICS.RAW.room_type_RAW\n",
      "HOTEL_ANALYTICS.RAW.room_RAW\n",
      "HOTEL_ANALYTICS.RAW.booking_RAW\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ##### Get information from config files\n",
    "    BASE_DIR = Path.cwd().parent \n",
    "    config_path = BASE_DIR / \"config\"\n",
    "    config_yaml=config_path/ \"postgres_to_snowflake_config.yaml\"\n",
    "    data_dir=BASE_DIR/\"data\"\n",
    "    passwords_path=config_path/\".env\"\n",
    "\n",
    "    ##### Store all passwords\n",
    "    load_dotenv(dotenv_path=passwords_path,override=True)  # loads .env into environment variables\n",
    "    postgres_password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "    snowflake_password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "    config=load_config(config_yaml)\n",
    "           \n",
    "    ##### Validate config file\n",
    "    if validate_yaml_file(config_yaml): \n",
    "        config=load_config(config_yaml)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"YAML config file not found or invalid: {config_yaml}\")\n",
    "\n",
    "    ##### Connect to Postgres DB\n",
    "    pg_cfg = config.get(\"source\", {}).get(\"postgres\", {})\n",
    "    \n",
    "    pg_creds=PostgresCreds(\n",
    "        host=pg_cfg.get(\"host\", \"localhost\"),\n",
    "        port=str(pg_cfg.get(\"port\", 5432)),\n",
    "        dbname=pg_cfg.get(\"database\", \"\"),\n",
    "        user=pg_cfg.get(\"user\", \"\"),\n",
    "        password=postgres_password,\n",
    "        schema=pg_cfg.get(\"schema\")  # can be None\n",
    "    )\n",
    "    \n",
    "    pg_conn=create_pg_connection(pg_creds)\n",
    "\n",
    "    ###### Connect to Snowflake\n",
    "    sf_cfg = config.get(\"target\", {}).get(\"snowflake\", {})\n",
    "    \n",
    "    sf_creds=SnowflakeCreds(\n",
    "        account=sf_cfg.get(\"account\"),\n",
    "        user=sf_cfg.get(\"user\"),\n",
    "        role=sf_cfg.get(\"role\"),\n",
    "        warehouse=sf_cfg.get(\"warehouse\"),\n",
    "        database=sf_cfg.get(\"database\"),\n",
    "        schema=sf_cfg.get(\"schema\"),\n",
    "        password=snowflake_password,\n",
    "        sf_landing_stage=sf_cfg.get(\"stage\")\n",
    "    )\n",
    "\n",
    "    net = configure_network_for_snowflake(sf_creds.account)\n",
    "    ####### Hotel Config\n",
    "    table_schema=pg_cfg.get(\"schema\")\n",
    "    sqls=[]\n",
    "    max_rows_per_file=config.get(\"export\", {}).get(\"row_grouping\",{}).get(\"max_rows_per_file\",{})\n",
    "    for table in config.get(\"tables\", {}).keys():\n",
    "        table_name=config.get(\"tables\", {})[table][0].get(\"name\")\n",
    "        print(f\"{sf_creds.database}.{sf_creds.schema}.{table_name}_RAW\")\n",
    "        table_cfg=config.get(\"tables\", {})[table][0]\n",
    "        globals()[f\"{table_name}_columns\"]=get_table_columns(pg_conn,table_schema,table_name)\n",
    "        globals()[f\"{table_name}_pk\"]=get_table_pk(pg_conn,table_schema,table_name)\n",
    "        globals()[f\"{table_name}_rowcount\"]=estimate_rowcount(pg_conn,f'SELECT * FROM {table_schema}.{table_name}')\n",
    "        globals()[f\"{table_name}_base_query\"]= build_base_query(table_cfg, table_schema, globals()[f\"{table_name}_columns\"])\n",
    "        globals()[f\"{table_name}_partition_spec\"]=table_cfg.get(\"partition\")\n",
    "\n",
    "        if globals()[f\"{table_name}_partition_spec\"]!=None:\n",
    "            sqls=apply_partition_clause(globals()[f\"{table_name}_base_query\"],globals()[f\"{table_name}_partition_spec\"])\n",
    "\n",
    "            for sql in sqls:\n",
    "                chunk=plan_file_splits(globals()[f\"{table_name}_rowcount\"], max_rows_per_file)\n",
    "                chunk_sql_query=build_chunk_query(sql,table_cfg['order_by'],chunk[0])\n",
    "                postgres_query_to_snowflake_table( pg_creds,sf_creds,data_dir,chunk_sql_query,\n",
    "                                              f\"@{sf_creds.database}.{sf_creds.schema}.{sf_creds.sf_landing_stage}\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.CSV_FMT\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.{table_name}_RAW\",False)\n",
    "\n",
    "        else:\n",
    "            chunk=plan_file_splits(globals()[f\"{table_name}_rowcount\"], max_rows_per_file)\n",
    "            chunk_sql_query=build_chunk_query(globals()[f\"{table_name}_base_query\"],table_cfg['order_by'],chunk[0])\n",
    "            postgres_query_to_snowflake_table( pg_creds,sf_creds,data_dir,chunk_sql_query,\n",
    "                                              f\"@{sf_creds.database}.{sf_creds.schema}.{sf_creds.sf_landing_stage}\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.CSV_FMT\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.{table_name}_RAW\")        \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9ef7d-2947-4f7f-94ea-5ad04f23bf06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b0715-7f8c-4d0c-b4f3-b9dd3bdf09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adada90a-9aa8-414d-b250-04313ad6f334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
