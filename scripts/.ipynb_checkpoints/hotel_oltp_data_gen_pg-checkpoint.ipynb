{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc3d6c69-cf36-4e64-b773-404c505c3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: public\n",
      "Tables: 40\n",
      "Enums detected: 13\n",
      "Output dir: C:\\Users\\Alekya Kastury\\github\\hotel-analytics-platform\\scripts\\scripts\\data\\faker\n",
      "Truncating tables...\n",
      "Truncate done.\n",
      "→ address: generating 2,000\n",
      "→ address: loading via COPY\n",
      "✅ address: generated+loaded 2,000 rows\n",
      "→ audit_log: generating 2,000\n",
      "→ audit_log: loading via COPY\n",
      "✅ audit_log: generated+loaded 2,000 rows\n",
      "→ channel: generating 12\n",
      "→ channel: loading via COPY\n",
      "✅ channel: generated+loaded 12 rows\n",
      "→ contact: generating 2,000\n",
      "→ contact: loading via COPY\n",
      "✅ contact: generated+loaded 2,000 rows\n",
      "→ customer: generating 30,000\n",
      "→ customer: loading via COPY\n",
      "✅ customer: generated+loaded 30,000 rows\n",
      "→ hotel: generating 12\n",
      "→ hotel: loading via COPY\n",
      "✅ hotel: generated+loaded 12 rows\n",
      "→ booking: generating 70,000\n",
      "→ booking: loading via COPY\n",
      "✅ booking: generated+loaded 70,000 rows\n",
      "→ booking_cancellation: generating 8,000\n",
      "→ booking_cancellation: loading via COPY\n",
      "✅ booking_cancellation: generated+loaded 8,000 rows\n",
      "→ booking_event: generating 70,000\n",
      "→ booking_event: loading via COPY\n",
      "✅ booking_event: generated+loaded 70,000 rows\n",
      "→ cancellation_policy: generating 50\n",
      "→ cancellation_policy: loading via COPY\n",
      "✅ cancellation_policy: generated+loaded 50 rows\n",
      "→ employee: generating 2,000\n",
      "→ employee: loading via COPY\n",
      "✅ employee: generated+loaded 2,000 rows\n",
      "→ identity_document: generating 2,000\n",
      "→ identity_document: loading via COPY\n",
      "✅ identity_document: generated+loaded 2,000 rows\n",
      "→ guest: generating 120,000\n",
      "→ guest: loading via COPY\n",
      "✅ guest: generated+loaded 120,000 rows\n",
      "→ invoice: generating 70,000\n",
      "→ invoice: loading via COPY\n",
      "✅ invoice: generated+loaded 70,000 rows\n",
      "→ invoice_line_item: generating 60,000\n",
      "→ invoice_line_item: loading via COPY\n",
      "✅ invoice_line_item: generated+loaded 60,000 rows\n",
      "→ no_show: generating 2,000\n",
      "→ no_show: loading via COPY\n",
      "✅ no_show: generated+loaded 2,000 rows\n",
      "→ payment: generating 60,000\n",
      "→ payment: loading via COPY\n",
      "✅ payment: generated+loaded 60,000 rows\n",
      "→ promotion: generating 2,000\n",
      "→ promotion: loading via COPY\n",
      "✅ promotion: generated+loaded 2,000 rows\n",
      "→ booking_discount: generating 70,000\n",
      "→ booking_discount: loading via COPY\n",
      "✅ booking_discount: generated+loaded 70,000 rows\n",
      "→ rate_plan: generating 50\n",
      "→ rate_plan: loading via COPY\n",
      "✅ rate_plan: generated+loaded 50 rows\n",
      "→ rate_calendar: generating 50\n",
      "→ rate_calendar: loading via COPY\n",
      "✅ rate_calendar: generated+loaded 50 rows\n",
      "→ refund: generating 8,000\n",
      "→ refund: loading via COPY\n",
      "✅ refund: generated+loaded 8,000 rows\n",
      "→ review: generating 2,000\n",
      "→ review: loading via COPY\n",
      "✅ review: generated+loaded 2,000 rows\n",
      "→ review_category: generating 2,000\n",
      "→ review_category: loading via COPY\n",
      "✅ review_category: generated+loaded 2,000 rows\n",
      "→ review_score: generating 2,000\n",
      "→ review_score: loading via COPY\n",
      "✅ review_score: generated+loaded 2,000 rows\n",
      "→ room_type: generating 50\n",
      "→ room_type: loading via COPY\n",
      "✅ room_type: generated+loaded 50 rows\n",
      "→ room: generating 1,000\n",
      "→ room: loading via COPY\n",
      "✅ room: generated+loaded 1,000 rows\n",
      "→ booking_room: generating 90,000\n",
      "→ booking_room: loading via COPY\n",
      "✅ booking_room: generated+loaded 90,000 rows\n",
      "→ housekeeping_task: generating 2,000\n",
      "→ housekeeping_task: loading via COPY\n",
      "✅ housekeeping_task: generated+loaded 2,000 rows\n",
      "→ maintenance_ticket: generating 2,000\n",
      "→ maintenance_ticket: loading via COPY\n",
      "✅ maintenance_ticket: generated+loaded 2,000 rows\n",
      "→ room_block: generating 2,000\n",
      "→ room_block: loading via COPY\n",
      "✅ room_block: generated+loaded 2,000 rows\n",
      "→ room_night: generating 2,000\n",
      "→ room_night: loading via COPY\n",
      "✅ room_night: generated+loaded 2,000 rows\n",
      "→ service_catalog: generating 50\n",
      "→ service_catalog: loading via COPY\n",
      "✅ service_catalog: generated+loaded 50 rows\n",
      "→ service_order: generating 2,000\n",
      "→ service_order: loading via COPY\n",
      "✅ service_order: generated+loaded 2,000 rows\n",
      "→ service_order_item: generating 2,000\n",
      "→ service_order_item: loading via COPY\n",
      "✅ service_order_item: generated+loaded 2,000 rows\n",
      "→ stay: generating 2,000\n",
      "→ stay: loading via COPY\n",
      "✅ stay: generated+loaded 2,000 rows\n",
      "→ check_in: generating 2,000\n",
      "→ check_in: loading via COPY\n",
      "✅ check_in: generated+loaded 2,000 rows\n",
      "→ check_out: generating 2,000\n",
      "→ check_out: loading via COPY\n",
      "✅ check_out: generated+loaded 2,000 rows\n",
      "→ stay_guest: generating 2,000\n",
      "→ stay_guest: loading via COPY\n",
      "✅ stay_guest: generated+loaded 2,000 rows\n",
      "→ tax_fee: generating 2,000\n",
      "→ tax_fee: loading via COPY\n",
      "✅ tax_fee: generated+loaded 2,000 rows\n",
      "✅ DONE\n"
     ]
    }
   ],
   "source": [
    "# seed_faker_one_time.py\n",
    "# One-time Faker seeder for your Hotel OLTP Postgres schema.\n",
    "#\n",
    "# ✅ Fixes included (based on your errors):\n",
    "# - Introspects schema: tables, columns, PK, FK, ENUMs, UNIQUE (single-column only)\n",
    "# - Loads in FK dependency order\n",
    "# - TRUNCATE ... RESTART IDENTITY CASCADE (optional)\n",
    "# - Generates FK-valid data\n",
    "# - Handles 1:1 tables automatically via UNIQUE(FK) without replacement\n",
    "# - Special-cases:\n",
    "#     - booking_room UNIQUE(booking_id, room_id)\n",
    "#     - room_night UNIQUE(room_id, night_date)\n",
    "#     - booking_discount UNIQUE(booking_id, promotion_id)\n",
    "#     - stay_check: actual_checkout_at >= actual_checkin_at when both present\n",
    "#     - booking_check: checkout_date >= checkin_date when both present\n",
    "# - ENUM-safe: never hardcode enum labels (samples from pg enums)\n",
    "#\n",
    "# ✅ CRITICAL FIX (for your current crash):\n",
    "# - After loading EACH table, if it has a single-column PK, we query the DB and store the\n",
    "#   actual PK values into ref_ids[table_lc].\n",
    "#   This guarantees downstream FK tables (like check_in.stay_id) get real stay_id values,\n",
    "#   instead of falling back to \"1\" (which caused check_in_stay_id_key duplicates).\n",
    "#\n",
    "# Run:\n",
    "#   python seed_faker_one_time.py\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install psycopg2-binary faker\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from datetime import date, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import psycopg2\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG (edit these)\n",
    "# =========================\n",
    "@dataclass(frozen=True)\n",
    "class PostgresCreds:\n",
    "    host: str\n",
    "    port: str\n",
    "    dbname: str\n",
    "    user: str\n",
    "    password: str\n",
    "    schema: str = \"public\"\n",
    "\n",
    "\n",
    "PG = PostgresCreds(\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    "    dbname=\"hotel_oltp\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\",\n",
    "    schema=\"public\",\n",
    ")\n",
    "\n",
    "OUT_DIR = Path(\"scripts/data/faker\")  # change to \"data/faker\" if you want\n",
    "SEED = 42\n",
    "TRUNCATE_FIRST = True\n",
    "\n",
    "# Override row counts (applied only if table exists)\n",
    "ROW_COUNTS_OVERRIDE = {\n",
    "    \"booking\": 70_000,\n",
    "    \"booking_room\": 90_000,\n",
    "    \"guest\": 120_000,\n",
    "    \"invoice\": 70_000,\n",
    "    \"payment\": 60_000,\n",
    "    \"refund\": 8_000,\n",
    "    \"booking_cancellation\": 8_000,\n",
    "    \"room\": 1_000,\n",
    "    \"hotel\": 12,\n",
    "    \"customer\": 30_000,\n",
    "    \"channel\": 12,\n",
    "    \"promotion\": 2_000,\n",
    "}\n",
    "# =========================\n",
    "\n",
    "\n",
    "def pg_dsn(pg: PostgresCreds) -> str:\n",
    "    return f\"host={pg.host} port={pg.port} dbname={pg.dbname} user={pg.user} password={pg.password}\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ColumnInfo:\n",
    "    table: str\n",
    "    column: str\n",
    "    data_type: str\n",
    "    udt_name: str\n",
    "    is_nullable: bool\n",
    "    char_max_len: Optional[int]\n",
    "    numeric_precision: Optional[int]\n",
    "    numeric_scale: Optional[int]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ForeignKey:\n",
    "    table: str\n",
    "    column: str\n",
    "    ref_table: str\n",
    "    ref_column: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrimaryKey:\n",
    "    table: str\n",
    "    columns: Tuple[str, ...]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Introspection\n",
    "# -------------------------\n",
    "def fetch_tables(conn, schema: str) -> List[str]:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = %s AND table_type='BASE TABLE'\n",
    "            ORDER BY table_name\n",
    "            \"\"\",\n",
    "            (schema,),\n",
    "        )\n",
    "        return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "def fetch_columns(conn, schema: str) -> Dict[str, List[ColumnInfo]]:\n",
    "    out: Dict[str, List[ColumnInfo]] = {}\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "              table_name,\n",
    "              column_name,\n",
    "              data_type,\n",
    "              udt_name,\n",
    "              is_nullable,\n",
    "              character_maximum_length,\n",
    "              numeric_precision,\n",
    "              numeric_scale\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = %s\n",
    "            ORDER BY table_name, ordinal_position\n",
    "            \"\"\",\n",
    "            (schema,),\n",
    "        )\n",
    "        for t, c, dt, udt, nul, cmax, prec, scale in cur.fetchall():\n",
    "            out.setdefault(t, []).append(\n",
    "                ColumnInfo(\n",
    "                    table=t,\n",
    "                    column=c,\n",
    "                    data_type=dt,\n",
    "                    udt_name=udt,\n",
    "                    is_nullable=(nul == \"YES\"),\n",
    "                    char_max_len=cmax,\n",
    "                    numeric_precision=prec,\n",
    "                    numeric_scale=scale,\n",
    "                )\n",
    "            )\n",
    "    return out\n",
    "\n",
    "\n",
    "def fetch_primary_keys(conn, schema: str) -> Dict[str, PrimaryKey]:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT tc.table_name, kcu.column_name, kcu.ordinal_position\n",
    "            FROM information_schema.table_constraints tc\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "              ON tc.constraint_name = kcu.constraint_name\n",
    "             AND tc.table_schema = kcu.table_schema\n",
    "            WHERE tc.table_schema = %s\n",
    "              AND tc.constraint_type = 'PRIMARY KEY'\n",
    "            ORDER BY tc.table_name, kcu.ordinal_position\n",
    "            \"\"\",\n",
    "            (schema,),\n",
    "        )\n",
    "        tmp: Dict[str, List[str]] = {}\n",
    "        for t, c, _ in cur.fetchall():\n",
    "            tmp.setdefault(t, []).append(c)\n",
    "    return {t: PrimaryKey(table=t, columns=tuple(cols)) for t, cols in tmp.items()}\n",
    "\n",
    "\n",
    "def fetch_foreign_keys(conn, schema: str) -> List[ForeignKey]:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "              tc.table_name,\n",
    "              kcu.column_name,\n",
    "              ccu.table_name AS ref_table_name,\n",
    "              ccu.column_name AS ref_column_name\n",
    "            FROM information_schema.table_constraints tc\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "              ON tc.constraint_name = kcu.constraint_name\n",
    "             AND tc.table_schema = kcu.table_schema\n",
    "            JOIN information_schema.constraint_column_usage ccu\n",
    "              ON ccu.constraint_name = tc.constraint_name\n",
    "             AND ccu.table_schema = tc.table_schema\n",
    "            WHERE tc.table_schema = %s\n",
    "              AND tc.constraint_type = 'FOREIGN KEY'\n",
    "            ORDER BY tc.table_name, kcu.column_name\n",
    "            \"\"\",\n",
    "            (schema,),\n",
    "        )\n",
    "        return [ForeignKey(t, c, rt, rc) for t, c, rt, rc in cur.fetchall()]\n",
    "\n",
    "\n",
    "def fetch_enum_values(conn) -> Dict[str, List[str]]:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT t.typname AS enum_name, e.enumlabel AS enum_value\n",
    "            FROM pg_type t\n",
    "            JOIN pg_enum e ON t.oid = e.enumtypid\n",
    "            ORDER BY t.typname, e.enumsortorder\n",
    "            \"\"\"\n",
    "        )\n",
    "        out: Dict[str, List[str]] = {}\n",
    "        for enum_name, enum_value in cur.fetchall():\n",
    "            out.setdefault(enum_name.lower(), []).append(enum_value)\n",
    "        return out\n",
    "\n",
    "\n",
    "def fetch_unique_columns(conn, schema: str) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    ✅ ONLY single-column UNIQUE constraints: {table_lc: {col,...}}\n",
    "    Store table keys lowercased to avoid casing mismatches.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            WITH uniq AS (\n",
    "              SELECT\n",
    "                tc.table_schema,\n",
    "                tc.table_name,\n",
    "                tc.constraint_name,\n",
    "                COUNT(kcu.column_name) AS col_count\n",
    "              FROM information_schema.table_constraints tc\n",
    "              JOIN information_schema.key_column_usage kcu\n",
    "                ON tc.constraint_name = kcu.constraint_name\n",
    "               AND tc.table_schema = kcu.table_schema\n",
    "              WHERE tc.table_schema = %s\n",
    "                AND tc.constraint_type = 'UNIQUE'\n",
    "              GROUP BY 1,2,3\n",
    "            )\n",
    "            SELECT kcu.table_name, kcu.column_name\n",
    "            FROM uniq\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "              ON uniq.constraint_name = kcu.constraint_name\n",
    "             AND uniq.table_schema = kcu.table_schema\n",
    "            WHERE uniq.col_count = 1\n",
    "            \"\"\",\n",
    "            (schema,),\n",
    "        )\n",
    "        out: Dict[str, Set[str]] = {}\n",
    "        for t, c in cur.fetchall():\n",
    "            out.setdefault(t.lower(), set()).add(c)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Dependency ordering\n",
    "# -------------------------\n",
    "def topo_sort_tables(tables: List[str], fks: List[ForeignKey]) -> List[str]:\n",
    "    deps: Dict[str, Set[str]] = {t: set() for t in tables}\n",
    "    rdeps: Dict[str, Set[str]] = {t: set() for t in tables}\n",
    "\n",
    "    for fk in fks:\n",
    "        if fk.table in deps and fk.ref_table in deps:\n",
    "            deps[fk.table].add(fk.ref_table)\n",
    "            rdeps[fk.ref_table].add(fk.table)\n",
    "\n",
    "    q = sorted([t for t in tables if not deps[t]])\n",
    "    out: List[str] = []\n",
    "\n",
    "    while q:\n",
    "        n = q.pop(0)\n",
    "        out.append(n)\n",
    "        for m in sorted(rdeps[n]):\n",
    "            deps[m].discard(n)\n",
    "            if not deps[m] and m not in out and m not in q:\n",
    "                q.append(m)\n",
    "        q.sort()\n",
    "\n",
    "    remaining = [t for t in tables if t not in out]\n",
    "    return out + sorted(remaining)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Default row counts\n",
    "# -------------------------\n",
    "def default_row_counts(tables: List[str]) -> Dict[str, int]:\n",
    "    rc: Dict[str, int] = {}\n",
    "    for t in tables:\n",
    "        tl = t.lower()\n",
    "        if any(k in tl for k in [\"lookup\", \"type\", \"status\", \"code\", \"catalog\", \"policy\", \"rate_plan\", \"rate_calendar\"]):\n",
    "            rc[t] = 50\n",
    "        elif tl == \"hotel\":\n",
    "            rc[t] = 12\n",
    "        elif tl == \"room\":\n",
    "            rc[t] = 1000\n",
    "        elif \"customer\" in tl:\n",
    "            rc[t] = 30_000\n",
    "        elif \"booking\" in tl:\n",
    "            rc[t] = 70_000\n",
    "        elif any(k in tl for k in [\"payment\", \"invoice\", \"transaction\", \"charge\"]):\n",
    "            rc[t] = 60_000\n",
    "        elif any(k in tl for k in [\"refund\", \"cancellation\"]):\n",
    "            rc[t] = 8_000\n",
    "        else:\n",
    "            rc[t] = 2_000\n",
    "    return rc\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Unique text registry\n",
    "# -------------------------\n",
    "_UNIQUE_SEEN: Dict[Tuple[str, str], Set[str]] = {}\n",
    "\n",
    "\n",
    "def unique_text(key: Tuple[str, str], make_value, max_tries: int = 200000) -> str:\n",
    "    seen = _UNIQUE_SEEN.setdefault(key, set())\n",
    "    for _ in range(max_tries):\n",
    "        v = str(make_value()).strip()\n",
    "        if v and v not in seen:\n",
    "            seen.add(v)\n",
    "            return v\n",
    "    raise RuntimeError(f\"Could not generate unique value for {key}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Value generator\n",
    "# -------------------------\n",
    "def generate_value(fake: Faker, col: ColumnInfo, row_idx: int, enums: Dict[str, List[str]]) -> Any:\n",
    "    name = col.column.lower()\n",
    "    dt = col.data_type.lower()\n",
    "    udt = col.udt_name.lower()\n",
    "\n",
    "    # ENUM\n",
    "    if udt in enums:\n",
    "        if col.is_nullable and random.random() < 0.03:\n",
    "            return None\n",
    "        return random.choice(enums[udt])\n",
    "\n",
    "    # standard timestamps\n",
    "    if name in {\"created_at\", \"updated_at\", \"loaded_at\", \"ingested_at\"}:\n",
    "        base = fake.date_time_between(start_date=\"-2y\", end_date=\"now\", tzinfo=timezone.utc)\n",
    "        if name == \"updated_at\":\n",
    "            base = base + timedelta(days=random.randint(0, 180))\n",
    "        return base\n",
    "\n",
    "    # date\n",
    "    if dt == \"date\":\n",
    "        return fake.date_between(start_date=\"-2y\", end_date=\"+1y\")\n",
    "\n",
    "    if dt in {\"timestamp without time zone\", \"timestamp with time zone\"} or udt in {\"timestamp\", \"timestamptz\"}:\n",
    "        return fake.date_time_between(start_date=\"-2y\", end_date=\"now\", tzinfo=timezone.utc)\n",
    "\n",
    "    # int\n",
    "    if dt in {\"integer\", \"bigint\", \"smallint\"} or udt in {\"int2\", \"int4\", \"int8\"}:\n",
    "        if name.endswith(\"_id\"):\n",
    "            return row_idx\n",
    "        if name == \"score\":\n",
    "            return random.randint(1, 5)\n",
    "        if any(k in name for k in [\"rating\", \"stars\", \"score\"]):\n",
    "            return random.randint(1, 5)\n",
    "        if any(k in name for k in [\"count\", \"qty\", \"quantity\", \"nights\", \"floor\", \"occupancy\"]):\n",
    "            return random.randint(1, 10)\n",
    "        return random.randint(1, 100000)\n",
    "\n",
    "    # uuid\n",
    "    if dt == \"uuid\" or udt == \"uuid\":\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "    # bool\n",
    "    if dt == \"boolean\":\n",
    "        return random.random() < 0.85 if (\"is_\" in name or name.endswith(\"_flag\")) else (random.random() < 0.5)\n",
    "\n",
    "    # numeric/decimal\n",
    "    if dt in {\"numeric\", \"decimal\"} or udt == \"numeric\":\n",
    "        scale = col.numeric_scale or 2\n",
    "        if \"percent\" in name or name.endswith(\"_pct\") or name.endswith(\"pct\"):\n",
    "            return round(random.uniform(0, 100), scale)\n",
    "        if \"ratio\" in name or \"fraction\" in name:\n",
    "            return round(random.uniform(0, 1), scale)\n",
    "        if col.table.lower() == \"promotion\" and name in {\"value\", \"discount_value\", \"discount_amount\", \"discount\"}:\n",
    "            return round(random.uniform(5, 50), scale)\n",
    "        if any(k in name for k in [\"amount\", \"price\", \"rate\", \"cost\", \"fee\", \"total\", \"tax\"]):\n",
    "            return round(random.uniform(20, 2000), scale)\n",
    "        return round(random.uniform(0, 1000), scale)\n",
    "\n",
    "    # text/varchar\n",
    "    if dt in {\"character varying\", \"character\", \"text\"}:\n",
    "        maxlen = col.char_max_len or 255\n",
    "        if name == \"email\":\n",
    "            return unique_text((col.table, col.column), lambda: fake.email())[:maxlen]\n",
    "        if name.endswith(\"_name\") or name in {\"name\", \"code\"}:\n",
    "            return unique_text((col.table, col.column), lambda: f\"{fake.word().title()}_{uuid.uuid4().hex[:6]}\")[:maxlen]\n",
    "        if \"timezone\" in name:\n",
    "            return \"America/New_York\"[:maxlen]\n",
    "        if maxlen <= 20:\n",
    "            return fake.word()[:maxlen]\n",
    "        if maxlen <= 80:\n",
    "            return fake.sentence(nb_words=6)[:maxlen]\n",
    "        return fake.sentence(nb_words=10)[:maxlen]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_fk_map(fks: List[ForeignKey]) -> Dict[Tuple[str, str], Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    ✅ Normalize table keys to lowercase to avoid casing mismatches.\n",
    "    \"\"\"\n",
    "    return {(fk.table.lower(), fk.column): (fk.ref_table.lower(), fk.ref_column) for fk in fks}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Special-case generators\n",
    "# -------------------------\n",
    "def generate_booking_room_csv(\n",
    "    *,\n",
    "    fake: Faker,\n",
    "    out_dir: Path,\n",
    "    table: str,\n",
    "    cols: List[ColumnInfo],\n",
    "    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n",
    "    ref_ids: Dict[str, List[Any]],\n",
    "    n_rows: int,\n",
    "    enums: Dict[str, List[str]],\n",
    ") -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / f\"{table}.csv\"\n",
    "    colnames = [c.column for c in cols]\n",
    "    col_lc = {c.column.lower(): c.column for c in cols}\n",
    "    table_lc = table.lower()\n",
    "\n",
    "    booking_ids = list(ref_ids.get(\"booking\", []))\n",
    "    room_ids = list(ref_ids.get(\"room\", []))\n",
    "    if not booking_ids or not room_ids:\n",
    "        raise RuntimeError(\"booking_room needs booking and room ids available before generation.\")\n",
    "\n",
    "    booking_id_col = col_lc.get(\"booking_id\")\n",
    "    room_id_col = col_lc.get(\"room_id\")\n",
    "    if not booking_id_col or not room_id_col:\n",
    "        raise RuntimeError(\"booking_room expected columns booking_id and room_id.\")\n",
    "\n",
    "    seen: set[tuple[Any, Any]] = set()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    random.shuffle(booking_ids)\n",
    "\n",
    "    i = 0\n",
    "    while len(rows) < n_rows:\n",
    "        b = booking_ids[i % len(booking_ids)]\n",
    "        rooms_for_booking = random.randint(1, 3)\n",
    "        for _ in range(rooms_for_booking):\n",
    "            r = random.choice(room_ids)\n",
    "            pair = (b, r)\n",
    "            if pair in seen:\n",
    "                continue\n",
    "            seen.add(pair)\n",
    "\n",
    "            row: Dict[str, Any] = {booking_id_col: b, room_id_col: r}\n",
    "            for c in cols:\n",
    "                if c.column in row:\n",
    "                    continue\n",
    "                fk_key = (table_lc, c.column)\n",
    "                if fk_key in fk_map:\n",
    "                    parent_table, _ = fk_map[fk_key]\n",
    "                    candidates = ref_ids.get(parent_table, [])\n",
    "                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n",
    "                else:\n",
    "                    v = generate_value(fake, c, len(rows) + 1, enums)\n",
    "                    if v is None and not c.is_nullable:\n",
    "                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n",
    "                    row[c.column] = v\n",
    "\n",
    "            rows.append(row)\n",
    "            if len(rows) >= n_rows:\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(colnames)\n",
    "        for row in rows[:n_rows]:\n",
    "            w.writerow([row.get(cn) for cn in colnames])\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_room_night_csv(\n",
    "    *,\n",
    "    fake: Faker,\n",
    "    out_dir: Path,\n",
    "    table: str,\n",
    "    cols: List[ColumnInfo],\n",
    "    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n",
    "    ref_ids: Dict[str, List[Any]],\n",
    "    n_rows: int,\n",
    "    enums: Dict[str, List[str]],\n",
    ") -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / f\"{table}.csv\"\n",
    "    colnames = [c.column for c in cols]\n",
    "    col_lc = {c.column.lower(): c.column for c in cols}\n",
    "    table_lc = table.lower()\n",
    "\n",
    "    room_ids = list(ref_ids.get(\"room\", []))\n",
    "    if not room_ids:\n",
    "        raise RuntimeError(\"room_night needs room ids available before generation.\")\n",
    "\n",
    "    room_id_col = col_lc.get(\"room_id\")\n",
    "    night_date_col = col_lc.get(\"night_date\")\n",
    "    if not room_id_col or not night_date_col:\n",
    "        raise RuntimeError(\"room_night expected columns room_id and night_date.\")\n",
    "\n",
    "    start = date.today() - timedelta(days=730)\n",
    "    end = date.today() + timedelta(days=365)\n",
    "    total_days = (end - start).days\n",
    "\n",
    "    seen: set[tuple[Any, date]] = set()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    per_room = max(1, n_rows // len(room_ids))\n",
    "    random.shuffle(room_ids)\n",
    "\n",
    "    for rid in room_ids:\n",
    "        if len(rows) >= n_rows:\n",
    "            break\n",
    "        k = min(per_room, total_days)\n",
    "        offsets = random.sample(range(total_days), k=k)\n",
    "        for off in offsets:\n",
    "            nd = start + timedelta(days=off)\n",
    "            pair = (rid, nd)\n",
    "            if pair in seen:\n",
    "                continue\n",
    "            seen.add(pair)\n",
    "\n",
    "            row: Dict[str, Any] = {room_id_col: rid, night_date_col: nd}\n",
    "            for c in cols:\n",
    "                if c.column in row:\n",
    "                    continue\n",
    "                fk_key = (table_lc, c.column)\n",
    "                if fk_key in fk_map:\n",
    "                    parent_table, _ = fk_map[fk_key]\n",
    "                    candidates = ref_ids.get(parent_table, [])\n",
    "                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n",
    "                else:\n",
    "                    v = generate_value(fake, c, len(rows) + 1, enums)\n",
    "                    if v is None and not c.is_nullable:\n",
    "                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n",
    "                    row[c.column] = v\n",
    "\n",
    "            rows.append(row)\n",
    "            if len(rows) >= n_rows:\n",
    "                break\n",
    "\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(colnames)\n",
    "        for row in rows[:n_rows]:\n",
    "            w.writerow([row.get(cn) for cn in colnames])\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_stay_csv(\n",
    "    *,\n",
    "    fake: Faker,\n",
    "    out_dir: Path,\n",
    "    table: str,\n",
    "    cols: List[ColumnInfo],\n",
    "    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n",
    "    ref_ids: Dict[str, List[Any]],\n",
    "    n_rows: int,\n",
    "    enums: Dict[str, List[str]],\n",
    "    unique_cols: Dict[str, Set[str]],\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Enforces:\n",
    "      actual_checkout_at IS NULL OR actual_checkin_at IS NULL OR actual_checkout_at >= actual_checkin_at\n",
    "\n",
    "    ✅ UNIQUE(FK): if stay.booking_id is UNIQUE, assign booking_id without replacement.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / f\"{table}.csv\"\n",
    "    colnames = [c.column for c in cols]\n",
    "    col_lc = {c.column.lower(): c.column for c in cols}\n",
    "    table_lc = table.lower()\n",
    "\n",
    "    booking_id_col = col_lc.get(\"booking_id\")\n",
    "    status_col = col_lc.get(\"stay_status\") or col_lc.get(\"status\")\n",
    "    aci_col = col_lc.get(\"actual_checkin_at\")\n",
    "    aco_col = col_lc.get(\"actual_checkout_at\")\n",
    "\n",
    "    booking_ids = list(ref_ids.get(\"booking\", []))\n",
    "\n",
    "    is_booking_unique = bool(booking_id_col and booking_id_col in unique_cols.get(table_lc, set()))\n",
    "    if is_booking_unique:\n",
    "        if len(booking_ids) < n_rows:\n",
    "            raise RuntimeError(\n",
    "                f'{table_lc}.\"{booking_id_col}\" is UNIQUE but only {len(booking_ids)} booking ids exist '\n",
    "                f\"for requested n_rows={n_rows}.\"\n",
    "            )\n",
    "        random.shuffle(booking_ids)\n",
    "\n",
    "    status_ci = next((c for c in cols if c.column == (status_col or \"\")), None)\n",
    "    stay_status_choices = enums.get(status_ci.udt_name.lower(), []) if status_ci else []\n",
    "\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(colnames)\n",
    "\n",
    "        for i in range(1, n_rows + 1):\n",
    "            row: Dict[str, Any] = {}\n",
    "\n",
    "            if booking_id_col:\n",
    "                if is_booking_unique:\n",
    "                    row[booking_id_col] = booking_ids[i - 1]\n",
    "                else:\n",
    "                    row[booking_id_col] = random.choice(booking_ids) if booking_ids else 1\n",
    "\n",
    "            scenario = None\n",
    "            if status_col and stay_status_choices:\n",
    "                scenario = random.choice(stay_status_choices)\n",
    "                row[status_col] = scenario\n",
    "\n",
    "            s = (scenario or \"\").upper()\n",
    "            is_cancel = \"CANCEL\" in s\n",
    "            is_out = \"OUT\" in s\n",
    "            is_in = (\"IN\" in s) and not is_out\n",
    "\n",
    "            if is_cancel:\n",
    "                if aci_col:\n",
    "                    row[aci_col] = None\n",
    "                if aco_col:\n",
    "                    row[aco_col] = None\n",
    "            elif is_out:\n",
    "                ci = fake.date_time_between(start_date=\"-180d\", end_date=\"now\", tzinfo=timezone.utc)\n",
    "                co = ci + timedelta(days=random.randint(1, 10), hours=random.randint(0, 6), minutes=random.randint(0, 59))\n",
    "                if aci_col:\n",
    "                    row[aci_col] = ci\n",
    "                if aco_col:\n",
    "                    row[aco_col] = co\n",
    "            elif is_in:\n",
    "                ci = fake.date_time_between(start_date=\"-180d\", end_date=\"now\", tzinfo=timezone.utc)\n",
    "                if aci_col:\n",
    "                    row[aci_col] = ci\n",
    "                if aco_col:\n",
    "                    row[aco_col] = None\n",
    "            else:\n",
    "                if aci_col:\n",
    "                    row[aci_col] = None\n",
    "                if aco_col:\n",
    "                    row[aco_col] = None\n",
    "\n",
    "            for c in cols:\n",
    "                if c.column in row:\n",
    "                    continue\n",
    "                fk_key = (table_lc, c.column)\n",
    "                if fk_key in fk_map:\n",
    "                    parent_table, _ = fk_map[fk_key]\n",
    "                    candidates = ref_ids.get(parent_table, [])\n",
    "                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n",
    "                else:\n",
    "                    v = generate_value(fake, c, i, enums)\n",
    "                    if v is None and not c.is_nullable:\n",
    "                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n",
    "                    row[c.column] = v\n",
    "\n",
    "            if aci_col and aco_col:\n",
    "                ci = row.get(aci_col)\n",
    "                co = row.get(aco_col)\n",
    "                if ci is not None and co is not None and co < ci:\n",
    "                    row[aco_col] = ci + timedelta(days=1)\n",
    "\n",
    "            w.writerow([row.get(cn) for cn in colnames])\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_booking_discount_csv(\n",
    "    *,\n",
    "    fake: Faker,\n",
    "    out_dir: Path,\n",
    "    table: str,\n",
    "    cols: List[ColumnInfo],\n",
    "    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n",
    "    ref_ids: Dict[str, List[Any]],\n",
    "    n_rows: int,\n",
    "    enums: Dict[str, List[str]],\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    booking_discount has UNIQUE(booking_id, promotion_id)\n",
    "    Generate rows with unique (booking_id, promotion_id) pairs.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / f\"{table}.csv\"\n",
    "    colnames = [c.column for c in cols]\n",
    "    col_lc = {c.column.lower(): c.column for c in cols}\n",
    "    table_lc = table.lower()\n",
    "\n",
    "    booking_id_col = col_lc.get(\"booking_id\")\n",
    "    promo_id_col = col_lc.get(\"promotion_id\") or col_lc.get(\"promo_id\")\n",
    "\n",
    "    if not booking_id_col or not promo_id_col:\n",
    "        raise RuntimeError(\"booking_discount expected booking_id and promotion_id columns.\")\n",
    "\n",
    "    booking_ids = list(ref_ids.get(\"booking\", []))\n",
    "    promo_ids = list(ref_ids.get(\"promotion\", []))\n",
    "\n",
    "    if not booking_ids or not promo_ids:\n",
    "        raise RuntimeError(\"booking_discount needs booking + promotion ids loaded first.\")\n",
    "\n",
    "    seen: set[tuple[Any, Any]] = set()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    random.shuffle(booking_ids)\n",
    "\n",
    "    max_pairs = len(booking_ids) * len(promo_ids)\n",
    "    if n_rows > max_pairs:\n",
    "        raise RuntimeError(\n",
    "            f\"Requested {n_rows} booking_discount rows but only {max_pairs} unique (booking_id,promotion_id) pairs exist.\"\n",
    "        )\n",
    "\n",
    "    i = 0\n",
    "    while len(rows) < n_rows:\n",
    "        b = booking_ids[i % len(booking_ids)]\n",
    "        promos_for_booking = random.randint(0, 2)\n",
    "        if promos_for_booking == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        for _ in range(promos_for_booking):\n",
    "            p = random.choice(promo_ids)\n",
    "            pair = (b, p)\n",
    "            if pair in seen:\n",
    "                continue\n",
    "            seen.add(pair)\n",
    "\n",
    "            row: Dict[str, Any] = {booking_id_col: b, promo_id_col: p}\n",
    "\n",
    "            for c in cols:\n",
    "                if c.column in row:\n",
    "                    continue\n",
    "                fk_key = (table_lc, c.column)\n",
    "                if fk_key in fk_map:\n",
    "                    parent_table, _ = fk_map[fk_key]\n",
    "                    candidates = ref_ids.get(parent_table, [])\n",
    "                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n",
    "                else:\n",
    "                    v = generate_value(fake, c, len(rows) + 1, enums)\n",
    "                    if v is None and not c.is_nullable:\n",
    "                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n",
    "                    row[c.column] = v\n",
    "\n",
    "            rows.append(row)\n",
    "            if len(rows) >= n_rows:\n",
    "                break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i > n_rows * 20 and len(rows) < n_rows:\n",
    "            for b2 in booking_ids:\n",
    "                for p2 in promo_ids:\n",
    "                    if len(rows) >= n_rows:\n",
    "                        break\n",
    "                    pair = (b2, p2)\n",
    "                    if pair in seen:\n",
    "                        continue\n",
    "                    seen.add(pair)\n",
    "                    row = {booking_id_col: b2, promo_id_col: p2}\n",
    "                    for c in cols:\n",
    "                        if c.column in row:\n",
    "                            continue\n",
    "                        fk_key = (table_lc, c.column)\n",
    "                        if fk_key in fk_map:\n",
    "                            parent_table, _ = fk_map[fk_key]\n",
    "                            candidates = ref_ids.get(parent_table, [])\n",
    "                            row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n",
    "                        else:\n",
    "                            v = generate_value(fake, c, len(rows) + 1, enums)\n",
    "                            if v is None and not c.is_nullable:\n",
    "                                v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n",
    "                            row[c.column] = v\n",
    "                    rows.append(row)\n",
    "                if len(rows) >= n_rows:\n",
    "                    break\n",
    "\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(colnames)\n",
    "        for row in rows[:n_rows]:\n",
    "            w.writerow([row.get(cn) for cn in colnames])\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_booking_csv(\n",
    "    *,\n",
    "    fake: Faker,\n",
    "    out_dir: Path,\n",
    "    table: str,\n",
    "    cols: List[ColumnInfo],\n",
    "    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n",
    "    ref_ids: Dict[str, List[Any]],\n",
    "    n_rows: int,\n",
    "    enums: Dict[str, List[str]],\n",
    "    unique_cols: Dict[str, Set[str]],\n",
    "    pk: Optional[PrimaryKey],\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Enforces booking_check ordering (if columns exist): checkout_date >= checkin_date\n",
    "    Status is ENUM-safe.\n",
    "    Also supports UNIQUE(FK) pools for single-column unique FK constraints.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / f\"{table}.csv\"\n",
    "    colnames = [c.column for c in cols]\n",
    "    col_lc = {c.column.lower(): c.column for c in cols}\n",
    "    table_lc = table.lower()\n",
    "\n",
    "    checkin_col = col_lc.get(\"checkin_date\")\n",
    "    checkout_col = col_lc.get(\"checkout_date\")\n",
    "    status_col = col_lc.get(\"booking_status\") or col_lc.get(\"status\")\n",
    "\n",
    "    pk_col = pk.columns[0] if (pk and len(pk.columns) == 1) else None\n",
    "    pk_vals: List[Any] = []\n",
    "\n",
    "    status_ci = next((c for c in cols if c.column == (status_col or \"\")), None)\n",
    "    booking_status_choices = enums.get(status_ci.udt_name.lower(), []) if status_ci else []\n",
    "\n",
    "    fk_cols_in_table = {c.column for c in cols if (table_lc, c.column) in fk_map}\n",
    "    uniq_cols_in_table = unique_cols.get(table_lc, set())\n",
    "    unique_fk_cols = fk_cols_in_table.intersection(uniq_cols_in_table)\n",
    "\n",
    "    unique_fk_pools: Dict[str, List[Any]] = {}\n",
    "    for fk_col in unique_fk_cols:\n",
    "        parent_table, _ = fk_map[(table_lc, fk_col)]\n",
    "        parent_ids = list(ref_ids.get(parent_table, []))\n",
    "        random.shuffle(parent_ids)\n",
    "        unique_fk_pools[fk_col] = parent_ids[:n_rows]\n",
    "\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(colnames)\n",
    "\n",
    "        for i in range(1, n_rows + 1):\n",
    "            row: Dict[str, Any] = {}\n",
    "\n",
    "            if status_col and booking_status_choices:\n",
    "                row[status_col] = random.choice(booking_status_choices)\n",
    "\n",
    "            if checkin_col and checkout_col:\n",
    "                ci = fake.date_between(start_date=\"-180d\", end_date=\"+365d\")\n",
    "                co = ci + timedelta(days=random.randint(1, 14))\n",
    "                row[checkin_col] = ci\n",
    "                row[checkout_col] = co\n",
    "\n",
    "            for c in cols:\n",
    "                if c.column in row:\n",
    "                    continue\n",
    "\n",
    "                if pk_col and c.column == pk_col:\n",
    "                    v = generate_value(fake, c, i, enums)\n",
    "                    row[c.column] = v\n",
    "                    pk_vals.append(v)\n",
    "                    continue\n",
    "\n",
    "                fk_key = (table_lc, c.column)\n",
    "                if fk_key in fk_map:\n",
    "                    parent_table, _ = fk_map[fk_key]\n",
    "                    if c.column in unique_fk_pools and unique_fk_pools[c.column]:\n",
    "                        idx = i - 1\n",
    "                        row[c.column] = unique_fk_pools[c.column][idx] if idx < len(unique_fk_pools[c.column]) else (\n",
    "                            None if c.is_nullable else unique_fk_pools[c.column][-1]\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    candidates = ref_ids.get(parent_table, [])\n",
    "                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n",
    "                    continue\n",
    "\n",
    "                v = generate_value(fake, c, i, enums)\n",
    "                if v is None and not c.is_nullable:\n",
    "                    if c.data_type.lower() in {\"character varying\", \"character\", \"text\"}:\n",
    "                        v = unique_text((c.table, c.column), lambda: f\"VAL_{uuid.uuid4().hex[:6]}\")\n",
    "                    elif c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"}:\n",
    "                        v = 1\n",
    "                    elif c.data_type.lower() == \"boolean\":\n",
    "                        v = False\n",
    "                    elif c.data_type.lower() == \"date\":\n",
    "                        v = date.today()\n",
    "                    else:\n",
    "                        v = \"VAL\"\n",
    "                row[c.column] = v\n",
    "\n",
    "            if checkin_col and checkout_col:\n",
    "                ci = row.get(checkin_col)\n",
    "                co = row.get(checkout_col)\n",
    "                if ci is not None and co is not None and co < ci:\n",
    "                    row[checkout_col] = ci + timedelta(days=1)\n",
    "\n",
    "            w.writerow([row.get(cn) for cn in colnames])\n",
    "\n",
    "    # NOTE: we still track pk_vals here, but the main loop will overwrite with DB-truth anyway.\n",
    "    if pk_col:\n",
    "        ref_ids[table_lc] = pk_vals\n",
    "    return path\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main CSV generator\n",
    "# -------------------------\n",
    "def generate_table_csv(\n",
    "    *,\n",
    "    fake: Faker,\n",
    "    out_dir: Path,\n",
    "    table: str,\n",
    "    cols: List[ColumnInfo],\n",
    "    pk: Optional[PrimaryKey],\n",
    "    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n",
    "    ref_ids: Dict[str, List[Any]],\n",
    "    n_rows: int,\n",
    "    enums: Dict[str, List[str]],\n",
    "    unique_cols: Dict[str, Set[str]],\n",
    ") -> Path:\n",
    "    tl = table.lower()\n",
    "\n",
    "    if tl == \"booking_room\":\n",
    "        return generate_booking_room_csv(\n",
    "            fake=fake, out_dir=out_dir, table=table, cols=cols, fk_map=fk_map, ref_ids=ref_ids, n_rows=n_rows, enums=enums\n",
    "        )\n",
    "    if tl == \"room_night\":\n",
    "        return generate_room_night_csv(\n",
    "            fake=fake, out_dir=out_dir, table=table, cols=cols, fk_map=fk_map, ref_ids=ref_ids, n_rows=n_rows, enums=enums\n",
    "        )\n",
    "    if tl == \"stay\":\n",
    "        return generate_stay_csv(\n",
    "            fake=fake,\n",
    "            out_dir=out_dir,\n",
    "            table=table,\n",
    "            cols=cols,\n",
    "            fk_map=fk_map,\n",
    "            ref_ids=ref_ids,\n",
    "            n_rows=n_rows,\n",
    "            enums=enums,\n",
    "            unique_cols=unique_cols,\n",
    "        )\n",
    "    if tl == \"booking\":\n",
    "        return generate_booking_csv(\n",
    "            fake=fake,\n",
    "            out_dir=out_dir,\n",
    "            table=table,\n",
    "            cols=cols,\n",
    "            fk_map=fk_map,\n",
    "            ref_ids=ref_ids,\n",
    "            n_rows=n_rows,\n",
    "            enums=enums,\n",
    "            unique_cols=unique_cols,\n",
    "            pk=pk,\n",
    "        )\n",
    "    if tl == \"booking_discount\":\n",
    "        return generate_booking_discount_csv(\n",
    "            fake=fake,\n",
    "            out_dir=out_dir,\n",
    "            table=table,\n",
    "            cols=cols,\n",
    "            fk_map=fk_map,\n",
    "            ref_ids=ref_ids,\n",
    "            n_rows=n_rows,\n",
    "            enums=enums,\n",
    "        )\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / f\"{table}.csv\"\n",
    "    colnames = [c.column for c in cols]\n",
    "    table_lc = table.lower()\n",
    "\n",
    "    pk_col = pk.columns[0] if (pk and len(pk.columns) == 1) else None\n",
    "    pk_vals: List[Any] = []\n",
    "\n",
    "    # UNIQUE(FK) auto-detection for 1:1 tables\n",
    "    fk_cols_in_table = {c.column for c in cols if (table_lc, c.column) in fk_map}\n",
    "    uniq_cols_in_table = unique_cols.get(table_lc, set())\n",
    "    unique_fk_cols = fk_cols_in_table.intersection(uniq_cols_in_table)\n",
    "\n",
    "    unique_fk_pools: Dict[str, List[Any]] = {}\n",
    "    for fk_col in unique_fk_cols:\n",
    "        parent_table, _ = fk_map[(table_lc, fk_col)]\n",
    "        parent_ids = list(ref_ids.get(parent_table, []))\n",
    "        random.shuffle(parent_ids)\n",
    "        unique_fk_pools[fk_col] = parent_ids[:n_rows]\n",
    "\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(colnames)\n",
    "\n",
    "        # Start/end date coherence (generic)\n",
    "        col_lc = {c.column.lower(): c.column for c in cols}\n",
    "        start_keys = {\"start_date\", \"from_date\", \"valid_from\", \"effective_start_date\", \"block_start_date\"}\n",
    "        end_keys = {\"end_date\", \"to_date\", \"valid_to\", \"effective_end_date\", \"block_end_date\", \"expires_on\"}\n",
    "\n",
    "        start_col = next((col_lc[k] for k in start_keys if k in col_lc), None)\n",
    "        end_col = next((col_lc[k] for k in end_keys if k in col_lc), None)\n",
    "\n",
    "        for i in range(1, n_rows + 1):\n",
    "            row: Dict[str, Any] = {}\n",
    "\n",
    "            if start_col and end_col:\n",
    "                d_from = fake.date_between(start_date=\"-365d\", end_date=\"+365d\")\n",
    "                d_to = d_from + timedelta(days=random.randint(1, 60))\n",
    "                row[start_col] = d_from\n",
    "                row[end_col] = d_to\n",
    "\n",
    "            for c in cols:\n",
    "                if c.column in row:\n",
    "                    continue\n",
    "\n",
    "                # PK\n",
    "                if pk_col and c.column == pk_col:\n",
    "                    v = generate_value(fake, c, i, enums)\n",
    "                    row[c.column] = v\n",
    "                    pk_vals.append(v)\n",
    "                    continue\n",
    "\n",
    "                # FK\n",
    "                fk_key = (table_lc, c.column)\n",
    "                if fk_key in fk_map:\n",
    "                    parent_table, _ = fk_map[fk_key]\n",
    "\n",
    "                    # UNIQUE(FK): assign without replacement\n",
    "                    if c.column in unique_fk_pools and unique_fk_pools[c.column]:\n",
    "                        idx = i - 1\n",
    "                        row[c.column] = unique_fk_pools[c.column][idx] if idx < len(unique_fk_pools[c.column]) else (\n",
    "                            None if c.is_nullable else unique_fk_pools[c.column][-1]\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    candidates = ref_ids.get(parent_table, [])\n",
    "                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n",
    "                    continue\n",
    "\n",
    "                v = generate_value(fake, c, i, enums)\n",
    "                if v is None and not c.is_nullable:\n",
    "                    if c.data_type.lower() in {\"character varying\", \"character\", \"text\"}:\n",
    "                        v = unique_text((c.table, c.column), lambda: f\"VAL_{uuid.uuid4().hex[:6]}\")\n",
    "                    elif c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"}:\n",
    "                        v = 1\n",
    "                    elif c.data_type.lower() == \"boolean\":\n",
    "                        v = False\n",
    "                    elif c.data_type.lower() == \"date\":\n",
    "                        v = date.today()\n",
    "                    else:\n",
    "                        v = \"VAL\"\n",
    "                row[c.column] = v\n",
    "\n",
    "            w.writerow([row.get(cn) for cn in colnames])\n",
    "\n",
    "    # NOTE: main loop overwrites with DB-truth anyway.\n",
    "    if pk_col:\n",
    "        ref_ids[table_lc] = pk_vals\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# COPY load + TRUNCATE + PK cache\n",
    "# -------------------------\n",
    "def copy_csv_to_postgres(conn, schema: str, table: str, csv_path: Path, columns: List[str]):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f'SET search_path TO \"{schema}\"')\n",
    "        with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)\n",
    "            cols_sql = \", \".join([f'\"{c}\"' for c in columns])\n",
    "            cur.copy_expert(\n",
    "                f'COPY \"{table}\" ({cols_sql}) FROM STDIN WITH (FORMAT CSV)',\n",
    "                f,\n",
    "            )\n",
    "\n",
    "\n",
    "def truncate_tables(conn, schema: str, load_order: List[str]):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f'SET search_path TO \"{schema}\"')\n",
    "        for t in reversed(load_order):\n",
    "            cur.execute(f'TRUNCATE TABLE \"{t}\" RESTART IDENTITY CASCADE;')\n",
    "\n",
    "\n",
    "def cache_pk_values(conn, schema: str, table: str, pk: Optional[PrimaryKey], ref_ids: Dict[str, List[Any]]):\n",
    "    \"\"\"\n",
    "    ✅ After loading a table, cache its single-column PK values into ref_ids[table_lc].\n",
    "    This fixes cases like check_in.stay_id where stay ids must exist in ref_ids[\"stay\"].\n",
    "    \"\"\"\n",
    "    if not pk or len(pk.columns) != 1:\n",
    "        return\n",
    "\n",
    "    table_lc = table.lower()\n",
    "    pk_col = pk.columns[0]\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f'SET search_path TO \"{schema}\"')\n",
    "        cur.execute(f'SELECT \"{pk_col}\" FROM \"{table}\" ORDER BY \"{pk_col}\"')\n",
    "        ref_ids[table_lc] = [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MAIN\n",
    "# -------------------------\n",
    "def main():\n",
    "    random.seed(SEED)\n",
    "    fake = Faker()\n",
    "    Faker.seed(SEED)\n",
    "\n",
    "    conn = psycopg2.connect(pg_dsn(PG))\n",
    "    conn.autocommit = True\n",
    "    schema = PG.schema\n",
    "\n",
    "    tables = fetch_tables(conn, schema)\n",
    "    cols_by_table = fetch_columns(conn, schema)\n",
    "    pks = fetch_primary_keys(conn, schema)\n",
    "    fks = fetch_foreign_keys(conn, schema)\n",
    "    enums = fetch_enum_values(conn)\n",
    "    unique_cols = fetch_unique_columns(conn, schema)\n",
    "\n",
    "    fk_map = build_fk_map(fks)\n",
    "    load_order = topo_sort_tables(tables, fks)\n",
    "\n",
    "    rc = default_row_counts(tables)\n",
    "    for k, v in ROW_COUNTS_OVERRIDE.items():\n",
    "        if k in rc:\n",
    "            rc[k] = v\n",
    "\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Schema: {schema}\", flush=True)\n",
    "    print(f\"Tables: {len(tables)}\", flush=True)\n",
    "    print(f\"Enums detected: {len(enums)}\", flush=True)\n",
    "    print(f\"Output dir: {OUT_DIR.resolve()}\", flush=True)\n",
    "\n",
    "    if TRUNCATE_FIRST:\n",
    "        print(\"Truncating tables...\", flush=True)\n",
    "        truncate_tables(conn, schema, load_order)\n",
    "        print(\"Truncate done.\", flush=True)\n",
    "\n",
    "    # ✅ Always use lowercase keys in ref_ids\n",
    "    ref_ids: Dict[str, List[Any]] = {}\n",
    "\n",
    "    for t in load_order:\n",
    "        cols = cols_by_table.get(t, [])\n",
    "        if not cols:\n",
    "            continue\n",
    "        n = int(rc.get(t, 0))\n",
    "        if n <= 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"→ {t}: generating {n:,}\", flush=True)\n",
    "        csv_path = generate_table_csv(\n",
    "            fake=fake,\n",
    "            out_dir=OUT_DIR,\n",
    "            table=t,\n",
    "            cols=cols,\n",
    "            pk=pks.get(t),\n",
    "            fk_map=fk_map,\n",
    "            ref_ids=ref_ids,\n",
    "            n_rows=n,\n",
    "            enums=enums,\n",
    "            unique_cols=unique_cols,\n",
    "        )\n",
    "\n",
    "        print(f\"→ {t}: loading via COPY\", flush=True)\n",
    "        copy_csv_to_postgres(conn, schema, t, csv_path, [c.column for c in cols])\n",
    "        print(f\"✅ {t}: generated+loaded {n:,} rows\", flush=True)\n",
    "\n",
    "        # ✅ CRITICAL: cache real PK ids for downstream FK generation\n",
    "        cache_pk_values(conn, schema, t, pks.get(t), ref_ids)\n",
    "\n",
    "    conn.close()\n",
    "    print(\"✅ DONE\", flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2f89e-d29e-46bf-94a1-8ca06900b718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
