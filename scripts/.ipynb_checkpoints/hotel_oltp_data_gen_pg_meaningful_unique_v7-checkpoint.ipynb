{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc3d6c69-cf36-4e64-b773-404c505c3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: public\n",
      "Tables: 40\n",
      "Enums detected: 13\n",
      "Output dir: C:\\Users\\Alekya Kastury\\github\\hotel-analytics-platform\\scripts\\scripts\\data\\faker\n",
      "Truncating tables...\n",
      "Truncate done.\n",
      "\u2192 address: generating 2,000\n",
      "\u2192 address: loading via COPY\n",
      "\u2705 address: generated+loaded 2,000 rows\n",
      "\u2192 audit_log: generating 2,000\n",
      "\u2192 audit_log: loading via COPY\n",
      "\u2705 audit_log: generated+loaded 2,000 rows\n",
      "\u2192 channel: generating 12\n",
      "\u2192 channel: loading via COPY\n",
      "\u2705 channel: generated+loaded 12 rows\n",
      "\u2192 contact: generating 2,000\n",
      "\u2192 contact: loading via COPY\n",
      "\u2705 contact: generated+loaded 2,000 rows\n",
      "\u2192 customer: generating 30,000\n",
      "\u2192 customer: loading via COPY\n",
      "\u2705 customer: generated+loaded 30,000 rows\n",
      "\u2192 hotel: generating 12\n",
      "\u2192 hotel: loading via COPY\n",
      "\u2705 hotel: generated+loaded 12 rows\n",
      "\u2192 booking: generating 70,000\n",
      "\u2192 booking: loading via COPY\n",
      "\u2705 booking: generated+loaded 70,000 rows\n",
      "\u2192 booking_cancellation: generating 8,000\n",
      "\u2192 booking_cancellation: loading via COPY\n",
      "\u2705 booking_cancellation: generated+loaded 8,000 rows\n",
      "\u2192 booking_event: generating 70,000\n",
      "\u2192 booking_event: loading via COPY\n",
      "\u2705 booking_event: generated+loaded 70,000 rows\n",
      "\u2192 cancellation_policy: generating 50\n",
      "\u2192 cancellation_policy: loading via COPY\n",
      "\u2705 cancellation_policy: generated+loaded 50 rows\n",
      "\u2192 employee: generating 2,000\n",
      "\u2192 employee: loading via COPY\n",
      "\u2705 employee: generated+loaded 2,000 rows\n",
      "\u2192 identity_document: generating 2,000\n",
      "\u2192 identity_document: loading via COPY\n",
      "\u2705 identity_document: generated+loaded 2,000 rows\n",
      "\u2192 guest: generating 120,000\n",
      "\u2192 guest: loading via COPY\n",
      "\u2705 guest: generated+loaded 120,000 rows\n",
      "\u2192 invoice: generating 70,000\n",
      "\u2192 invoice: loading via COPY\n",
      "\u2705 invoice: generated+loaded 70,000 rows\n",
      "\u2192 invoice_line_item: generating 60,000\n",
      "\u2192 invoice_line_item: loading via COPY\n",
      "\u2705 invoice_line_item: generated+loaded 60,000 rows\n",
      "\u2192 no_show: generating 2,000\n",
      "\u2192 no_show: loading via COPY\n",
      "\u2705 no_show: generated+loaded 2,000 rows\n",
      "\u2192 payment: generating 60,000\n",
      "\u2192 payment: loading via COPY\n",
      "\u2705 payment: generated+loaded 60,000 rows\n",
      "\u2192 promotion: generating 2,000\n",
      "\u2192 promotion: loading via COPY\n",
      "\u2705 promotion: generated+loaded 2,000 rows\n",
      "\u2192 booking_discount: generating 70,000\n",
      "\u2192 booking_discount: loading via COPY\n",
      "\u2705 booking_discount: generated+loaded 70,000 rows\n",
      "\u2192 rate_plan: generating 50\n",
      "\u2192 rate_plan: loading via COPY\n",
      "\u2705 rate_plan: generated+loaded 50 rows\n",
      "\u2192 rate_calendar: generating 50\n",
      "\u2192 rate_calendar: loading via COPY\n",
      "\u2705 rate_calendar: generated+loaded 50 rows\n",
      "\u2192 refund: generating 8,000\n",
      "\u2192 refund: loading via COPY\n",
      "\u2705 refund: generated+loaded 8,000 rows\n",
      "\u2192 review: generating 2,000\n",
      "\u2192 review: loading via COPY\n",
      "\u2705 review: generated+loaded 2,000 rows\n",
      "\u2192 review_category: generating 2,000\n",
      "\u2192 review_category: loading via COPY\n",
      "\u2705 review_category: generated+loaded 2,000 rows\n",
      "\u2192 review_score: generating 2,000\n",
      "\u2192 review_score: loading via COPY\n",
      "\u2705 review_score: generated+loaded 2,000 rows\n",
      "\u2192 room_type: generating 50\n",
      "\u2192 room_type: loading via COPY\n",
      "\u2705 room_type: generated+loaded 50 rows\n",
      "\u2192 room: generating 1,000\n",
      "\u2192 room: loading via COPY\n",
      "\u2705 room: generated+loaded 1,000 rows\n",
      "\u2192 booking_room: generating 90,000\n",
      "\u2192 booking_room: loading via COPY\n",
      "\u2705 booking_room: generated+loaded 90,000 rows\n",
      "\u2192 housekeeping_task: generating 2,000\n",
      "\u2192 housekeeping_task: loading via COPY\n",
      "\u2705 housekeeping_task: generated+loaded 2,000 rows\n",
      "\u2192 maintenance_ticket: generating 2,000\n",
      "\u2192 maintenance_ticket: loading via COPY\n",
      "\u2705 maintenance_ticket: generated+loaded 2,000 rows\n",
      "\u2192 room_block: generating 2,000\n",
      "\u2192 room_block: loading via COPY\n",
      "\u2705 room_block: generated+loaded 2,000 rows\n",
      "\u2192 room_night: generating 2,000\n",
      "\u2192 room_night: loading via COPY\n",
      "\u2705 room_night: generated+loaded 2,000 rows\n",
      "\u2192 service_catalog: generating 50\n",
      "\u2192 service_catalog: loading via COPY\n",
      "\u2705 service_catalog: generated+loaded 50 rows\n",
      "\u2192 service_order: generating 2,000\n",
      "\u2192 service_order: loading via COPY\n",
      "\u2705 service_order: generated+loaded 2,000 rows\n",
      "\u2192 service_order_item: generating 2,000\n",
      "\u2192 service_order_item: loading via COPY\n",
      "\u2705 service_order_item: generated+loaded 2,000 rows\n",
      "\u2192 stay: generating 2,000\n",
      "\u2192 stay: loading via COPY\n",
      "\u2705 stay: generated+loaded 2,000 rows\n",
      "\u2192 check_in: generating 2,000\n",
      "\u2192 check_in: loading via COPY\n",
      "\u2705 check_in: generated+loaded 2,000 rows\n",
      "\u2192 check_out: generating 2,000\n",
      "\u2192 check_out: loading via COPY\n",
      "\u2705 check_out: generated+loaded 2,000 rows\n",
      "\u2192 stay_guest: generating 2,000\n",
      "\u2192 stay_guest: loading via COPY\n",
      "\u2705 stay_guest: generated+loaded 2,000 rows\n",
      "\u2192 tax_fee: generating 2,000\n",
      "\u2192 tax_fee: loading via COPY\n",
      "\u2705 tax_fee: generated+loaded 2,000 rows\n",
      "\u2705 DONE\n"
     ]
    }
   ],
   "source": [
    "# seed_faker_one_time.py\n# One-time Faker seeder for your Hotel OLTP Postgres schema.\n#\n# \u2705 Fixes included (based on your errors):\n# - Introspects schema: tables, columns, PK, FK, ENUMs, UNIQUE (single-column only)\n# - Loads in FK dependency order\n# - TRUNCATE ... RESTART IDENTITY CASCADE (optional)\n# - Generates FK-valid data\n# - Handles 1:1 tables automatically via UNIQUE(FK) without replacement\n# - Special-cases:\n#     - booking_room UNIQUE(booking_id, room_id)\n#     - room_night UNIQUE(room_id, night_date)\n#     - booking_discount UNIQUE(booking_id, promotion_id)\n#     - stay_check: actual_checkout_at >= actual_checkin_at when both present\n#     - booking_check: checkout_date >= checkin_date when both present\n# - ENUM-safe: never hardcode enum labels (samples from pg enums)\n#\n# \u2705 CRITICAL FIX (for your current crash):\n# - After loading EACH table, if it has a single-column PK, we query the DB and store the\n#   actual PK values into ref_ids[table_lc].\n#   This guarantees downstream FK tables (like check_in.stay_id) get real stay_id values,\n#   instead of falling back to \"1\" (which caused check_in_stay_id_key duplicates).\n#\n# Run:\n#   python seed_faker_one_time.py\n#\n# Requirements:\n#   pip install psycopg2-binary faker\n\nfrom __future__ import annotations\n\nimport csv\nimport random\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import date, timedelta, timezone\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nimport psycopg2\nfrom faker import Faker\n\n\n# =========================\n# CONFIG (edit these)\n# =========================\n@dataclass(frozen=True)\nclass PostgresCreds:\n    host: str\n    port: str\n    dbname: str\n    user: str\n    password: str\n    schema: str = \"public\"\n\n\nPG = PostgresCreds(\n    host=\"localhost\",\n    port=\"5432\",\n    dbname=\"hotel_oltp\",\n    user=\"postgres\",\n    password=\"postgres\",\n    schema=\"public\",\n)\n\nOUT_DIR = Path(\"scripts/data/faker\")  # change to \"data/faker\" if you want\nSEED = 42\n\n\n# -------------------------\n# MEANINGFUL DIMENSIONS\n# -------------------------\n# Faker's city/state are independent draws, which creates nonsense combos.\n# We keep a small, realistic location pool and sample consistently per-row.\n\nLOCATION_POOL = [\n    # US (mix of timezones)\n    {\"city\": \"Boston\", \"state\": \"MA\", \"country\": \"US\", \"postal_prefix\": \"02\", \"timezone\": \"America/New_York\"},\n    {\"city\": \"New York\", \"state\": \"NY\", \"country\": \"US\", \"postal_prefix\": \"10\", \"timezone\": \"America/New_York\"},\n    {\"city\": \"Chicago\", \"state\": \"IL\", \"country\": \"US\", \"postal_prefix\": \"60\", \"timezone\": \"America/Chicago\"},\n    {\"city\": \"Austin\", \"state\": \"TX\", \"country\": \"US\", \"postal_prefix\": \"78\", \"timezone\": \"America/Chicago\"},\n    {\"city\": \"Denver\", \"state\": \"CO\", \"country\": \"US\", \"postal_prefix\": \"80\", \"timezone\": \"America/Denver\"},\n    {\"city\": \"Seattle\", \"state\": \"WA\", \"country\": \"US\", \"postal_prefix\": \"98\", \"timezone\": \"America/Los_Angeles\"},\n    {\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"US\", \"postal_prefix\": \"94\", \"timezone\": \"America/Los_Angeles\"},\n    {\"city\": \"San Jose\", \"state\": \"CA\", \"country\": \"US\", \"postal_prefix\": \"95\", \"timezone\": \"America/Los_Angeles\"},\n    {\"city\": \"Miami\", \"state\": \"FL\", \"country\": \"US\", \"postal_prefix\": \"33\", \"timezone\": \"America/New_York\"},\n    # India (for variety)\n    {\"city\": \"Bengaluru\", \"state\": \"KA\", \"country\": \"IN\", \"postal_prefix\": \"560\", \"timezone\": \"Asia/Kolkata\"},\n    {\"city\": \"Hyderabad\", \"state\": \"TS\", \"country\": \"IN\", \"postal_prefix\": \"500\", \"timezone\": \"Asia/Kolkata\"},\n    {\"city\": \"Mumbai\", \"state\": \"MH\", \"country\": \"IN\", \"postal_prefix\": \"400\", \"timezone\": \"Asia/Kolkata\"},\n    {\"city\": \"Chennai\", \"state\": \"TN\", \"country\": \"IN\", \"postal_prefix\": \"600\", \"timezone\": \"Asia/Kolkata\"},\n]\n\nHOTEL_BRANDS = [\n    \"Marriott\", \"Hilton\", \"Hyatt\", \"Westin\", \"Sheraton\", \"Ibis\", \"Novotel\", \"Taj\", \"Oberoi\", \"Radisson\"\n]\n\nROOM_TYPE_NAMES = [\n    \"Standard King\", \"Standard Queen\", \"Deluxe King\", \"Deluxe Queen\",\n    \"Studio\", \"Junior Suite\", \"Executive Suite\", \"Family Suite\"\n]\n\n# Per-row cache so city/state/country/timezone stay consistent within a generated row.\n_ROW_LOCATION: dict[tuple[str, int], dict] = {}\n\n\ndef get_row_location(fake, table: str, row_idx: int) -> dict:\n    key = (table.lower(), row_idx)\n    loc = _ROW_LOCATION.get(key)\n    if loc is None:\n        loc = dict(random.choice(LOCATION_POOL))\n        # derive a plausible postal code\n        pfx = loc.get(\"postal_prefix\", \"\")\n        if loc[\"country\"] == \"US\":\n            loc[\"postal_code\"] = f\"{pfx}{random.randint(0, 999):03d}\"\n        else:  # IN\n            loc[\"postal_code\"] = f\"{pfx}{random.randint(0, 999):03d}\" if len(pfx) == 3 else f\"{pfx}{random.randint(0, 9999):04d}\"\n        # plausible address lines\n        loc[\"street1\"] = f\"{random.randint(10, 9999)} {fake.street_name()}\"\n        loc[\"street2\"] = random.choice([None, f\"Apt {random.randint(1, 999)}\", f\"Suite {random.randint(100, 1999)}\"])\n        _ROW_LOCATION[key] = loc\n    return loc\nTRUNCATE_FIRST = True\n\n# Override row counts (applied only if table exists)\nROW_COUNTS_OVERRIDE = {\n    \"booking\": 70_000,\n    \"booking_room\": 90_000,\n    \"guest\": 120_000,\n    \"invoice\": 70_000,\n    \"payment\": 60_000,\n    \"refund\": 8_000,\n    \"booking_cancellation\": 8_000,\n    \"room\": 1_000,\n    \"hotel\": 12,\n    \"customer\": 30_000,\n    \"channel\": 12,\n    \"promotion\": 2_000,\n}\n# =========================\n\n\ndef pg_dsn(pg: PostgresCreds) -> str:\n    return f\"host={pg.host} port={pg.port} dbname={pg.dbname} user={pg.user} password={pg.password}\"\n\n\n@dataclass(frozen=True)\nclass ColumnInfo:\n    table: str\n    column: str\n    data_type: str\n    udt_name: str\n    is_nullable: bool\n    char_max_len: Optional[int]\n    numeric_precision: Optional[int]\n    numeric_scale: Optional[int]\n\n\n@dataclass(frozen=True)\nclass ForeignKey:\n    table: str\n    column: str\n    ref_table: str\n    ref_column: str\n\n\n@dataclass(frozen=True)\nclass PrimaryKey:\n    table: str\n    columns: Tuple[str, ...]\n\n\n# -------------------------\n# Introspection\n# -------------------------\ndef fetch_tables(conn, schema: str) -> List[str]:\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            SELECT table_name\n            FROM information_schema.tables\n            WHERE table_schema = %s AND table_type='BASE TABLE'\n            ORDER BY table_name\n            \"\"\",\n            (schema,),\n        )\n        return [r[0] for r in cur.fetchall()]\n\n\ndef fetch_columns(conn, schema: str) -> Dict[str, List[ColumnInfo]]:\n    out: Dict[str, List[ColumnInfo]] = {}\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            SELECT\n              table_name,\n              column_name,\n              data_type,\n              udt_name,\n              is_nullable,\n              character_maximum_length,\n              numeric_precision,\n              numeric_scale\n            FROM information_schema.columns\n            WHERE table_schema = %s\n            ORDER BY table_name, ordinal_position\n            \"\"\",\n            (schema,),\n        )\n        for t, c, dt, udt, nul, cmax, prec, scale in cur.fetchall():\n            out.setdefault(t, []).append(\n                ColumnInfo(\n                    table=t,\n                    column=c,\n                    data_type=dt,\n                    udt_name=udt,\n                    is_nullable=(nul == \"YES\"),\n                    char_max_len=cmax,\n                    numeric_precision=prec,\n                    numeric_scale=scale,\n                )\n            )\n    return out\n\n\ndef fetch_primary_keys(conn, schema: str) -> Dict[str, PrimaryKey]:\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            SELECT tc.table_name, kcu.column_name, kcu.ordinal_position\n            FROM information_schema.table_constraints tc\n            JOIN information_schema.key_column_usage kcu\n              ON tc.constraint_name = kcu.constraint_name\n             AND tc.table_schema = kcu.table_schema\n            WHERE tc.table_schema = %s\n              AND tc.constraint_type = 'PRIMARY KEY'\n            ORDER BY tc.table_name, kcu.ordinal_position\n            \"\"\",\n            (schema,),\n        )\n        tmp: Dict[str, List[str]] = {}\n        for t, c, _ in cur.fetchall():\n            tmp.setdefault(t, []).append(c)\n    return {t: PrimaryKey(table=t, columns=tuple(cols)) for t, cols in tmp.items()}\n\n\ndef fetch_foreign_keys(conn, schema: str) -> List[ForeignKey]:\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            SELECT\n              tc.table_name,\n              kcu.column_name,\n              ccu.table_name AS ref_table_name,\n              ccu.column_name AS ref_column_name\n            FROM information_schema.table_constraints tc\n            JOIN information_schema.key_column_usage kcu\n              ON tc.constraint_name = kcu.constraint_name\n             AND tc.table_schema = kcu.table_schema\n            JOIN information_schema.constraint_column_usage ccu\n              ON ccu.constraint_name = tc.constraint_name\n             AND ccu.table_schema = tc.table_schema\n            WHERE tc.table_schema = %s\n              AND tc.constraint_type = 'FOREIGN KEY'\n            ORDER BY tc.table_name, kcu.column_name\n            \"\"\",\n            (schema,),\n        )\n        return [ForeignKey(t, c, rt, rc) for t, c, rt, rc in cur.fetchall()]\n\n\ndef fetch_enum_values(conn) -> Dict[str, List[str]]:\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            SELECT t.typname AS enum_name, e.enumlabel AS enum_value\n            FROM pg_type t\n            JOIN pg_enum e ON t.oid = e.enumtypid\n            ORDER BY t.typname, e.enumsortorder\n            \"\"\"\n        )\n        out: Dict[str, List[str]] = {}\n        for enum_name, enum_value in cur.fetchall():\n            out.setdefault(enum_name.lower(), []).append(enum_value)\n        return out\n\n\ndef fetch_unique_columns(conn, schema: str) -> Dict[str, Set[str]]:\n    \"\"\"\n    \u2705 ONLY single-column UNIQUE constraints: {table_lc: {col,...}}\n    Store table keys lowercased to avoid casing mismatches.\n    \"\"\"\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            WITH uniq AS (\n              SELECT\n                tc.table_schema,\n                tc.table_name,\n                tc.constraint_name,\n                COUNT(kcu.column_name) AS col_count\n              FROM information_schema.table_constraints tc\n              JOIN information_schema.key_column_usage kcu\n                ON tc.constraint_name = kcu.constraint_name\n               AND tc.table_schema = kcu.table_schema\n              WHERE tc.table_schema = %s\n                AND tc.constraint_type = 'UNIQUE'\n              GROUP BY 1,2,3\n            )\n            SELECT kcu.table_name, kcu.column_name\n            FROM uniq\n            JOIN information_schema.key_column_usage kcu\n              ON uniq.constraint_name = kcu.constraint_name\n             AND uniq.table_schema = kcu.table_schema\n            WHERE uniq.col_count = 1\n            \"\"\",\n            (schema,),\n        )\n        out: Dict[str, Set[str]] = {}\n        for t, c in cur.fetchall():\n            out.setdefault(t.lower(), set()).add(c)\n        return out\n\n\n# -------------------------\n# Dependency ordering\n# -------------------------\ndef topo_sort_tables(tables: List[str], fks: List[ForeignKey]) -> List[str]:\n    deps: Dict[str, Set[str]] = {t: set() for t in tables}\n    rdeps: Dict[str, Set[str]] = {t: set() for t in tables}\n\n    for fk in fks:\n        if fk.table in deps and fk.ref_table in deps:\n            deps[fk.table].add(fk.ref_table)\n            rdeps[fk.ref_table].add(fk.table)\n\n    q = sorted([t for t in tables if not deps[t]])\n    out: List[str] = []\n\n    while q:\n        n = q.pop(0)\n        out.append(n)\n        for m in sorted(rdeps[n]):\n            deps[m].discard(n)\n            if not deps[m] and m not in out and m not in q:\n                q.append(m)\n        q.sort()\n\n    remaining = [t for t in tables if t not in out]\n    return out + sorted(remaining)\n\n\n# -------------------------\n# Default row counts\n# -------------------------\ndef default_row_counts(tables: List[str]) -> Dict[str, int]:\n    rc: Dict[str, int] = {}\n    for t in tables:\n        tl = t.lower()\n        if any(k in tl for k in [\"lookup\", \"type\", \"status\", \"code\", \"catalog\", \"policy\", \"rate_plan\", \"rate_calendar\"]):\n            rc[t] = 50\n        elif tl == \"hotel\":\n            rc[t] = 12\n        elif tl == \"room\":\n            rc[t] = 1000\n        elif \"customer\" in tl:\n            rc[t] = 30_000\n        elif \"booking\" in tl:\n            rc[t] = 70_000\n        elif any(k in tl for k in [\"payment\", \"invoice\", \"transaction\", \"charge\"]):\n            rc[t] = 60_000\n        elif any(k in tl for k in [\"refund\", \"cancellation\"]):\n            rc[t] = 8_000\n        else:\n            rc[t] = 2_000\n    return rc\n\n\n# -------------------------\n# Unique text registry\n# -------------------------\n_UNIQUE_SEEN: Dict[Tuple[str, str], Set[str]] = {}\n\n\ndef unique_text(key: Tuple[str, str], make_value, max_tries: int = 200000) -> str:\n    seen = _UNIQUE_SEEN.setdefault(key, set())\n    for _ in range(max_tries):\n        v = str(make_value()).strip()\n        if v and v not in seen:\n            seen.add(v)\n            return v\n\n\n# Deterministic-ish unique choice from a small list (for UNIQUE(name) dimension tables)\n_UNIQUE_POOL_STATE: dict = {}\n\ndef unique_choice_from_pool(key, base_list, maxlen: int | None = None) -> str:\n    \"\"\"\n    Returns values from base_list without repeats until exhausted; then falls back to suffixing.\n    key: (table, column) or any hashable identifier\n    \"\"\"\n    st=_UNIQUE_POOL_STATE.get(key)\n    if st is None:\n        pool=list(base_list)\n        random.shuffle(pool)\n        st={\"pool\": pool}\n        _UNIQUE_POOL_STATE[key]=st\n    pool=st[\"pool\"]\n    if pool:\n        v=pool.pop()\n    else:\n        v=f\"{random.choice(base_list)}_{uuid.uuid4().hex[:6]}\"\n    if maxlen is not None:\n        return str(v)[:maxlen]\n    return str(v)\n    raise RuntimeError(f\"Could not generate unique value for {key}\")\n\n\n# -------------------------\n# Value generator\n# -------------------------\ndef generate_value(fake: Faker, col: ColumnInfo, row_idx: int, enums: Dict[str, List[str]]) -> Any:\n    name = col.column.lower()\n    dt = col.data_type.lower()\n    udt = col.udt_name.lower()\n\n    # ENUM\n    if udt in enums:\n        if col.is_nullable and random.random() < 0.03:\n            return None\n        return random.choice(enums[udt])\n\n    # standard timestamps\n    if name in {\"created_at\", \"updated_at\", \"loaded_at\", \"ingested_at\"}:\n        base = fake.date_time_between(start_date=\"-2y\", end_date=\"now\", tzinfo=timezone.utc)\n        if name == \"updated_at\":\n            base = base + timedelta(days=random.randint(0, 180))\n        return base\n\n    # date\n    if dt == \"date\":\n        return fake.date_between(start_date=\"-2y\", end_date=\"+1y\")\n\n    if dt in {\"timestamp without time zone\", \"timestamp with time zone\"} or udt in {\"timestamp\", \"timestamptz\"}:\n        return fake.date_time_between(start_date=\"-2y\", end_date=\"now\", tzinfo=timezone.utc)\n\n    # int\n    if dt in {\"integer\", \"bigint\", \"smallint\"} or udt in {\"int2\", \"int4\", \"int8\"}:\n        if name.endswith(\"_id\"):\n            return row_idx\n        if name == \"score\":\n            return random.randint(1, 5)\n        if any(k in name for k in [\"rating\", \"stars\", \"score\"]):\n            return random.randint(1, 5)\n        if any(k in name for k in [\"count\", \"qty\", \"quantity\", \"nights\", \"floor\", \"occupancy\"]):\n            return random.randint(1, 10)\n        return random.randint(1, 100000)\n\n    # uuid\n    if dt == \"uuid\" or udt == \"uuid\":\n        return str(uuid.uuid4())\n\n    # bool\n    if dt == \"boolean\":\n        return random.random() < 0.85 if (\"is_\" in name or name.endswith(\"_flag\")) else (random.random() < 0.5)\n\n    # numeric/decimal\n    if dt in {\"numeric\", \"decimal\"} or udt == \"numeric\":\n        scale = col.numeric_scale or 2\n        if \"percent\" in name or name.endswith(\"_pct\") or name.endswith(\"pct\"):\n            return round(random.uniform(0, 100), scale)\n        if \"ratio\" in name or \"fraction\" in name:\n            return round(random.uniform(0, 1), scale)\n        if col.table.lower() == \"promotion\" and name in {\"value\", \"discount_value\", \"discount_amount\", \"discount\"}:\n            return round(random.uniform(5, 50), scale)\n        if any(k in name for k in [\"amount\", \"price\", \"rate\", \"cost\", \"fee\", \"total\", \"tax\"]):\n            return round(random.uniform(20, 2000), scale)\n        return round(random.uniform(0, 1000), scale)\n\n    # text/varchar\n    if dt in {\"character varying\", \"character\", \"text\"}:\n        maxlen = col.char_max_len or 255\n        # location-aware fields (keep city/state/country/timezone consistent per row)\n        if name in {\"city\", \"state\", \"country\", \"postal_code\", \"zipcode\", \"zip\"} or \"timezone\" in name or name in {\"address_line1\", \"address_line2\", \"street\", \"street1\", \"street2\"}:\n            loc = get_row_location(fake, col.table, row_idx)\n            if \"timezone\" in name:\n                return loc.get(\"timezone\", \"America/New_York\")[:maxlen]\n            if name in {\"city\"}:\n                return loc[\"city\"][:maxlen]\n            if name in {\"state\"}:\n                return loc[\"state\"][:maxlen]\n            if name in {\"country\"}:\n                return loc[\"country\"][:maxlen]\n            if name in {\"postal_code\", \"zipcode\", \"zip\"}:\n                return str(loc.get(\"postal_code\", \"00000\"))[:maxlen]\n            if name in {\"address_line1\", \"street\", \"street1\"}:\n                return loc.get(\"street1\")[:maxlen]\n            if name in {\"address_line2\", \"street2\"}:\n                v = loc.get(\"street2\")\n                return (\"\" if v is None else str(v))[:maxlen]\n\n        # more meaningful domain strings\n        if col.table.lower() == \"hotel\" and name in {\"name\", \"hotel_name\"}:\n            brand = random.choice(HOTEL_BRANDS)\n            loc = get_row_location(fake, col.table, row_idx)\n            base = f\"{brand} {loc['city']}\"\n            suffix = random.choice([\"Hotel\", \"Resort\", \"Suites\", \"Inn\"])\n            return f\"{base} {suffix}\"[:maxlen]\n        if col.table.lower() == \"room_type\" and name in {\"name\", \"room_type_name\"}:\n            return unique_choice_from_pool((col.table, col.column), ROOM_TYPE_NAMES, maxlen)\n        if name in {\"phone\", \"phone_number\"}:\n            return fake.phone_number()[:maxlen]\n        if name in {\"currency\", \"currency_code\"}:\n            return random.choice([\"USD\", \"INR\"])[:maxlen]\n        if name in {\"state_code\", \"state_abbr\"}:\n            loc = get_row_location(fake, col.table, row_idx)\n            return loc.get(\"state\", \"NA\")[:maxlen]\n        if \"timezone\" in name:\n            # fallback if location table not used in this row\n            return \"America/New_York\"[:maxlen]\n        if name == \"email\":\n            return unique_text((col.table, col.column), lambda: fake.email())[:maxlen]\n        if name.endswith(\"_name\") or name in {\"name\", \"code\"}:\n            return unique_text((col.table, col.column), lambda: f\"{fake.word().title()}_{uuid.uuid4().hex[:6]}\")[:maxlen]\n        if \"timezone\" in name:\n            return \"America/New_York\"[:maxlen]\n        if maxlen <= 20:\n            return fake.word()[:maxlen]\n        if maxlen <= 80:\n            return fake.sentence(nb_words=6)[:maxlen]\n        return fake.sentence(nb_words=10)[:maxlen]\n\n    return None\n\n\ndef build_fk_map(fks: List[ForeignKey]) -> Dict[Tuple[str, str], Tuple[str, str]]:\n    \"\"\"\n    \u2705 Normalize table keys to lowercase to avoid casing mismatches.\n    \"\"\"\n    return {(fk.table.lower(), fk.column): (fk.ref_table.lower(), fk.ref_column) for fk in fks}\n\n\n# -------------------------\n# Special-case generators\n# -------------------------\ndef generate_booking_room_csv(\n    *,\n    fake: Faker,\n    out_dir: Path,\n    table: str,\n    cols: List[ColumnInfo],\n    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n    ref_ids: Dict[str, List[Any]],\n    n_rows: int,\n    enums: Dict[str, List[str]],\n) -> Path:\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"{table}.csv\"\n    colnames = [c.column for c in cols]\n    col_lc = {c.column.lower(): c.column for c in cols}\n    table_lc = table.lower()\n\n    booking_ids = list(ref_ids.get(\"booking\", []))\n    room_ids = list(ref_ids.get(\"room\", []))\n    if not booking_ids or not room_ids:\n        raise RuntimeError(\"booking_room needs booking and room ids available before generation.\")\n\n    booking_id_col = col_lc.get(\"booking_id\")\n    room_id_col = col_lc.get(\"room_id\")\n    if not booking_id_col or not room_id_col:\n        raise RuntimeError(\"booking_room expected columns booking_id and room_id.\")\n\n    seen: set[tuple[Any, Any]] = set()\n    rows: List[Dict[str, Any]] = []\n\n    random.shuffle(booking_ids)\n\n    i = 0\n    while len(rows) < n_rows:\n        b = booking_ids[i % len(booking_ids)]\n        rooms_for_booking = random.randint(1, 3)\n        for _ in range(rooms_for_booking):\n            r = random.choice(room_ids)\n            pair = (b, r)\n            if pair in seen:\n                continue\n            seen.add(pair)\n\n            row: Dict[str, Any] = {booking_id_col: b, room_id_col: r}\n            for c in cols:\n                if c.column in row:\n                    continue\n                fk_key = (table_lc, c.column)\n                if fk_key in fk_map:\n                    parent_table, _ = fk_map[fk_key]\n                    candidates = ref_ids.get(parent_table, [])\n                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                else:\n                    v = generate_value(fake, c, len(rows) + 1, enums)\n                    if v is None and not c.is_nullable:\n                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n                    row[c.column] = v\n\n            rows.append(row)\n            if len(rows) >= n_rows:\n                break\n        i += 1\n\n    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(colnames)\n        for row in rows[:n_rows]:\n            w.writerow([row.get(cn) for cn in colnames])\n\n    return path\n\n\ndef generate_room_night_csv(\n    *,\n    fake: Faker,\n    out_dir: Path,\n    table: str,\n    cols: List[ColumnInfo],\n    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n    ref_ids: Dict[str, List[Any]],\n    n_rows: int,\n    enums: Dict[str, List[str]],\n) -> Path:\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"{table}.csv\"\n    colnames = [c.column for c in cols]\n    col_lc = {c.column.lower(): c.column for c in cols}\n    table_lc = table.lower()\n\n    room_ids = list(ref_ids.get(\"room\", []))\n    if not room_ids:\n        raise RuntimeError(\"room_night needs room ids available before generation.\")\n\n    room_id_col = col_lc.get(\"room_id\")\n    night_date_col = col_lc.get(\"night_date\")\n    if not room_id_col or not night_date_col:\n        raise RuntimeError(\"room_night expected columns room_id and night_date.\")\n\n    start = date.today() - timedelta(days=730)\n    end = date.today() + timedelta(days=365)\n    total_days = (end - start).days\n\n    seen: set[tuple[Any, date]] = set()\n    rows: List[Dict[str, Any]] = []\n\n    per_room = max(1, n_rows // len(room_ids))\n    random.shuffle(room_ids)\n\n    for rid in room_ids:\n        if len(rows) >= n_rows:\n            break\n        k = min(per_room, total_days)\n        offsets = random.sample(range(total_days), k=k)\n        for off in offsets:\n            nd = start + timedelta(days=off)\n            pair = (rid, nd)\n            if pair in seen:\n                continue\n            seen.add(pair)\n\n            row: Dict[str, Any] = {room_id_col: rid, night_date_col: nd}\n            for c in cols:\n                if c.column in row:\n                    continue\n                fk_key = (table_lc, c.column)\n                if fk_key in fk_map:\n                    parent_table, _ = fk_map[fk_key]\n                    candidates = ref_ids.get(parent_table, [])\n                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                else:\n                    v = generate_value(fake, c, len(rows) + 1, enums)\n                    if v is None and not c.is_nullable:\n                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n                    row[c.column] = v\n\n            rows.append(row)\n            if len(rows) >= n_rows:\n                break\n\n    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(colnames)\n        for row in rows[:n_rows]:\n            w.writerow([row.get(cn) for cn in colnames])\n\n    return path\n\n\ndef generate_stay_csv(\n    *,\n    fake: Faker,\n    out_dir: Path,\n    table: str,\n    cols: List[ColumnInfo],\n    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n    ref_ids: Dict[str, List[Any]],\n    n_rows: int,\n    enums: Dict[str, List[str]],\n    unique_cols: Dict[str, Set[str]],\n) -> Path:\n    \"\"\"\n    Enforces:\n      actual_checkout_at IS NULL OR actual_checkin_at IS NULL OR actual_checkout_at >= actual_checkin_at\n\n    \u2705 UNIQUE(FK): if stay.booking_id is UNIQUE, assign booking_id without replacement.\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"{table}.csv\"\n    colnames = [c.column for c in cols]\n    col_lc = {c.column.lower(): c.column for c in cols}\n    table_lc = table.lower()\n\n    booking_id_col = col_lc.get(\"booking_id\")\n    status_col = col_lc.get(\"stay_status\") or col_lc.get(\"status\")\n    aci_col = col_lc.get(\"actual_checkin_at\")\n    aco_col = col_lc.get(\"actual_checkout_at\")\n\n    booking_ids = list(ref_ids.get(\"booking\", []))\n\n    is_booking_unique = bool(booking_id_col and booking_id_col in unique_cols.get(table_lc, set()))\n    if is_booking_unique:\n        if len(booking_ids) < n_rows:\n            raise RuntimeError(\n                f'{table_lc}.\"{booking_id_col}\" is UNIQUE but only {len(booking_ids)} booking ids exist '\n                f\"for requested n_rows={n_rows}.\"\n            )\n        random.shuffle(booking_ids)\n\n    status_ci = next((c for c in cols if c.column == (status_col or \"\")), None)\n    stay_status_choices = enums.get(status_ci.udt_name.lower(), []) if status_ci else []\n\n    uniq_cols_in_table: Set[str] = set(unique_cols.get(table_lc, set()))\n    seen_uniques: Dict[str, Set[Any]] = {c: set() for c in uniq_cols_in_table}\n\n    def _register_unique(col: str, val: Any) -> bool:\n        if val is None:\n            return True\n        s = seen_uniques.get(col)\n        if s is None:\n            return True\n        if val in s:\n            return False\n        s.add(val)\n        return True\n\n\n    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(colnames)\n\n        for i in range(1, n_rows + 1):\n            row: Dict[str, Any] = {}\n\n            if booking_id_col:\n                if is_booking_unique:\n                    row[booking_id_col] = booking_ids[i - 1]\n                else:\n                    row[booking_id_col] = random.choice(booking_ids) if booking_ids else 1\n\n            scenario = None\n            if status_col and stay_status_choices:\n                scenario = random.choice(stay_status_choices)\n                row[status_col] = scenario\n\n            s = (scenario or \"\").upper()\n            is_cancel = \"CANCEL\" in s\n            is_out = \"OUT\" in s\n            is_in = (\"IN\" in s) and not is_out\n\n            if is_cancel:\n                if aci_col:\n                    row[aci_col] = None\n                if aco_col:\n                    row[aco_col] = None\n            elif is_out:\n                ci = fake.date_time_between(start_date=\"-180d\", end_date=\"now\", tzinfo=timezone.utc)\n                co = ci + timedelta(days=random.randint(1, 10), hours=random.randint(0, 6), minutes=random.randint(0, 59))\n                if aci_col:\n                    row[aci_col] = ci\n                if aco_col:\n                    row[aco_col] = co\n            elif is_in:\n                ci = fake.date_time_between(start_date=\"-180d\", end_date=\"now\", tzinfo=timezone.utc)\n                if aci_col:\n                    row[aci_col] = ci\n                if aco_col:\n                    row[aco_col] = None\n            else:\n                if aci_col:\n                    row[aci_col] = None\n                if aco_col:\n                    row[aco_col] = None\n\n            for c in cols:\n                if c.column in row:\n                    continue\n                fk_key = (table_lc, c.column)\n                if fk_key in fk_map:\n                    parent_table, _ = fk_map[fk_key]\n                    candidates = ref_ids.get(parent_table, [])\n                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                else:\n                    v = generate_value(fake, c, i, enums)\n\n                # Enforce single-column UNIQUE constraints (safe for NULLs)\n                if c.column in seen_uniques and v is not None:\n                    tries = 0\n                    max_tries = 50\n                    while v in seen_uniques[c.column] and tries < max_tries:\n                        tries += 1\n                        v = generate_value(fake, c, i + tries, enums)\n\n                    if v in seen_uniques[c.column]:\n                        # Force uniqueness deterministically as a last resort\n                        if isinstance(v, str):\n                            suffix = uuid.uuid4().hex[:6]\n                            maxlen = c.character_maximum_length or 255\n                            keep = max(1, maxlen - (1 + len(suffix)))\n                            v = (str(v)[:keep] + \"_\" + suffix)[:maxlen]\n                        elif isinstance(v, int):\n                            v = int(v) + (i * 1000) + tries\n                        else:\n                            v = f\"{v}_{uuid.uuid4().hex[:6]}\"\n\n                    seen_uniques[c.column].add(v)\n                if v is None and not c.is_nullable:\n                    if c.data_type.lower() in {\"character varying\", \"character\", \"text\"}:\n                        v = unique_text((c.table, c.column), lambda: f\"VAL_{uuid.uuid4().hex[:6]}\")\n                    elif c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"}:\n                        v = 1\n                    elif c.data_type.lower() == \"boolean\":\n                        v = False\n                    elif c.data_type.lower() == \"date\":\n                        v = date.today()\n                    else:\n                        v = \"VAL\"\n\n                # Enforce UNIQUE(column) constraints (single-column) to prevent COPY UniqueViolation\n                if c.column in uniq_cols_in_table:\n                    # Try a few regenerations before forcing uniqueness\n                    for _ in range(50):\n                        if _register_unique(c.column, v):\n                            break\n                        v = generate_value(fake, c, i, enums)\n\n                # Enforce single-column UNIQUE constraints (safe for NULLs)\n                if c.column in seen_uniques and v is not None:\n                    tries = 0\n                    max_tries = 50\n                    while v in seen_uniques[c.column] and tries < max_tries:\n                        tries += 1\n                        v = generate_value(fake, c, i + tries, enums)\n\n                    if v in seen_uniques[c.column]:\n                        # Force uniqueness deterministically as a last resort\n                        if isinstance(v, str):\n                            suffix = uuid.uuid4().hex[:6]\n                            maxlen = c.character_maximum_length or 255\n                            keep = max(1, maxlen - (1 + len(suffix)))\n                            v = (str(v)[:keep] + \"_\" + suffix)[:maxlen]\n                        elif isinstance(v, int):\n                            v = int(v) + (i * 1000) + tries\n                        else:\n                            v = f\"{v}_{uuid.uuid4().hex[:6]}\"\n\n                    seen_uniques[c.column].add(v)\n\n                row[c.column] = v\n\n            if aci_col and aco_col:\n                ci = row.get(aci_col)\n                co = row.get(aco_col)\n                if ci is not None and co is not None and co < ci:\n                    row[aco_col] = ci + timedelta(days=1)\n\n            w.writerow([row.get(cn) for cn in colnames])\n\n    return path\n\n\ndef generate_booking_discount_csv(\n    *,\n    fake: Faker,\n    out_dir: Path,\n    table: str,\n    cols: List[ColumnInfo],\n    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n    ref_ids: Dict[str, List[Any]],\n    n_rows: int,\n    enums: Dict[str, List[str]],\n) -> Path:\n    \"\"\"\n    booking_discount has UNIQUE(booking_id, promotion_id)\n    Generate rows with unique (booking_id, promotion_id) pairs.\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"{table}.csv\"\n    colnames = [c.column for c in cols]\n    col_lc = {c.column.lower(): c.column for c in cols}\n    table_lc = table.lower()\n\n    booking_id_col = col_lc.get(\"booking_id\")\n    promo_id_col = col_lc.get(\"promotion_id\") or col_lc.get(\"promo_id\")\n\n    if not booking_id_col or not promo_id_col:\n        raise RuntimeError(\"booking_discount expected booking_id and promotion_id columns.\")\n\n    booking_ids = list(ref_ids.get(\"booking\", []))\n    promo_ids = list(ref_ids.get(\"promotion\", []))\n\n    if not booking_ids or not promo_ids:\n        raise RuntimeError(\"booking_discount needs booking + promotion ids loaded first.\")\n\n    seen: set[tuple[Any, Any]] = set()\n    rows: List[Dict[str, Any]] = []\n\n    random.shuffle(booking_ids)\n\n    max_pairs = len(booking_ids) * len(promo_ids)\n    if n_rows > max_pairs:\n        raise RuntimeError(\n            f\"Requested {n_rows} booking_discount rows but only {max_pairs} unique (booking_id,promotion_id) pairs exist.\"\n        )\n\n    i = 0\n    while len(rows) < n_rows:\n        b = booking_ids[i % len(booking_ids)]\n        promos_for_booking = random.randint(0, 2)\n        if promos_for_booking == 0:\n            i += 1\n            continue\n\n        for _ in range(promos_for_booking):\n            p = random.choice(promo_ids)\n            pair = (b, p)\n            if pair in seen:\n                continue\n            seen.add(pair)\n\n            row: Dict[str, Any] = {booking_id_col: b, promo_id_col: p}\n\n            for c in cols:\n                if c.column in row:\n                    continue\n                fk_key = (table_lc, c.column)\n                if fk_key in fk_map:\n                    parent_table, _ = fk_map[fk_key]\n                    candidates = ref_ids.get(parent_table, [])\n                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                else:\n                    v = generate_value(fake, c, len(rows) + 1, enums)\n                    if v is None and not c.is_nullable:\n                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n                    row[c.column] = v\n\n            rows.append(row)\n            if len(rows) >= n_rows:\n                break\n\n        i += 1\n\n        if i > n_rows * 20 and len(rows) < n_rows:\n            for b2 in booking_ids:\n                for p2 in promo_ids:\n                    if len(rows) >= n_rows:\n                        break\n                    pair = (b2, p2)\n                    if pair in seen:\n                        continue\n                    seen.add(pair)\n                    row = {booking_id_col: b2, promo_id_col: p2}\n                    for c in cols:\n                        if c.column in row:\n                            continue\n                        fk_key = (table_lc, c.column)\n                        if fk_key in fk_map:\n                            parent_table, _ = fk_map[fk_key]\n                            candidates = ref_ids.get(parent_table, [])\n                            row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                        else:\n                            v = generate_value(fake, c, len(rows) + 1, enums)\n                            if v is None and not c.is_nullable:\n                                v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n                            row[c.column] = v\n                    rows.append(row)\n                if len(rows) >= n_rows:\n                    break\n\n    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(colnames)\n        for row in rows[:n_rows]:\n            w.writerow([row.get(cn) for cn in colnames])\n\n    return path\n\n\ndef generate_review_score_csv(\n    *,\n    fake: Faker,\n    out_dir: Path,\n    table: str,\n    cols: List[ColumnInfo],\n    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n    ref_ids: Dict[str, List[Any]],\n    n_rows: int,\n    enums: Dict[str, List[str]],\n) -> Path:\n    \"\"\"\n    review_score has UNIQUE(review_id, review_category_id)\n    Generate rows with unique (review_id, review_category_id) pairs.\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"{table}.csv\"\n    colnames = [c.column for c in cols]\n    col_lc = {c.column.lower(): c.column for c in cols}\n    table_lc = table.lower()\n\n    review_id_col = col_lc.get(\"review_id\")\n    category_id_col = col_lc.get(\"review_category_id\") or col_lc.get(\"category_id\")\n\n    if not review_id_col or not category_id_col:\n        raise RuntimeError(\"review_score expected review_id and review_category_id columns.\")\n\n    review_ids = list(ref_ids.get(\"review\", []))\n    category_ids = list(ref_ids.get(\"review_category\", []))\n\n    if not review_ids or not category_ids:\n        raise RuntimeError(\"review_score needs review + review_category ids loaded first.\")\n\n    # Cap to maximum unique pairs available\n    max_pairs = len(review_ids) * len(category_ids)\n    if n_rows > max_pairs:\n        raise RuntimeError(\n            f\"Requested {n_rows} review_score rows but only {max_pairs} unique (review_id,review_category_id) pairs exist.\"\n        )\n\n    seen: set[tuple[Any, Any]] = set()\n    rows: List[Dict[str, Any]] = []\n\n    # Generate by sampling categories per review without replacement to guarantee uniqueness.\n    random.shuffle(review_ids)\n    i = 0\n    while len(rows) < n_rows:\n        rid = review_ids[i % len(review_ids)]\n        # 1-5 categories per review (bounded by available categories)\n        k = random.randint(1, min(5, len(category_ids)))\n        for cid in random.sample(category_ids, k=k):\n            if len(rows) >= n_rows:\n                break\n            pair = (rid, cid)\n            if pair in seen:\n                continue\n            seen.add(pair)\n\n            row: Dict[str, Any] = {review_id_col: rid, category_id_col: cid}\n\n            for c in cols:\n                if c.column in row:\n                    continue\n                fk_key = (table_lc, c.column)\n                if fk_key in fk_map:\n                    parent_table, _ = fk_map[fk_key]\n                    candidates = ref_ids.get(parent_table, [])\n                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                else:\n                    v = generate_value(fake, c, len(rows) + 1, enums)\n                    if v is None and not c.is_nullable:\n                        v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n                    row[c.column] = v\n\n            rows.append(row)\n\n        i += 1\n\n        # Fallback: if somehow not enough rows (very unlikely), fill deterministically over the cartesian product\n        if i > n_rows * 10 and len(rows) < n_rows:\n            for rid2 in review_ids:\n                for cid2 in category_ids:\n                    if len(rows) >= n_rows:\n                        break\n                    pair = (rid2, cid2)\n                    if pair in seen:\n                        continue\n                    seen.add(pair)\n                    row = {review_id_col: rid2, category_id_col: cid2}\n                    for c in cols:\n                        if c.column in row:\n                            continue\n                        fk_key = (table_lc, c.column)\n                        if fk_key in fk_map:\n                            parent_table, _ = fk_map[fk_key]\n                            candidates = ref_ids.get(parent_table, [])\n                            row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                        else:\n                            v = generate_value(fake, c, len(rows) + 1, enums)\n                            if v is None and not c.is_nullable:\n                                v = 1 if c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"} else f\"VAL_{uuid.uuid4().hex[:6]}\"\n                            row[c.column] = v\n                    rows.append(row)\n                if len(rows) >= n_rows:\n                    break\n            break\n\n    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(colnames)\n        for r in rows:\n            w.writerow([r.get(cn) for cn in colnames])\n\n    return path\n\n\n\ndef generate_booking_csv(\n    *,\n    fake: Faker,\n    out_dir: Path,\n    table: str,\n    cols: List[ColumnInfo],\n    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n    ref_ids: Dict[str, List[Any]],\n    n_rows: int,\n    enums: Dict[str, List[str]],\n    unique_cols: Dict[str, Set[str]],\n    pk: Optional[PrimaryKey],\n) -> Path:\n    \"\"\"\n    Enforces booking_check ordering (if columns exist): checkout_date >= checkin_date\n    Status is ENUM-safe.\n    Also supports UNIQUE(FK) pools for single-column unique FK constraints.\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"{table}.csv\"\n    colnames = [c.column for c in cols]\n    col_lc = {c.column.lower(): c.column for c in cols}\n    table_lc = table.lower()\n\n    checkin_col = col_lc.get(\"checkin_date\")\n    checkout_col = col_lc.get(\"checkout_date\")\n    status_col = col_lc.get(\"booking_status\") or col_lc.get(\"status\")\n\n    pk_col = pk.columns[0] if (pk and len(pk.columns) == 1) else None\n    pk_vals: List[Any] = []\n\n    status_ci = next((c for c in cols if c.column == (status_col or \"\")), None)\n    booking_status_choices = enums.get(status_ci.udt_name.lower(), []) if status_ci else []\n\n    fk_cols_in_table = {c.column for c in cols if (table_lc, c.column) in fk_map}\n    uniq_cols_in_table = unique_cols.get(table_lc, set())\n    unique_fk_cols = fk_cols_in_table.intersection(uniq_cols_in_table)\n\n    unique_fk_pools: Dict[str, List[Any]] = {}\n    for fk_col in unique_fk_cols:\n        parent_table, _ = fk_map[(table_lc, fk_col)]\n        parent_ids = list(ref_ids.get(parent_table, []))\n        random.shuffle(parent_ids)\n        unique_fk_pools[fk_col] = parent_ids[:n_rows]\n\n    # Track seen values for single-column UNIQUE constraints\n    seen_uniques: Dict[str, Set[Any]] = {c: set() for c in uniq_cols_in_table}\n\n\n\n    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(colnames)\n\n        for i in range(1, n_rows + 1):\n            row: Dict[str, Any] = {}\n\n            if status_col and booking_status_choices:\n                row[status_col] = random.choice(booking_status_choices)\n\n            if checkin_col and checkout_col:\n                ci = fake.date_between(start_date=\"-180d\", end_date=\"+365d\")\n                co = ci + timedelta(days=random.randint(1, 14))\n                row[checkin_col] = ci\n                row[checkout_col] = co\n\n            for c in cols:\n                if c.column in row:\n                    continue\n\n                if pk_col and c.column == pk_col:\n                    v = generate_value(fake, c, i, enums)\n\n                # Enforce single-column UNIQUE constraints (safe for NULLs)\n                if c.column in seen_uniques and v is not None:\n                    tries = 0\n                    max_tries = 50\n                    while v in seen_uniques[c.column] and tries < max_tries:\n                        tries += 1\n                        v = generate_value(fake, c, i + tries, enums)\n\n                    if v in seen_uniques[c.column]:\n                        # Force uniqueness deterministically as a last resort\n                        if isinstance(v, str):\n                            suffix = uuid.uuid4().hex[:6]\n                            maxlen = c.character_maximum_length or 255\n                            keep = max(1, maxlen - (1 + len(suffix)))\n                            v = (str(v)[:keep] + \"_\" + suffix)[:maxlen]\n                        elif isinstance(v, int):\n                            v = int(v) + (i * 1000) + tries\n                        else:\n                            v = f\"{v}_{uuid.uuid4().hex[:6]}\"\n\n                    seen_uniques[c.column].add(v)\n                    row[c.column] = v\n                    pk_vals.append(v)\n                    continue\n\n                fk_key = (table_lc, c.column)\n                if fk_key in fk_map:\n                    parent_table, _ = fk_map[fk_key]\n                    if c.column in unique_fk_pools and unique_fk_pools[c.column]:\n                        idx = i - 1\n                        row[c.column] = unique_fk_pools[c.column][idx] if idx < len(unique_fk_pools[c.column]) else (\n                            None if c.is_nullable else unique_fk_pools[c.column][-1]\n                        )\n                        continue\n\n                    candidates = ref_ids.get(parent_table, [])\n                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                    continue\n\n                v = generate_value(fake, c, i, enums)\n\n                # Enforce single-column UNIQUE constraints (safe for NULLs)\n                if c.column in seen_uniques and v is not None:\n                    tries = 0\n                    max_tries = 50\n                    while v in seen_uniques[c.column] and tries < max_tries:\n                        tries += 1\n                        v = generate_value(fake, c, i + tries, enums)\n\n                    if v in seen_uniques[c.column]:\n                        # Force uniqueness deterministically as a last resort\n                        if isinstance(v, str):\n                            suffix = uuid.uuid4().hex[:6]\n                            maxlen = c.character_maximum_length or 255\n                            keep = max(1, maxlen - (1 + len(suffix)))\n                            v = (str(v)[:keep] + \"_\" + suffix)[:maxlen]\n                        elif isinstance(v, int):\n                            v = int(v) + (i * 1000) + tries\n                        else:\n                            v = f\"{v}_{uuid.uuid4().hex[:6]}\"\n\n                    seen_uniques[c.column].add(v)\n                if v is None and not c.is_nullable:\n                    if c.data_type.lower() in {\"character varying\", \"character\", \"text\"}:\n                        v = unique_text((c.table, c.column), lambda: f\"VAL_{uuid.uuid4().hex[:6]}\")\n                    elif c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"}:\n                        v = 1\n                    elif c.data_type.lower() == \"boolean\":\n                        v = False\n                    elif c.data_type.lower() == \"date\":\n                        v = date.today()\n                    else:\n                        v = \"VAL\"\n                row[c.column] = v\n\n            if checkin_col and checkout_col:\n                ci = row.get(checkin_col)\n                co = row.get(checkout_col)\n                if ci is not None and co is not None and co < ci:\n                    row[checkout_col] = ci + timedelta(days=1)\n\n            w.writerow([row.get(cn) for cn in colnames])\n\n    # NOTE: we still track pk_vals here, but the main loop will overwrite with DB-truth anyway.\n    if pk_col:\n        ref_ids[table_lc] = pk_vals\n    return path\n\n\n# -------------------------\n# Main CSV generator\n# -------------------------\ndef generate_table_csv(\n    *,\n    fake: Faker,\n    out_dir: Path,\n    table: str,\n    cols: List[ColumnInfo],\n    pk: Optional[PrimaryKey],\n    fk_map: Dict[Tuple[str, str], Tuple[str, str]],\n    ref_ids: Dict[str, List[Any]],\n    n_rows: int,\n    enums: Dict[str, List[str]],\n    unique_cols: Dict[str, Set[str]],\n) -> Path:\n    tl = table.lower()\n\n    if tl == \"booking_room\":\n        return generate_booking_room_csv(\n            fake=fake, out_dir=out_dir, table=table, cols=cols, fk_map=fk_map, ref_ids=ref_ids, n_rows=n_rows, enums=enums\n        )\n    if tl == \"room_night\":\n        return generate_room_night_csv(\n            fake=fake, out_dir=out_dir, table=table, cols=cols, fk_map=fk_map, ref_ids=ref_ids, n_rows=n_rows, enums=enums\n        )\n    if tl == \"stay\":\n        return generate_stay_csv(\n            fake=fake,\n            out_dir=out_dir,\n            table=table,\n            cols=cols,\n            fk_map=fk_map,\n            ref_ids=ref_ids,\n            n_rows=n_rows,\n            enums=enums,\n            unique_cols=unique_cols,\n        )\n    if tl == \"booking\":\n        return generate_booking_csv(\n            fake=fake,\n            out_dir=out_dir,\n            table=table,\n            cols=cols,\n            fk_map=fk_map,\n            ref_ids=ref_ids,\n            n_rows=n_rows,\n            enums=enums,\n            unique_cols=unique_cols,\n            pk=pk,\n        )\n    if tl == \"booking_discount\":\n        return generate_booking_discount_csv(\n            fake=fake,\n            out_dir=out_dir,\n            table=table,\n            cols=cols,\n            fk_map=fk_map,\n            ref_ids=ref_ids,\n            n_rows=n_rows,\n            enums=enums,\n        )\n    if tl == \"review_score\":\n        return generate_review_score_csv(\n            fake=fake,\n            out_dir=out_dir,\n            table=table,\n            cols=cols,\n            fk_map=fk_map,\n            ref_ids=ref_ids,\n            n_rows=n_rows,\n            enums=enums,\n        )\n\n\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"{table}.csv\"\n    colnames = [c.column for c in cols]\n    table_lc = table.lower()\n\n    pk_col = pk.columns[0] if (pk and len(pk.columns) == 1) else None\n    pk_vals: List[Any] = []\n\n    # UNIQUE(FK) auto-detection for 1:1 tables\n    fk_cols_in_table = {c.column for c in cols if (table_lc, c.column) in fk_map}\n    uniq_cols_in_table = unique_cols.get(table_lc, set())\n    unique_fk_cols = fk_cols_in_table.intersection(uniq_cols_in_table)\n\n    unique_fk_pools: Dict[str, List[Any]] = {}\n    for fk_col in unique_fk_cols:\n        parent_table, _ = fk_map[(table_lc, fk_col)]\n        parent_ids = list(ref_ids.get(parent_table, []))\n        random.shuffle(parent_ids)\n        unique_fk_pools[fk_col] = parent_ids[:n_rows]\n\n    # Track seen values for single-column UNIQUE constraints\n    seen_uniques: Dict[str, Set[Any]] = {c: set() for c in uniq_cols_in_table}\n\n    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(colnames)\n\n        # Start/end date coherence (generic)\n        col_lc = {c.column.lower(): c.column for c in cols}\n        start_keys = {\"start_date\", \"from_date\", \"valid_from\", \"effective_start_date\", \"block_start_date\"}\n        end_keys = {\"end_date\", \"to_date\", \"valid_to\", \"effective_end_date\", \"block_end_date\", \"expires_on\"}\n\n        start_col = next((col_lc[k] for k in start_keys if k in col_lc), None)\n        end_col = next((col_lc[k] for k in end_keys if k in col_lc), None)\n\n        for i in range(1, n_rows + 1):\n            row: Dict[str, Any] = {}\n\n            if start_col and end_col:\n                d_from = fake.date_between(start_date=\"-365d\", end_date=\"+365d\")\n                d_to = d_from + timedelta(days=random.randint(1, 60))\n                row[start_col] = d_from\n                row[end_col] = d_to\n\n            for c in cols:\n                if c.column in row:\n                    continue\n\n                # PK\n                if pk_col and c.column == pk_col:\n                    v = generate_value(fake, c, i, enums)\n\n                # Enforce single-column UNIQUE constraints (safe for NULLs)\n                if c.column in seen_uniques and v is not None:\n                    tries = 0\n                    max_tries = 50\n                    while v in seen_uniques[c.column] and tries < max_tries:\n                        tries += 1\n                        v = generate_value(fake, c, i + tries, enums)\n\n                    if v in seen_uniques[c.column]:\n                        # Force uniqueness deterministically as a last resort\n                        if isinstance(v, str):\n                            suffix = uuid.uuid4().hex[:6]\n                            maxlen = c.character_maximum_length or 255\n                            keep = max(1, maxlen - (1 + len(suffix)))\n                            v = (str(v)[:keep] + \"_\" + suffix)[:maxlen]\n                        elif isinstance(v, int):\n                            v = int(v) + (i * 1000) + tries\n                        else:\n                            v = f\"{v}_{uuid.uuid4().hex[:6]}\"\n\n                    seen_uniques[c.column].add(v)\n                    row[c.column] = v\n                    pk_vals.append(v)\n                    continue\n\n                # FK\n                fk_key = (table_lc, c.column)\n                if fk_key in fk_map:\n                    parent_table, _ = fk_map[fk_key]\n\n                    # UNIQUE(FK): assign without replacement\n                    if c.column in unique_fk_pools and unique_fk_pools[c.column]:\n                        idx = i - 1\n                        row[c.column] = unique_fk_pools[c.column][idx] if idx < len(unique_fk_pools[c.column]) else (\n                            None if c.is_nullable else unique_fk_pools[c.column][-1]\n                        )\n                        continue\n\n                    candidates = ref_ids.get(parent_table, [])\n                    row[c.column] = random.choice(candidates) if candidates else (None if c.is_nullable else 1)\n                    continue\n\n                v = generate_value(fake, c, i, enums)\n\n                # Enforce single-column UNIQUE constraints (safe for NULLs)\n                if c.column in seen_uniques and v is not None:\n                    tries = 0\n                    max_tries = 50\n                    while v in seen_uniques[c.column] and tries < max_tries:\n                        tries += 1\n                        v = generate_value(fake, c, i + tries, enums)\n\n                    if v in seen_uniques[c.column]:\n                        # Force uniqueness deterministically as a last resort\n                        if isinstance(v, str):\n                            suffix = uuid.uuid4().hex[:6]\n                            maxlen = c.character_maximum_length or 255\n                            keep = max(1, maxlen - (1 + len(suffix)))\n                            v = (str(v)[:keep] + \"_\" + suffix)[:maxlen]\n                        elif isinstance(v, int):\n                            v = int(v) + (i * 1000) + tries\n                        else:\n                            v = f\"{v}_{uuid.uuid4().hex[:6]}\"\n\n                    seen_uniques[c.column].add(v)\n                if v is None and not c.is_nullable:\n                    if c.data_type.lower() in {\"character varying\", \"character\", \"text\"}:\n                        v = unique_text((c.table, c.column), lambda: f\"VAL_{uuid.uuid4().hex[:6]}\")\n                    elif c.data_type.lower() in {\"integer\", \"bigint\", \"smallint\"}:\n                        v = 1\n                    elif c.data_type.lower() == \"boolean\":\n                        v = False\n                    elif c.data_type.lower() == \"date\":\n                        v = date.today()\n                    else:\n                        v = \"VAL\"\n                row[c.column] = v\n\n            w.writerow([row.get(cn) for cn in colnames])\n\n    # NOTE: main loop overwrites with DB-truth anyway.\n    if pk_col:\n        ref_ids[table_lc] = pk_vals\n\n    return path\n\n\n# -------------------------\n# COPY load + TRUNCATE + PK cache\n# -------------------------\ndef copy_csv_to_postgres(conn, schema: str, table: str, csv_path: Path, columns: List[str]):\n    with conn.cursor() as cur:\n        cur.execute(f'SET search_path TO \"{schema}\"')\n        with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n            next(f)\n            cols_sql = \", \".join([f'\"{c}\"' for c in columns])\n            cur.copy_expert(\n                f'COPY \"{table}\" ({cols_sql}) FROM STDIN WITH (FORMAT CSV)',\n                f,\n            )\n\n\ndef truncate_tables(conn, schema: str, load_order: List[str]):\n    with conn.cursor() as cur:\n        cur.execute(f'SET search_path TO \"{schema}\"')\n        for t in reversed(load_order):\n            cur.execute(f'TRUNCATE TABLE \"{t}\" RESTART IDENTITY CASCADE;')\n\n\ndef cache_pk_values(conn, schema: str, table: str, pk: Optional[PrimaryKey], ref_ids: Dict[str, List[Any]]):\n    \"\"\"\n    \u2705 After loading a table, cache its single-column PK values into ref_ids[table_lc].\n    This fixes cases like check_in.stay_id where stay ids must exist in ref_ids[\"stay\"].\n    \"\"\"\n    if not pk or len(pk.columns) != 1:\n        return\n\n    table_lc = table.lower()\n    pk_col = pk.columns[0]\n\n    with conn.cursor() as cur:\n        cur.execute(f'SET search_path TO \"{schema}\"')\n        cur.execute(f'SELECT \"{pk_col}\" FROM \"{table}\" ORDER BY \"{pk_col}\"')\n        ref_ids[table_lc] = [r[0] for r in cur.fetchall()]\n\n\n# -------------------------\n# MAIN\n# -------------------------\ndef main():\n    random.seed(SEED)\n    fake = Faker()\n    Faker.seed(SEED)\n\n    conn = psycopg2.connect(pg_dsn(PG))\n    conn.autocommit = True\n    schema = PG.schema\n\n    tables = fetch_tables(conn, schema)\n    cols_by_table = fetch_columns(conn, schema)\n    pks = fetch_primary_keys(conn, schema)\n    fks = fetch_foreign_keys(conn, schema)\n    enums = fetch_enum_values(conn)\n    unique_cols = fetch_unique_columns(conn, schema)\n\n    fk_map = build_fk_map(fks)\n    load_order = topo_sort_tables(tables, fks)\n\n    rc = default_row_counts(tables)\n    for k, v in ROW_COUNTS_OVERRIDE.items():\n        if k in rc:\n            rc[k] = v\n\n    OUT_DIR.mkdir(parents=True, exist_ok=True)\n\n    print(f\"Schema: {schema}\", flush=True)\n    print(f\"Tables: {len(tables)}\", flush=True)\n    print(f\"Enums detected: {len(enums)}\", flush=True)\n    print(f\"Output dir: {OUT_DIR.resolve()}\", flush=True)\n\n    if TRUNCATE_FIRST:\n        print(\"Truncating tables...\", flush=True)\n        truncate_tables(conn, schema, load_order)\n        print(\"Truncate done.\", flush=True)\n\n    # \u2705 Always use lowercase keys in ref_ids\n    ref_ids: Dict[str, List[Any]] = {}\n\n    for t in load_order:\n        cols = cols_by_table.get(t, [])\n        if not cols:\n            continue\n        n = int(rc.get(t, 0))\n        if n <= 0:\n            continue\n\n        print(f\"\u2192 {t}: generating {n:,}\", flush=True)\n        csv_path = generate_table_csv(\n            fake=fake,\n            out_dir=OUT_DIR,\n            table=t,\n            cols=cols,\n            pk=pks.get(t),\n            fk_map=fk_map,\n            ref_ids=ref_ids,\n            n_rows=n,\n            enums=enums,\n            unique_cols=unique_cols,\n        )\n\n        print(f\"\u2192 {t}: loading via COPY\", flush=True)\n        copy_csv_to_postgres(conn, schema, t, csv_path, [c.column for c in cols])\n        print(f\"\u2705 {t}: generated+loaded {n:,} rows\", flush=True)\n\n        # \u2705 CRITICAL: cache real PK ids for downstream FK generation\n        cache_pk_values(conn, schema, t, pks.get(t), ref_ids)\n\n    conn.close()\n    print(\"\u2705 DONE\", flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2f89e-d29e-46bf-94a1-8ca06900b718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}