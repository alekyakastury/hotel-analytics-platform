{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dac2340-210c-4df7-beaf-3dcf375b779e",
   "metadata": {},
   "source": [
    "## Postgres to Snowflake Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4e8b9b6-718e-4103-8072-2935f2c7f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone, timedelta, date\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from psycopg2.extensions import connection as Psycopg2Connection\n",
    "from psycopg2 import sql\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import subprocess\n",
    "import shlex\n",
    "from dateutil import parser\n",
    "from snowflake.ingest import SimpleIngestManager, StagedFile\n",
    "import json\n",
    "import uuid\n",
    "import socket\n",
    "import requests\n",
    "from cryptography.hazmat.primitives import serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5aa29-a707-4ce8-b78e-f263219f6d48",
   "metadata": {},
   "source": [
    "## 1) Config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b55f7-e86f-4282-b8f9-aece8fdebc46",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5876705-caa6-44b5-abf4-12c35b63add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            return config or {}\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML in config file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03388238-9938-4cec-9896-10579339b808",
   "metadata": {},
   "source": [
    "### Validate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05b687fc-21cf-4a34-a860-7a5c1de3acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config(cfg: Dict[str, Any]) -> None:\n",
    "    # Top-level keys\n",
    "    required_top_keys = {\"version\", \"source\", \"export\", \"tables\"}\n",
    "    missing = required_top_keys - cfg.keys()\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing top-level keys: {', '.join(sorted(missing))}\")\n",
    "\n",
    "    # Validate source\n",
    "    if not isinstance(cfg[\"source\"], dict) or not cfg[\"source\"]:\n",
    "        raise ValueError(\"'source' must be a non-empty dictionary\")\n",
    "\n",
    "    for source_name, conn in cfg[\"source\"].items():\n",
    "        if not isinstance(conn, dict):\n",
    "            raise TypeError(f\"Source '{source_name}' must be a dictionary\")\n",
    "        for key in (\"host\", \"port\", \"database\", \"user\", \"password_env\", \"schema\"):\n",
    "            if key not in conn:\n",
    "                raise KeyError(f\"Missing '{key}' in source '{source_name}'\")\n",
    "                \n",
    "    # Validate target\n",
    "    if not isinstance(cfg[\"target\"], dict) or not cfg[\"target\"]:\n",
    "        raise ValueError(\"'target' must be a non-empty dictionary\")\n",
    "\n",
    "    for target_name, conn in cfg[\"target\"].items():\n",
    "        if not isinstance(conn, dict):\n",
    "            raise TypeError(f\"Target '{target_name}' must be a dictionary\")\n",
    "        for key in (\"account\", \"user\", \"password_env\", \"role\", \"database\", \"schema\"):\n",
    "            if key not in conn:\n",
    "                raise KeyError(f\"Missing '{key}' in target '{target_name}'\")\n",
    "\n",
    "    # Validate export\n",
    "    export = cfg[\"export\"]\n",
    "    if not isinstance(export, dict):\n",
    "        raise TypeError(\"'export' must be a dictionary\")\n",
    "    for key in (\"output_dir\", \"format\"):\n",
    "        if key not in export:\n",
    "            raise KeyError(f\"Missing '{key}' in 'export'\")\n",
    "\n",
    "    # Validate tables\n",
    "    tables = cfg[\"tables\"]\n",
    "    if not isinstance(tables, dict) or not tables:\n",
    "        raise ValueError(\"'tables' must be a non-empty list\")\n",
    "    for i, table in enumerate(tables):\n",
    "        t=tables[table][0]\n",
    "        if not isinstance(t, dict):\n",
    "            raise TypeError(f\"Table at index {i} must be a dictionary\")\n",
    "        if \"name\" not in t or \"mode\" not in t or \"order_by\" not in t:\n",
    "            raise KeyError(f\"Table {t} missing required keys\")\n",
    "        if t[\"mode\"] == \"table\" and \"table\" not in t:\n",
    "            raise KeyError(f\"Table {t['name']} missing 'table' key for mode 'table'\")\n",
    "        if t[\"mode\"] == \"query\" and \"query\" not in t:\n",
    "            raise KeyError(f\"Table {t['name']} missing 'query' key for mode 'query'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c259d8b-84b7-4064-875c-5655e2c9be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_yaml_file(path: str) -> Dict[str, Any]:\n",
    "    # Load YAML\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"YAML file not found: {path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML syntax: {e}\")\n",
    "\n",
    "    if not isinstance(cfg, dict):\n",
    "        raise ValueError(\"YAML root must be a dictionary\")\n",
    "\n",
    "    validate_config(cfg)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fdd40-b424-4217-80d1-93c2da378179",
   "metadata": {},
   "source": [
    "## 2) Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feab731-5144-411b-8418-98e13b64e300",
   "metadata": {},
   "source": [
    "### Create postgres database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e4fd0b7-6ce3-4748-840f-4d8391271ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PostgresCreds:\n",
    "    host: str\n",
    "    port: str\n",
    "    dbname: str\n",
    "    user: str\n",
    "    password: str # use password_env in YAML (recommended)\n",
    "    schema: Optional[str]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4220613f-0772-472a-b382-aca8ea865a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pg_connection(creds: PostgresCreds) -> Psycopg2Connection:\n",
    "    \"\"\"\n",
    "    Create a PostgreSQL connection using psycopg2.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=creds.host,\n",
    "        port=creds.port,\n",
    "        dbname=creds.dbname,\n",
    "        user=creds.user,\n",
    "        password=creds.password,\n",
    "    )\n",
    "\n",
    "    # Optionally set search_path if schema is provided\n",
    "    if creds.schema:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                sql.SQL(\"SET search_path TO {}\").format(\n",
    "                    sql.Identifier(creds.schema)\n",
    "                )\n",
    "            )\n",
    "            conn.commit()\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17550cc-0bb0-4283-b462-9d441fd03851",
   "metadata": {},
   "source": [
    "### Create Snowflake connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34b540cf-37cc-4c7f-a339-6a1e98ccdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SnowflakeCreds:\n",
    "    account: str\n",
    "    user: str\n",
    "    role: Optional[str]\n",
    "    warehouse: Optional[str]\n",
    "    database: Optional[str]\n",
    "    schema: Optional[str]\n",
    "    password: Optional[str]\n",
    "    sf_landing_stage: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86b66c96-bae9-4389-b723-db531ad9bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _snowsql_base_cmd(creds: SnowflakeCreds, snowsql_path: str = \"snowsql\") -> list[str]:\n",
    "    cmd = [\n",
    "        snowsql_path,\n",
    "        \"-a\", creds.account,\n",
    "        \"-u\", creds.user,\n",
    "        \"-o\", \"exit_on_error=true\",\n",
    "        \"-o\", \"friendly=false\",\n",
    "        \"-o\", \"quiet=false\",\n",
    "    ]\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb0a0d02-672c-4658-879b-a1f2a647b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_snowsql(creds: SnowflakeCreds, sql: str, *, snowsql_path: str = \"snowsql\", timeout_sec: int = 300) -> str:\n",
    "    env = os.environ.copy()\n",
    "    if creds.password:\n",
    "        env[\"SNOWSQL_PWD\"] = creds.password\n",
    "\n",
    "    cmd = _snowsql_base_cmd(creds, snowsql_path=snowsql_path) + [\"-q\", sql]\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        cmd,\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=timeout_sec,\n",
    "    )\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            \"SnowSQL command failed.\\n\"\n",
    "            f\"Command: {shlex.join(cmd)}\\n\"\n",
    "            f\"STDOUT:\\n{proc.stdout}\\n\"\n",
    "            f\"STDERR:\\n{proc.stderr}\"\n",
    "        )\n",
    "    return proc.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a367bc9f-c646-4764-9002-02f6e675c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_network_for_snowflake(\n",
    "    proxy_url: str = \"http://proxy.mycorp.com:8080\",\n",
    "    snowflake_host: str = \"account_name\",\n",
    "    timeout_s: int = 8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Makes Snowflake + stage (S3) uploads work without manual toggling.\n",
    "\n",
    "    Strategy:\n",
    "    - If proxy hostname resolves (likely on VPN/corp network):\n",
    "        Use proxy for general internet (so S3 works),\n",
    "        but bypass proxy for *.snowflakecomputing.com (so Snowflake doesn't get stuck on proxy DNS rules).\n",
    "    - If proxy hostname does NOT resolve (likely off VPN/home network):\n",
    "        Disable proxy env vars entirely and go direct.\n",
    "    - Then run quick connectivity checks for Snowflake host DNS + S3 reachability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def can_resolve(host: str) -> bool:\n",
    "        try:\n",
    "            socket.getaddrinfo(host, 80)\n",
    "            return True\n",
    "        except OSError:\n",
    "            return False\n",
    "\n",
    "    proxy_host = proxy_url.replace(\"http://\", \"\").replace(\"https://\", \"\").split(\":\")[0]\n",
    "    proxy_resolves = can_resolve(proxy_host)\n",
    "\n",
    "    if proxy_resolves:\n",
    "        # Proxy is usable (VPN/corp). Keep it for S3, but bypass for Snowflake.\n",
    "        for k in [\"HTTP_PROXY\", \"HTTPS_PROXY\", \"http_proxy\", \"https_proxy\"]:\n",
    "            os.environ[k] = proxy_url\n",
    "\n",
    "        no_proxy = \",\".join([\n",
    "            \"localhost\", \"127.0.0.1\",\n",
    "            snowflake_host,\n",
    "            \".snowflakecomputing.com\",\n",
    "        ])\n",
    "        os.environ[\"NO_PROXY\"] = no_proxy\n",
    "        os.environ[\"no_proxy\"] = no_proxy\n",
    "        mode = \"PROXY_FOR_S3__BYPASS_FOR_SNOWFLAKE\"\n",
    "    else:\n",
    "        # Proxy not resolvable (home/off VPN). Go fully direct.\n",
    "        for k in [\"HTTP_PROXY\",\"HTTPS_PROXY\",\"ALL_PROXY\",\"http_proxy\",\"https_proxy\",\"all_proxy\"]:\n",
    "            os.environ.pop(k, None)\n",
    "        os.environ[\"NO_PROXY\"] = \",\".join([\"localhost\",\"127.0.0.1\",snowflake_host,\".snowflakecomputing.com\"])\n",
    "        os.environ[\"no_proxy\"] = os.environ[\"NO_PROXY\"]\n",
    "        mode = \"DIRECT_NO_PROXY\"\n",
    "\n",
    "    # --- Fast checks ---\n",
    "    # 1) Snowflake DNS\n",
    "    try:\n",
    "        socket.getaddrinfo(snowflake_host, 443)\n",
    "        snowflake_dns_ok = True\n",
    "    except OSError:\n",
    "        snowflake_dns_ok = False\n",
    "\n",
    "    # 2) S3 reachability (HEAD is enough)\n",
    "    s3_ok = None\n",
    "    try:\n",
    "        r = requests.head(\"https://s3.amazonaws.com\", timeout=timeout_s)\n",
    "        s3_ok = (r.status_code < 500)\n",
    "    except Exception:\n",
    "        s3_ok = False\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"proxy_resolves\": proxy_resolves,\n",
    "        \"snowflake_dns_ok\": snowflake_dns_ok,\n",
    "        \"s3_ok\": s3_ok,\n",
    "        \"http_proxy\": os.environ.get(\"HTTP_PROXY\"),\n",
    "        \"no_proxy\": os.environ.get(\"NO_PROXY\"),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c207a3-6b6b-4975-b388-b885051848ed",
   "metadata": {},
   "source": [
    "## 3) Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f48ae-1708-4cb1-b145-e95e40670e1b",
   "metadata": {},
   "source": [
    "### Extract data from postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12ed8538-581a-4d38-a852-636265b86ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_columns(conn: psycopg2.extensions.connection, schema: str, table: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch column names for a given schema.table in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        schema: schema name (e.g., 'public')\n",
    "        table: table name (e.g., 'hotel')\n",
    "\n",
    "    Returns:\n",
    "        List of column names in order.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = %s AND table_name = %s\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (schema, table))\n",
    "        columns = [row[0] for row in cur.fetchall()]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f9a4983-f67f-4fab-9b5b-4f513c5945ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_pk(conn: psycopg2.extensions.connection, schema: str, table: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Fetch the primary key column names for a given schema.table in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        schema: schema name (e.g., 'public')\n",
    "        table: table name (e.g., 'hotel')\n",
    "\n",
    "    Returns:\n",
    "        List of primary key column names in order, or None if table has no PK.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT kcu.column_name\n",
    "    FROM information_schema.table_constraints tc\n",
    "    JOIN information_schema.key_column_usage kcu\n",
    "      ON tc.constraint_name = kcu.constraint_name\n",
    "      AND tc.table_schema = kcu.table_schema\n",
    "    WHERE tc.table_schema = %s\n",
    "      AND tc.table_name = %s\n",
    "      AND tc.constraint_type = 'PRIMARY KEY'\n",
    "    ORDER BY kcu.ordinal_position;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (schema, table))\n",
    "        pk_columns = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    return pk_columns if pk_columns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8a58347-0b42-4bf6-98ea-064a7e981a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rowcount(conn: psycopg2.extensions.connection, sql: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the number of rows a SQL query will return.\n",
    "    Uses COUNT(*) wrapped around the query.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        sql: SQL query (string)\n",
    "\n",
    "    Returns:\n",
    "        Estimated row count (int)\n",
    "    \"\"\"\n",
    "    # Wrap the original query as a subquery\n",
    "    count_sql = f\"SELECT COUNT(*) FROM ({sql}) AS subquery\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(count_sql)\n",
    "            rowcount = cur.fetchone()[0]\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    return rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f22e569-56da-47cb-b1b0-0ef04accb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_query(table_cfg: Dict, schema_default: str, columns: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Build the base SQL query for a table export.\n",
    "\n",
    "    Args:\n",
    "        table_cfg: dictionary containing table config from YAML\n",
    "                   Must include 'mode' and either 'table' or 'query'\n",
    "        schema_default: default schema to use if table_cfg does not specify one\n",
    "        columns: optional list of columns to select (for mode='table')\n",
    "\n",
    "    Returns:\n",
    "        SQL string (no trailing semicolon)\n",
    "    \"\"\"\n",
    "    mode = table_cfg.get(\"mode\")\n",
    "    \n",
    "    if mode == \"table\":\n",
    "        table_name = table_cfg.get(\"table\")\n",
    "        if not table_name:\n",
    "            raise KeyError(f\"Table config '{table_cfg.get('name')}' missing 'table' key for mode='table'\")\n",
    "        \n",
    "        schema = table_cfg.get(\"schema\", schema_default)\n",
    "        \n",
    "        # Use all columns if not specified\n",
    "        cols_sql = \", \".join(columns) if columns else \"*\"\n",
    "        \n",
    "        sql = f\"SELECT {cols_sql} FROM {schema}.{table_name}\"\n",
    "        return sql\n",
    "\n",
    "    elif mode == \"query\":\n",
    "        query = table_cfg.get(\"query\")\n",
    "        if not query:\n",
    "            raise KeyError(f\"Table config '{table_cfg.get('name')}' missing 'query' key for mode='query'\")\n",
    "        return query.rstrip().rstrip(\";\")  # strip trailing semicolon\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode '{mode}' for table '{table_cfg.get('name')}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96211b57-2c85-4f35-a8aa-3b03db7c7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_partition_clause(\n",
    "    base_sql: str,\n",
    "    partition_spec: Dict\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Partition base_sql across [start_date, end_date) and return a list of SQL strings,\n",
    "    one per partition window.\n",
    "\n",
    "    Supported:\n",
    "      partition_spec = {\"type\": \"date_range\", \"column\": \"checkin_date\", \"granularity\": \"month\"|\"day\"}\n",
    "\n",
    "    Notes:\n",
    "      - end_date is EXCLUSIVE (so end_date=\"2026-01-01\" includes all of 2025).\n",
    "      - If base_sql already contains a WHERE (case-insensitive), appends with AND.\n",
    "    \"\"\"\n",
    "    column = partition_spec.get(\"column\")\n",
    "    start_date=partition_spec.get(\"start\")\n",
    "    end_date=partition_spec.get(\"end\")\n",
    "    if not column:\n",
    "        raise KeyError(\"partition_spec must have a 'column' key\")\n",
    "\n",
    "    if partition_spec.get(\"type\") != \"date_range\":\n",
    "        raise ValueError(f\"Unsupported partition type '{partition_spec.get('type')}'\")\n",
    "\n",
    "    granularity = partition_spec.get(\"granularity\", \"month\").lower()\n",
    "    if granularity not in {\"month\", \"day\"}:\n",
    "        raise ValueError(f\"Unsupported granularity '{granularity}' (use 'month' or 'day')\")\n",
    "\n",
    "    # Parse dates\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    if start >= end:\n",
    "        raise ValueError(f\"start_date must be < end_date. Got {start_date} .. {end_date}\")\n",
    "\n",
    "    # Helper: next month boundary from a given date\n",
    "    def next_month(d: date) -> date:\n",
    "        if d.month == 12:\n",
    "            return date(d.year + 1, 1, 1)\n",
    "        return date(d.year, d.month + 1, 1)\n",
    "\n",
    "    # Helper: attach WHERE/AND\n",
    "    has_where = \" where \" in base_sql.lower()\n",
    "\n",
    "    sqls: List[str] = []\n",
    "    cur = start\n",
    "\n",
    "    while cur < end:\n",
    "        if granularity == \"day\":\n",
    "            nxt = cur + timedelta(days=1)\n",
    "        else:  # month\n",
    "            # advance to first of next month; works best when cur is 1st-of-month (your case)\n",
    "            nxt = next_month(cur)\n",
    "\n",
    "        window_end = nxt if nxt < end else end\n",
    "\n",
    "        clause = (\n",
    "            f\"{column} >= '{cur.strftime('%Y-%m-%d')}' \"\n",
    "            f\"AND {column} < '{window_end.strftime('%Y-%m-%d')}'\"\n",
    "        )\n",
    "\n",
    "        if has_where:\n",
    "            sqls.append(f\"{base_sql} AND {clause}\")\n",
    "        else:\n",
    "            sqls.append(f\"{base_sql} WHERE {clause}\")\n",
    "\n",
    "        cur = window_end\n",
    "\n",
    "    return sqls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2039f321-5011-4f93-9197-58be8fb1f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_file_splits(rowcount: int, max_rows_per_file: int) -> List[Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Plan file splits for a given rowcount and max rows per file.\n",
    "\n",
    "    Args:\n",
    "        rowcount: total number of rows in the partition\n",
    "        max_rows_per_file: maximum rows per output file\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys:\n",
    "            - start_row: 0-based inclusive start\n",
    "            - end_row: exclusive end\n",
    "    \"\"\"\n",
    "    if rowcount <= 0:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < rowcount:\n",
    "        end = min(start + max_rows_per_file, rowcount)\n",
    "        chunks.append({\"start_row\": start, \"end_row\": end})\n",
    "        start = end\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "affdb183-d88c-4218-8970-b4a5a635de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chunk_query(\n",
    "    sql: str,\n",
    "    order_by: Optional[str],\n",
    "    chunk: Dict[str, int]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a SQL query for a specific chunk of rows.\n",
    "\n",
    "    Args:\n",
    "        sql: base SQL (should include WHERE/filters/partition/order)\n",
    "        order_by: column(s) used for deterministic ordering\n",
    "        chunk: dict with 'start_row' (inclusive) and 'end_row' (exclusive)\n",
    "\n",
    "    Returns:\n",
    "        SQL string with ORDER BY + OFFSET/LIMIT applied\n",
    "    \"\"\"\n",
    "    if not order_by:\n",
    "        raise ValueError(\"order_by must be specified for chunking\")\n",
    "\n",
    "    start = chunk[\"start_row\"]\n",
    "    limit = chunk[\"end_row\"] - chunk[\"start_row\"]\n",
    "    \n",
    "    # Ensure SQL has ORDER BY\n",
    "    sql_ordered = sql if \" order by \" in sql.lower() else f\"{sql} ORDER BY {order_by}\"\n",
    "\n",
    "    return f\"{sql_ordered} OFFSET {start} LIMIT {limit}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544632f1-e853-4fc8-ac77-856970c63a7e",
   "metadata": {},
   "source": [
    "#### Load data into snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cadedf1-d3df-4c09-bae4-e530cd5d88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_query_to_snowflake_table(\n",
    "    pg: PostgresCreds,\n",
    "    sf: SnowflakeCreds,\n",
    "    data_dir: str,\n",
    "    sql: str,                               # your dynamic SELECT\n",
    "    stage_fqn: str,                         # e.g. \"@HOTEL_ANALYTICS.RAW.LANDING_STAGE\"\n",
    "    file_format_fqn: str,                   # e.g. \"HOTEL_ANALYTICS.RAW.CSV_FMT\" (CSV, SKIP_HEADER=1, COMPRESSION=GZIP)\n",
    "    target_table_fqn: str,                  # e.g. \"HOTEL_ANALYTICS.RAW.BOOKING_SLICE\"\n",
    "    overwrite_table: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Executes SELECT on Postgres, stages results as gzipped CSV, then loads into Snowflake.\n",
    "    - Requires Snowflake INTERNAL stage (PUT must work).\n",
    "    - Creates/overwrites target table with inferred column types as VARCHAR by default.\n",
    "      (Good enough for a project slice; you can tighten types later.)\n",
    "\n",
    "    Returns basic run stats.\n",
    "    \"\"\"\n",
    "    run_id = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "    # --- Connect Postgres ---\n",
    "    pg_conn = psycopg2.connect(\n",
    "        host=pg.host, port=pg.port, dbname=pg.dbname, user=pg.user, password=pg.password\n",
    "    )\n",
    "    pg_cur = pg_conn.cursor()\n",
    "\n",
    "    pg_cur.execute(sql)\n",
    "    cols: List[str] = [d.name for d in pg_cur.description]\n",
    "\n",
    "    # --- Write gzipped CSV ---\n",
    "    tmp_dir = Path(data_dir) / \"tmp\"\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local_file = os.path.join(tmp_dir, f\"extract__{run_id}.csv.gz\")\n",
    "    file_uri= \"file://\" + Path(local_file).resolve().as_posix()\n",
    "    \n",
    "    rows_extracted = 0\n",
    "    with gzip.open(local_file, \"wt\", newline=\"\", encoding=\"utf-8\") as gz:\n",
    "        w = csv.writer(gz)\n",
    "        w.writerow(cols)  # header\n",
    "        for row in pg_cur:\n",
    "            w.writerow(row)\n",
    "            rows_extracted += 1\n",
    "\n",
    "    pg_cur.close()\n",
    "    pg_conn.close()\n",
    "\n",
    "    if rows_extracted == 0:\n",
    "        return {\"status\": \"SUCCESS\", \"rows_extracted\": 0, \"rows_loaded\": 0, \"target_table\": target_table_fqn}\n",
    "\n",
    "\n",
    "        # --- Connect Snowflake ---\n",
    "    sf_conn = snowflake.connector.connect(\n",
    "        account=sf.account,\n",
    "        user=sf.user,\n",
    "        password=sf.password,\n",
    "        role=sf.role,\n",
    "        warehouse=sf.warehouse,\n",
    "        database=sf.database,\n",
    "        schema=sf.schema,\n",
    "        autocommit=True,\n",
    "    )\n",
    "    sf_cur = sf_conn.cursor()\n",
    "\n",
    "    # Put into a deterministic folder under the stage\n",
    "    stage_path = f\"{stage_fqn}/pg_extract/{run_id}\"\n",
    "    sf_cur.execute(f\"PUT '{file_uri}' {stage_path} AUTO_COMPRESS=FALSE OVERWRITE=TRUE\")\n",
    "\n",
    "    # Optionally drop and re-create the table (simple project-friendly behavior)\n",
    "    if overwrite_table:\n",
    "        sf_cur.execute(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
    "\n",
    "    # Create a table with simple VARCHAR columns (fast + robust for a demo)\n",
    "    col_ddl = \", \".join([f'\"{c.upper()}\" VARCHAR' for c in cols])\n",
    "    sf_cur.execute(f\"CREATE TABLE IF NOT EXISTS {target_table_fqn} ({col_ddl})\")\n",
    "    \n",
    "        # 2) COPY into RAW TABLE\n",
    "    sf_cur.execute(f\"\"\"\n",
    "        COPY INTO {target_table_fqn}\n",
    "        FROM {stage_path}\n",
    "        FILE_FORMAT = (FORMAT_NAME = {file_format_fqn})\n",
    "        PATTERN = '.*\\\\.csv\\\\.gz'\n",
    "        ON_ERROR = 'ABORT_STATEMENT'\n",
    "    \"\"\")\n",
    "    results = sf_cur.fetchall()\n",
    "    loaded = sum(int(r[3]) for r in results if len(r) > 3)\n",
    "\n",
    "    sf_cur.close()\n",
    "    sf_cur.close()\n",
    "\n",
    "    return {\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"rows_extracted\": rows_extracted,\n",
    "        \"rows_loaded\": loaded,\n",
    "        \"local_file\": str(local_file),\n",
    "        \"stage_path\": stage_path,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6092742f-0d7f-4e16-87ef-944b7c1b2e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilation!Manifesto6\n",
      "HOTEL_ANALYTICS.RAW.hotel_RAW\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "250001 (08001): Failed to connect to DB: BAJLLOB-CN38560.snowflakecomputing.com:443. Your user account has been temporarily locked. Try again later or contact your account administrator for assistance. For more information about this error, go to https://community.snowflake.com/s/error-your-user-login-has-been-locked.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 85\u001b[0m\n\u001b[0;32m     78\u001b[0m             chunk_sql_query\u001b[38;5;241m=\u001b[39mbuild_chunk_query(\u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_base_query\u001b[39m\u001b[38;5;124m\"\u001b[39m],table_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_by\u001b[39m\u001b[38;5;124m'\u001b[39m],chunk[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     79\u001b[0m             postgres_query_to_snowflake_table( pg_creds,sf_creds,data_dir,chunk_sql_query,\n\u001b[0;32m     80\u001b[0m                                               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39msf_landing_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m                                               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.CSV_FMT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m                                               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_RAW\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:main()\n",
      "Cell \u001b[1;32mIn[58], line 79\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m chunk\u001b[38;5;241m=\u001b[39mplan_file_splits(\u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rowcount\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_rows_per_file)\n\u001b[0;32m     78\u001b[0m chunk_sql_query\u001b[38;5;241m=\u001b[39mbuild_chunk_query(\u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_base_query\u001b[39m\u001b[38;5;124m\"\u001b[39m],table_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_by\u001b[39m\u001b[38;5;124m'\u001b[39m],chunk[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 79\u001b[0m postgres_query_to_snowflake_table( pg_creds,sf_creds,data_dir,chunk_sql_query,\n\u001b[0;32m     80\u001b[0m                                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39msf_landing_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m                                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.CSV_FMT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m                                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf_creds\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_RAW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[56], line 52\u001b[0m, in \u001b[0;36mpostgres_query_to_snowflake_table\u001b[1;34m(pg, sf, data_dir, sql, stage_fqn, file_format_fqn, target_table_fqn, overwrite_table)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUCCESS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows_extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows_loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_table\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_table_fqn}\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# --- Connect Snowflake ---\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m sf_conn \u001b[38;5;241m=\u001b[39m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m     53\u001b[0m     account\u001b[38;5;241m=\u001b[39msf\u001b[38;5;241m.\u001b[39maccount,\n\u001b[0;32m     54\u001b[0m     user\u001b[38;5;241m=\u001b[39msf\u001b[38;5;241m.\u001b[39muser,\n\u001b[0;32m     55\u001b[0m     password\u001b[38;5;241m=\u001b[39msf\u001b[38;5;241m.\u001b[39mpassword,\n\u001b[0;32m     56\u001b[0m     role\u001b[38;5;241m=\u001b[39msf\u001b[38;5;241m.\u001b[39mrole,\n\u001b[0;32m     57\u001b[0m     warehouse\u001b[38;5;241m=\u001b[39msf\u001b[38;5;241m.\u001b[39mwarehouse,\n\u001b[0;32m     58\u001b[0m     database\u001b[38;5;241m=\u001b[39msf\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[0;32m     59\u001b[0m     schema\u001b[38;5;241m=\u001b[39msf\u001b[38;5;241m.\u001b[39mschema,\n\u001b[0;32m     60\u001b[0m     autocommit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     62\u001b[0m sf_cur \u001b[38;5;241m=\u001b[39m sf_conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Put into a deterministic folder under the stage\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\__init__.py:66\u001b[0m, in \u001b[0;36mConnect\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(SnowflakeConnection\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mConnect\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SnowflakeConnection:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SnowflakeConnection(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\connection.py:678\u001b[0m, in \u001b[0;36mSnowflakeConnection.__init__\u001b[1;34m(self, connection_name, connections_file_path, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m _get_default_connection_params()\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_error_attributes()\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry \u001b[38;5;241m=\u001b[39m TelemetryClient(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest)\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpired \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\connection.py:1172\u001b[0m, in \u001b[0;36mSnowflakeConnection.connect\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(exceptions_dict))\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__open_connection()\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Register the connection in the pool after successful connection\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m _connections_registry\u001b[38;5;241m.\u001b[39madd_connection(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\connection.py:1600\u001b[0m, in \u001b[0;36mSnowflakeConnection.__open_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;66;03m# okta URL, e.g., https://<account>.okta.com/\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_class \u001b[38;5;241m=\u001b[39m AuthByOkta(\n\u001b[0;32m   1595\u001b[0m         application\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplication,\n\u001b[0;32m   1596\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogin_timeout,\n\u001b[0;32m   1597\u001b[0m         backoff_generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backoff_generator,\n\u001b[0;32m   1598\u001b[0m     )\n\u001b[1;32m-> 1600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauthenticate_with_retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_class)\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_password \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ensure password won't persist\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_class\u001b[38;5;241m.\u001b[39mreset_secrets()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\connection.py:1936\u001b[0m, in \u001b[0;36mSnowflakeConnection.authenticate_with_retry\u001b[1;34m(self, auth_instance)\u001b[0m\n\u001b[0;32m   1933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauthenticate_with_retry\u001b[39m(\u001b[38;5;28mself\u001b[39m, auth_instance) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;66;03m# make some changes if needed before real __authenticate\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1936\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authenticate(auth_instance)\n\u001b[0;32m   1937\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m   1938\u001b[0m         \u001b[38;5;66;03m# cached id_token expiration error, we have cleaned id_token and try to authenticate again\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID token expired. Reauthenticating...: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, ex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\connection.py:1968\u001b[0m, in \u001b[0;36mSnowflakeConnection._authenticate\u001b[1;34m(self, auth_instance)\u001b[0m\n\u001b[0;32m   1966\u001b[0m auth_instance\u001b[38;5;241m.\u001b[39m_retry_ctx\u001b[38;5;241m.\u001b[39mset_start_time()\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1968\u001b[0m     auth\u001b[38;5;241m.\u001b[39mauthenticate(\n\u001b[0;32m   1969\u001b[0m         auth_instance\u001b[38;5;241m=\u001b[39mauth_instance,\n\u001b[0;32m   1970\u001b[0m         account\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccount,\n\u001b[0;32m   1971\u001b[0m         user\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser,\n\u001b[0;32m   1972\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[0;32m   1973\u001b[0m         schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema,\n\u001b[0;32m   1974\u001b[0m         warehouse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarehouse,\n\u001b[0;32m   1975\u001b[0m         role\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole,\n\u001b[0;32m   1976\u001b[0m         passcode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_passcode,\n\u001b[0;32m   1977\u001b[0m         passcode_in_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_passcode_in_password,\n\u001b[0;32m   1978\u001b[0m         mfa_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mfa_callback,\n\u001b[0;32m   1979\u001b[0m         password_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_password_callback,\n\u001b[0;32m   1980\u001b[0m         session_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session_parameters,\n\u001b[0;32m   1981\u001b[0m     )\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1983\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   1984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperational Error raised at authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1985\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor authenticator: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(auth_instance)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1986\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\auth\\_auth.py:420\u001b[0m, in \u001b[0;36mAuth.authenticate\u001b[1;34m(self, auth_instance, account, user, database, schema, warehouse, role, passcode, passcode_in_password, mfa_callback, password_callback, session_parameters, timeout)\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(auth_instance, AuthByUsrPwdMfa):\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delete_temporary_credential(\n\u001b[0;32m    418\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest\u001b[38;5;241m.\u001b[39m_host, user, TokenType\u001b[38;5;241m.\u001b[39mMFA_TOKEN\n\u001b[0;32m    419\u001b[0m         )\n\u001b[1;32m--> 420\u001b[0m     Error\u001b[38;5;241m.\u001b[39merrorhandler_wrapper(\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest\u001b[38;5;241m.\u001b[39m_connection,\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    423\u001b[0m         DatabaseError,\n\u001b[0;32m    424\u001b[0m         {\n\u001b[0;32m    425\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[0;32m    426\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to connect to DB: \u001b[39m\u001b[38;5;132;01m{host}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{port}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{message}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m             )\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    428\u001b[0m                 host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest\u001b[38;5;241m.\u001b[39m_host,\n\u001b[0;32m    429\u001b[0m                 port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest\u001b[38;5;241m.\u001b[39m_port,\n\u001b[0;32m    430\u001b[0m                 message\u001b[38;5;241m=\u001b[39mret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    431\u001b[0m             ),\n\u001b[0;32m    432\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m: ER_FAILED_TO_CONNECT_TO_DB,\n\u001b[0;32m    433\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,\n\u001b[0;32m    434\u001b[0m         },\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    439\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m         ),\n\u001b[0;32m    444\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\errors.py:286\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[0;32m    265\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m Error\u001b[38;5;241m.\u001b[39mhand_to_other_handler(\n\u001b[0;32m    287\u001b[0m         connection,\n\u001b[0;32m    288\u001b[0m         cursor,\n\u001b[0;32m    289\u001b[0m         error_class,\n\u001b[0;32m    290\u001b[0m         error_value,\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[0;32m    294\u001b[0m             error_class,\n\u001b[0;32m    295\u001b[0m             error_value,\n\u001b[0;32m    296\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\errors.py:344\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     connection\u001b[38;5;241m.\u001b[39merrorhandler(connection, cursor, error_class, error_value)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\snowflake\\connector\\errors.py:217\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    215\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[0;32m    218\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    219\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[0;32m    220\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    221\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    222\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    223\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[0;32m    225\u001b[0m     ),\n\u001b[0;32m    226\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[0;32m    227\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[0;32m    228\u001b[0m )\n",
      "\u001b[1;31mDatabaseError\u001b[0m: 250001 (08001): Failed to connect to DB: BAJLLOB-CN38560.snowflakecomputing.com:443. Your user account has been temporarily locked. Try again later or contact your account administrator for assistance. For more information about this error, go to https://community.snowflake.com/s/error-your-user-login-has-been-locked."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ##### Get information from config files\n",
    "    BASE_DIR = Path.cwd().parent \n",
    "    config_path = BASE_DIR / \"config\"\n",
    "    config_yaml=config_path/ \"postgres_to_snowflake_config.yaml\"\n",
    "    data_dir=BASE_DIR/\"data\"\n",
    "    passwords_path=config_path/\".env\"\n",
    "\n",
    "    ##### Store all passwords\n",
    "    load_dotenv(dotenv_path=passwords_path,override=True)  # loads .env into environment variables\n",
    "    postgres_password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "    snowflake_password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "    config=load_config(config_yaml)\n",
    "           \n",
    "    ##### Validate config file\n",
    "    if validate_yaml_file(config_yaml): \n",
    "        config=load_config(config_yaml)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"YAML config file not found or invalid: {config_yaml}\")\n",
    "\n",
    "    ##### Connect to Postgres DB\n",
    "    pg_cfg = config.get(\"source\", {}).get(\"postgres\", {})\n",
    "    \n",
    "    pg_creds=PostgresCreds(\n",
    "        host=pg_cfg.get(\"host\", \"localhost\"),\n",
    "        port=str(pg_cfg.get(\"port\", 5432)),\n",
    "        dbname=pg_cfg.get(\"database\", \"\"),\n",
    "        user=pg_cfg.get(\"user\", \"\"),\n",
    "        password=postgres_password,\n",
    "        schema=pg_cfg.get(\"schema\")  # can be None\n",
    "    )\n",
    "    \n",
    "    pg_conn=create_pg_connection(pg_creds)\n",
    "\n",
    "    ###### Connect to Snowflake\n",
    "    sf_cfg = config.get(\"target\", {}).get(\"snowflake\", {})\n",
    "    \n",
    "    sf_creds=SnowflakeCreds(\n",
    "        account=sf_cfg.get(\"account\"),\n",
    "        user=sf_cfg.get(\"user\"),\n",
    "        role=sf_cfg.get(\"role\"),\n",
    "        warehouse=sf_cfg.get(\"warehouse\"),\n",
    "        database=sf_cfg.get(\"database\"),\n",
    "        schema=sf_cfg.get(\"schema\"),\n",
    "        password=snowflake_password,\n",
    "        sf_landing_stage=sf_cfg.get(\"stage\")\n",
    "    )\n",
    "\n",
    "    net = configure_network_for_snowflake(sf_creds.account)\n",
    "    ####### Hotel Config\n",
    "    table_schema=pg_cfg.get(\"schema\")\n",
    "    sqls=[]\n",
    "    max_rows_per_file=config.get(\"export\", {}).get(\"row_grouping\",{}).get(\"max_rows_per_file\",{})\n",
    "    for table in config.get(\"tables\", {}).keys():\n",
    "        table_name=config.get(\"tables\", {})[table][0].get(\"name\")\n",
    "        print(f\"{sf_creds.database}.{sf_creds.schema}.{table_name}_RAW\")\n",
    "        table_cfg=config.get(\"tables\", {})[table][0]\n",
    "        globals()[f\"{table_name}_columns\"]=get_table_columns(pg_conn,table_schema,table_name)\n",
    "        globals()[f\"{table_name}_pk\"]=get_table_pk(pg_conn,table_schema,table_name)\n",
    "        globals()[f\"{table_name}_rowcount\"]=estimate_rowcount(pg_conn,f'SELECT * FROM {table_schema}.{table_name}')\n",
    "        globals()[f\"{table_name}_base_query\"]= build_base_query(table_cfg, table_schema, globals()[f\"{table_name}_columns\"])\n",
    "        globals()[f\"{table_name}_partition_spec\"]=table_cfg.get(\"partition\")\n",
    "\n",
    "        if globals()[f\"{table_name}_partition_spec\"]!=None:\n",
    "            sqls=apply_partition_clause(globals()[f\"{table_name}_base_query\"],globals()[f\"{table_name}_partition_spec\"])\n",
    "\n",
    "            for sql in sqls:\n",
    "                chunk=plan_file_splits(globals()[f\"{table_name}_rowcount\"], max_rows_per_file)\n",
    "                chunk_sql_query=build_chunk_query(sql,table_cfg['order_by'],chunk[0])\n",
    "                postgres_query_to_snowflake_table( pg_creds,sf_creds,data_dir,chunk_sql_query,\n",
    "                                              f\"@{sf_creds.database}.{sf_creds.schema}.{sf_creds.sf_landing_stage}\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.CSV_FMT\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.{table_name}_RAW\",False)\n",
    "\n",
    "        else:\n",
    "            chunk=plan_file_splits(globals()[f\"{table_name}_rowcount\"], max_rows_per_file)\n",
    "            chunk_sql_query=build_chunk_query(globals()[f\"{table_name}_base_query\"],table_cfg['order_by'],chunk[0])\n",
    "            postgres_query_to_snowflake_table( pg_creds,sf_creds,data_dir,chunk_sql_query,\n",
    "                                              f\"@{sf_creds.database}.{sf_creds.schema}.{sf_creds.sf_landing_stage}\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.CSV_FMT\",\n",
    "                                              f\"{sf_creds.database}.{sf_creds.schema}.{table_name}_RAW\")        \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9ef7d-2947-4f7f-94ea-5ad04f23bf06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b0715-7f8c-4d0c-b4f3-b9dd3bdf09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adada90a-9aa8-414d-b250-04313ad6f334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
