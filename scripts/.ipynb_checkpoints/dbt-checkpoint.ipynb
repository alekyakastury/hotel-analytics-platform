{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96cc17ea-b897-4b1e-8a19-0d56f8e8ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "import copy\n",
    "import gzip\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import psycopg2\n",
    "import psycopg2.extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e68c0e66-951a-4d46-b987-50539268268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent   # notebook's parent directory\n",
    "config_path = BASE_DIR / \"config\" / \"postgres_to_csv_export.yaml\"\n",
    "\n",
    "\n",
    "os.environ[\"SNOWFLAKE_PASSWORD\"] = \"JimmyPage@1234\"\n",
    "os.environ[\"POSTGRES_PASSWORD\"] = \"postgres\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c69aa-f622-45e0-91ce-43bfaffd5732",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb865b4-6875-49ce-a70a-f3e1ce968c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            return config or {}\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML in config file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d69a545-e35c-4647-9a78-b23ad822d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config(cfg: Dict[str, Any]) -> None:\n",
    "    # Top-level keys\n",
    "    required_top_keys = {\"version\", \"source\", \"export\", \"tables\"}\n",
    "    missing = required_top_keys - cfg.keys()\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing top-level keys: {', '.join(sorted(missing))}\")\n",
    "\n",
    "    # Validate source\n",
    "    if not isinstance(cfg[\"source\"], dict) or not cfg[\"source\"]:\n",
    "        raise ValueError(\"'source' must be a non-empty dictionary\")\n",
    "\n",
    "    for source_name, conn in cfg[\"source\"].items():\n",
    "        if not isinstance(conn, dict):\n",
    "            raise TypeError(f\"Source '{source_name}' must be a dictionary\")\n",
    "        for key in (\"host\", \"port\", \"database\", \"user\", \"password_env\", \"schema\"):\n",
    "            if key not in conn:\n",
    "                raise KeyError(f\"Missing '{key}' in source '{source_name}'\")\n",
    "\n",
    "    # Validate export\n",
    "    export = cfg[\"export\"]\n",
    "    if not isinstance(export, dict):\n",
    "        raise TypeError(\"'export' must be a dictionary\")\n",
    "    for key in (\"output_dir\", \"format\"):\n",
    "        if key not in export:\n",
    "            raise KeyError(f\"Missing '{key}' in 'export'\")\n",
    "\n",
    "    # Validate tables\n",
    "    tables = cfg[\"tables\"]\n",
    "    if not isinstance(tables, list) or not tables:\n",
    "        raise ValueError(\"'tables' must be a non-empty list\")\n",
    "    for i, table in enumerate(tables):\n",
    "        if not isinstance(table, dict):\n",
    "            raise TypeError(f\"Table at index {i} must be a dictionary\")\n",
    "        if \"name\" not in table or \"mode\" not in table or \"order_by\" not in table:\n",
    "            raise KeyError(f\"Table {table.get('name', i)} missing required keys\")\n",
    "        if table[\"mode\"] == \"table\" and \"table\" not in table:\n",
    "            raise KeyError(f\"Table {table['name']} missing 'table' key for mode 'table'\")\n",
    "        if table[\"mode\"] == \"query\" and \"query\" not in table:\n",
    "            raise KeyError(f\"Table {table['name']} missing 'query' key for mode 'query'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19203e2-2afc-4cb4-aa4c-500a22a5cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_yaml_file(path: str) -> Dict[str, Any]:\n",
    "    # Load YAML\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"YAML file not found: {path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML syntax: {e}\")\n",
    "\n",
    "    if not isinstance(cfg, dict):\n",
    "        raise ValueError(\"YAML root must be a dictionary\")\n",
    "\n",
    "    validate_config(cfg)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a8af9c-ec39-4c10-a187-8e5dd4794c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d17ad61-3978-4cb5-82d1-c1268da72769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_secrets(cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Return a copy of the config with secrets resolved.\n",
    "    \n",
    "    Replaces any key ending with '_env' with the actual environment variable value,\n",
    "    and renames the key by removing '_env' (e.g., 'password_env' -> 'password').\n",
    "\n",
    "    Raises KeyError if the environment variable is not set.\n",
    "    \"\"\"\n",
    "\n",
    "    def _resolve(obj: Any) -> Any:\n",
    "        if isinstance(obj, dict):\n",
    "            resolved = {}\n",
    "            for k, v in obj.items():\n",
    "                if k.endswith(\"_env\") and isinstance(v, str):\n",
    "                    env_var = v\n",
    "                    secret_value = os.getenv(env_var)\n",
    "                    if secret_value is None:\n",
    "                        raise KeyError(f\"Environment variable '{env_var}' not set\")\n",
    "                    new_key = k[:-4]  # remove '_env'\n",
    "                    resolved[new_key] = secret_value\n",
    "                else:\n",
    "                    resolved[k] = _resolve(v)  # recurse for nested dicts/lists\n",
    "            return resolved\n",
    "        elif isinstance(obj, list):\n",
    "            return [_resolve(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    return _resolve(copy.deepcopy(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e41290-858c-45e7-a7ff-dc1bfe7d7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extensions import connection as Psycopg2Connection\n",
    "from typing import Dict\n",
    "\n",
    "def create_pg_connection(conn_cfg: Dict[str, str]) -> Psycopg2Connection:\n",
    "    \"\"\"\n",
    "    Create a PostgreSQL connection using psycopg2.\n",
    "\n",
    "    conn_cfg must include:\n",
    "        host, port, database, user, password, schema (optional)\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=conn_cfg[\"host\"],\n",
    "        port=conn_cfg.get(\"port\", 5432),\n",
    "        dbname=conn_cfg[\"database\"],\n",
    "        user=conn_cfg[\"user\"],\n",
    "        password=conn_cfg[\"password\"]\n",
    "    )\n",
    "\n",
    "    # Optionally set search_path if schema is provided\n",
    "    if \"schema\" in conn_cfg and conn_cfg[\"schema\"]:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"SET search_path TO {conn_cfg['schema']};\")\n",
    "            conn.commit()\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7fe435-042b-4d38-abfd-bd34b794a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_pwd=resolve_secrets(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9df4454-84af-42a2-b088-a463e0ee8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn=create_pg_connection(cfg_pwd[\"source\"]['postgres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf21e4e-9b9f-424f-881a-af6738af64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import psycopg2\n",
    "\n",
    "def get_table_columns(conn: psycopg2.extensions.connection, schema: str, table: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch column names for a given schema.table in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        schema: schema name (e.g., 'public')\n",
    "        table: table name (e.g., 'hotel')\n",
    "\n",
    "    Returns:\n",
    "        List of column names in order.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = %s AND table_name = %s\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (schema, table))\n",
    "        columns = [row[0] for row in cur.fetchall()]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aeeb964-6cf5-4aa3-9458-a31e64605775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_pk(conn: psycopg2.extensions.connection, schema: str, table: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Fetch the primary key column names for a given schema.table in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        schema: schema name (e.g., 'public')\n",
    "        table: table name (e.g., 'hotel')\n",
    "\n",
    "    Returns:\n",
    "        List of primary key column names in order, or None if table has no PK.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT kcu.column_name\n",
    "    FROM information_schema.table_constraints tc\n",
    "    JOIN information_schema.key_column_usage kcu\n",
    "      ON tc.constraint_name = kcu.constraint_name\n",
    "      AND tc.table_schema = kcu.table_schema\n",
    "    WHERE tc.table_schema = %s\n",
    "      AND tc.table_name = %s\n",
    "      AND tc.constraint_type = 'PRIMARY KEY'\n",
    "    ORDER BY kcu.ordinal_position;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (schema, table))\n",
    "        pk_columns = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    return pk_columns if pk_columns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e42b445d-5a1c-4660-a946-bc269eed5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from typing import Optional\n",
    "\n",
    "def estimate_rowcount(conn: psycopg2.extensions.connection, sql: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the number of rows a SQL query will return.\n",
    "    Uses COUNT(*) wrapped around the query.\n",
    "\n",
    "    Args:\n",
    "        conn: psycopg2 connection\n",
    "        sql: SQL query (string)\n",
    "\n",
    "    Returns:\n",
    "        Estimated row count (int)\n",
    "    \"\"\"\n",
    "    # Wrap the original query as a subquery\n",
    "    count_sql = f\"SELECT COUNT(*) FROM ({sql}) AS subquery\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(count_sql)\n",
    "            rowcount = cur.fetchone()[0]\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    return rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3c1efc-3b32-46ac-bffc-39e1d1aab02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def build_base_query(table_cfg: Dict, schema_default: str, columns: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Build the base SQL query for a table export.\n",
    "\n",
    "    Args:\n",
    "        table_cfg: dictionary containing table config from YAML\n",
    "                   Must include 'mode' and either 'table' or 'query'\n",
    "        schema_default: default schema to use if table_cfg does not specify one\n",
    "        columns: optional list of columns to select (for mode='table')\n",
    "\n",
    "    Returns:\n",
    "        SQL string (no trailing semicolon)\n",
    "    \"\"\"\n",
    "    mode = table_cfg.get(\"mode\")\n",
    "    \n",
    "    if mode == \"table\":\n",
    "        table_name = table_cfg.get(\"table\")\n",
    "        if not table_name:\n",
    "            raise KeyError(f\"Table config '{table_cfg.get('name')}' missing 'table' key for mode='table'\")\n",
    "        \n",
    "        schema = table_cfg.get(\"schema\", schema_default)\n",
    "        \n",
    "        # Use all columns if not specified\n",
    "        cols_sql = \", \".join(columns) if columns else \"*\"\n",
    "        \n",
    "        sql = f\"SELECT {cols_sql} FROM {schema}.{table_name}\"\n",
    "        return sql\n",
    "\n",
    "    elif mode == \"query\":\n",
    "        query = table_cfg.get(\"query\")\n",
    "        if not query:\n",
    "            raise KeyError(f\"Table config '{table_cfg.get('name')}' missing 'query' key for mode='query'\")\n",
    "        return query.rstrip().rstrip(\";\")  # strip trailing semicolon\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode '{mode}' for table '{table_cfg.get('name')}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a9e36f0-4e23-44a9-9b32-a817c3e5463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=get_table_columns(conn,'public','hotel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be68a48e-3e34-4dbd-8d77-d5ee2d9f7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_query=build_base_query(cfg_pwd['tables'][0], 'public',cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e736236-4785-4276-a25f-f2b8810104d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def apply_filters(base_sql: str, filters_cfg: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Apply filters to a base SQL query by adding a WHERE clause.\n",
    "\n",
    "    Args:\n",
    "        base_sql: the base SQL query (without trailing semicolon)\n",
    "        filters_cfg: dictionary of filters, e.g.\n",
    "                     {\"checkin_date__gte\": \"2025-01-01\",\n",
    "                      \"checkin_date__lt\": \"2026-01-01\",\n",
    "                      \"status\": \"confirmed\"}\n",
    "\n",
    "    Returns:\n",
    "        SQL string with WHERE clauses applied.\n",
    "    \"\"\"\n",
    "    if not filters_cfg:\n",
    "        return base_sql.rstrip().rstrip(\";\")  # no filters\n",
    "\n",
    "    conditions: List[str] = []\n",
    "\n",
    "    for key, value in filters_cfg.items():\n",
    "        if key.endswith(\"__gte\"):\n",
    "            col = key[:-5]\n",
    "            conditions.append(f\"{col} >= '{value}'\")\n",
    "        elif key.endswith(\"__lte\"):\n",
    "            col = key[:-5]\n",
    "            conditions.append(f\"{col} <= '{value}'\")\n",
    "        elif key.endswith(\"__gt\"):\n",
    "            col = key[:-4]\n",
    "            conditions.append(f\"{col} > '{value}'\")\n",
    "        elif key.endswith(\"__lt\"):\n",
    "            col = key[:-4]\n",
    "            conditions.append(f\"{col} < '{value}'\")\n",
    "        else:\n",
    "            # exact match\n",
    "            conditions.append(f\"{key} = '{value}'\")\n",
    "\n",
    "    where_clause = \" AND \".join(conditions)\n",
    "\n",
    "    # Determine if base_sql already has WHERE\n",
    "    if \" where \" in base_sql.lower():\n",
    "        return f\"{base_sql} AND {where_clause}\"\n",
    "    else:\n",
    "        return f\"{base_sql} WHERE {where_clause}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b01ca3eb-4827-421b-be96-2cffa8ff9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_cfg={\"created_at__gte\": \"2026-01-01\",\n",
    "                      \"created_at__lt\": \"2026-02-01\"}\n",
    "base_query_filters=apply_filters(base_query, filters_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c522db-7e2a-4dbe-a430-935b19524eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def apply_partition_clause(base_sql: str, partition_spec: Dict, partition_value: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply a partition filter to a base SQL query.\n",
    "\n",
    "    Args:\n",
    "        base_sql: base SQL string (no trailing semicolon)\n",
    "        partition_spec: dictionary describing the partition, e.g.\n",
    "                        {\n",
    "                            \"type\": \"date_range\",\n",
    "                            \"column\": \"checkin_date\",\n",
    "                            \"granularity\": \"month\"\n",
    "                        }\n",
    "        partition_value: string representing the partition value, e.g. \"2025-01\" or \"2025-01-15\"\n",
    "\n",
    "    Returns:\n",
    "        SQL string with WHERE clause for this partition applied\n",
    "    \"\"\"\n",
    "    column = partition_spec.get(\"column\")\n",
    "    if not column:\n",
    "        raise KeyError(\"partition_spec must have a 'column' key\")\n",
    "\n",
    "    conditions = []\n",
    "\n",
    "    # Determine partition type\n",
    "    if partition_spec.get(\"type\") == \"date_range\":\n",
    "        granularity = partition_spec.get(\"granularity\", \"day\")\n",
    "\n",
    "        if granularity == \"month\":\n",
    "            # partition_value = \"YYYY-MM\"\n",
    "            start = f\"{partition_value}-01\"\n",
    "            from datetime import datetime, timedelta\n",
    "            # compute next month start for < condition\n",
    "            start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "            # handle year/month increment\n",
    "            if start_date.month == 12:\n",
    "                end_date = start_date.replace(year=start_date.year+1, month=1)\n",
    "            else:\n",
    "                end_date = start_date.replace(month=start_date.month+1)\n",
    "            end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "            conditions.append(f\"{column} >= '{start}'\")\n",
    "            conditions.append(f\"{column} < '{end_str}'\")\n",
    "\n",
    "        elif granularity == \"day\":\n",
    "            # partition_value = \"YYYY-MM-DD\"\n",
    "            from datetime import datetime, timedelta\n",
    "            start_date = datetime.strptime(partition_value, \"%Y-%m-%d\")\n",
    "            end_date = start_date + timedelta(days=1)\n",
    "            conditions.append(f\"{column} >= '{partition_value}'\")\n",
    "            conditions.append(f\"{column} < '{end_date.strftime('%Y-%m-%d')}'\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported granularity '{granularity}'\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported partition type '{partition_spec.get('type')}'\")\n",
    "\n",
    "    partition_clause = \" AND \".join(conditions)\n",
    "\n",
    "    # Determine if base_sql already has WHERE\n",
    "    if \" where \" in base_sql.lower():\n",
    "        return f\"{base_sql} AND {partition_clause}\"\n",
    "    else:\n",
    "        return f\"{base_sql} WHERE {partition_clause}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0911cf70-60da-4530-adbb-06d9b38bde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_spec= {\"type\": \"date_range\",\n",
    "                            \"column\": \"created_at\",\n",
    "                            \"granularity\": \"month\" }\n",
    "partition_value = \"2026-01\"\n",
    "base_query_filters_p=apply_partition_clause(base_query_filters, partition_spec, partition_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f49552-d252-48d7-858b-ddc52c80b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def apply_ordering(base_sql: str, order_by: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Apply ORDER BY clause to the base SQL for deterministic export.\n",
    "\n",
    "    Args:\n",
    "        base_sql: SQL query (string, no trailing semicolon)\n",
    "        order_by: column name(s) to order by, e.g., \"id\" or \"id, name\"\n",
    "\n",
    "    Returns:\n",
    "        SQL string with ORDER BY clause applied.\n",
    "    \"\"\"\n",
    "    if not order_by:\n",
    "        # No ordering specified, return as-is\n",
    "        return base_sql.rstrip().rstrip(\";\")\n",
    "      \n",
    "    # Determine if base_sql already has ORDER BY\n",
    "    if \" order by \" in base_sql.lower():\n",
    "        # Already has ORDER BY, skip or append? Here we leave as-is\n",
    "        return base_sql.rstrip().rstrip(\";\")\n",
    "    \n",
    "    return f\"{base_sql} ORDER BY {order_by}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d02526e5-6e54-4a48-8b8b-a873bd2b7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_by=cfg_pwd[\"tables\"][0]['order_by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de331ee7-5fce-4d2f-bae4-bc8dc51e83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_query=apply_ordering(base_query_filters, order_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc39084c-a7ef-468b-9818-469a71c02883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_partitions(partition_spec: Dict) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Generate partitions based on a partition specification.\n",
    "\n",
    "    Args:\n",
    "        partition_spec: dictionary with keys like:\n",
    "            - type: \"date_range\"\n",
    "            - column: column to partition by (ignored here)\n",
    "            - granularity: \"month\" or \"day\"\n",
    "            - start: \"YYYY-MM-DD\"\n",
    "            - end: \"YYYY-MM-DD\"\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys:\n",
    "            - suffix: string for file naming (e.g., \"2025-01\")\n",
    "            - start_date: partition start date\n",
    "            - end_date: partition end date\n",
    "    \"\"\"\n",
    "    if partition_spec.get(\"type\") != \"date_range\":\n",
    "        raise ValueError(\"Only date_range partitions are supported\")\n",
    "\n",
    "    start_str = partition_spec[\"start\"]\n",
    "    end_str = partition_spec[\"end\"]\n",
    "    granularity = partition_spec.get(\"granularity\", \"month\")\n",
    "\n",
    "    start_date = datetime.strptime(start_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_str, \"%Y-%m-%d\")\n",
    "\n",
    "    partitions = []\n",
    "\n",
    "    current = start_date\n",
    "    while current < end_date:\n",
    "        if granularity == \"month\":\n",
    "            # Compute first day of next month\n",
    "            if current.month == 12:\n",
    "                next_partition = current.replace(year=current.year + 1, month=1)\n",
    "            else:\n",
    "                next_partition = current.replace(month=current.month + 1)\n",
    "            suffix = current.strftime(\"%Y-%m\")\n",
    "        elif granularity == \"day\":\n",
    "            next_partition = current + timedelta(days=1)\n",
    "            suffix = current.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported granularity '{granularity}'\")\n",
    "\n",
    "        # Ensure end_date of partition does not exceed overall end\n",
    "        partition_end = min(next_partition, end_date)\n",
    "\n",
    "        partitions.append({\n",
    "            \"suffix\": suffix,\n",
    "            \"start_date\": current.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": partition_end.strftime(\"%Y-%m-%d\")\n",
    "        })\n",
    "\n",
    "        current = next_partition\n",
    "\n",
    "    return partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af7d1e-ba96-4d0f-bc44-6768d8fc51cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8e9dcc8-1c29-4924-9815-748443af05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def plan_file_splits(rowcount: int, max_rows_per_file: int) -> List[Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Plan file splits for a given rowcount and max rows per file.\n",
    "\n",
    "    Args:\n",
    "        rowcount: total number of rows in the partition\n",
    "        max_rows_per_file: maximum rows per output file\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys:\n",
    "            - start_row: 0-based inclusive start\n",
    "            - end_row: exclusive end\n",
    "    \"\"\"\n",
    "    if rowcount <= 0:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < rowcount:\n",
    "        end = min(start + max_rows_per_file, rowcount)\n",
    "        chunks.append({\"start_row\": start, \"end_row\": end})\n",
    "        start = end\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e716d45a-e807-4242-8455-9779ff879f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcount=estimate_rowcount(conn,master_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99fd2feb-f8e7-4cfc-9cfe-7c0580a2e5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b0a6e94-19d1-4c63-968c-734f450dd050",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk=plan_file_splits(rowcount, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b9ba32e-d2af-4728-8611-1242bd1db396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "def build_chunk_query(\n",
    "    sql: str,\n",
    "    order_by: Optional[str],\n",
    "    chunk: Dict[str, int]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a SQL query for a specific chunk of rows.\n",
    "\n",
    "    Args:\n",
    "        sql: base SQL (should include WHERE/filters/partition/order)\n",
    "        order_by: column(s) used for deterministic ordering\n",
    "        chunk: dict with 'start_row' (inclusive) and 'end_row' (exclusive)\n",
    "\n",
    "    Returns:\n",
    "        SQL string with ORDER BY + OFFSET/LIMIT applied\n",
    "    \"\"\"\n",
    "    if not order_by:\n",
    "        raise ValueError(\"order_by must be specified for chunking\")\n",
    "\n",
    "    start = chunk[\"start_row\"]\n",
    "    limit = chunk[\"end_row\"] - chunk[\"start_row\"]\n",
    "    \n",
    "    # Ensure SQL has ORDER BY\n",
    "    sql_ordered = sql if \" order by \" in sql.lower() else f\"{sql} ORDER BY {order_by}\"\n",
    "\n",
    "    return f\"{sql_ordered} OFFSET {start} LIMIT {limit}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9e84578-cf40-4122-966c-1c78e1da0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_query=build_chunk_query(master_query,order_by,chunk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4f7f471-b46c-4433-9b51-1ae42db28cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "def export_query_to_temp_csv_gz(conn, sql: str, *, delimiter: str = \",\", header: bool = True, null_as: str = \"\") -> Path:\n",
    "    \"\"\"\n",
    "    Writes a query result to a TEMP .csv.gz file using Postgres COPY for speed.\n",
    "    Returns the temp file path.\n",
    "    \"\"\"\n",
    "    sql = sql.strip().rstrip(\";\")\n",
    "\n",
    "    copy_sql = f\"COPY ({sql}) TO STDOUT WITH CSV DELIMITER '{delimiter}'\"\n",
    "    if header:\n",
    "        copy_sql += \" HEADER\"\n",
    "    if null_as != \"\":\n",
    "        copy_sql += f\" NULL '{null_as}'\"\n",
    "\n",
    "    # Temporary file path\n",
    "    tmp_dir = Path(tempfile.mkdtemp(prefix=\"pg_to_sf_\"))\n",
    "    out_path = tmp_dir / \"extract.csv.gz\"\n",
    "\n",
    "    with gzip.open(out_path, \"wt\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.copy_expert(copy_sql, f_out)\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa054b66-86d5-4082-a8d0-070f2eff18d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/ALEKYA~1/AppData/Local/Temp/pg_to_sf_z4lcqyc8/extract.csv.gz')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_query_to_temp_csv_gz(conn,final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0b973b3-fa01-4102-9d3f-5723b4dd712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "713add85-1bf3-4ea5-832a-dc37e6fd7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SnowflakeCreds:\n",
    "    account: str\n",
    "    user: str\n",
    "    role: Optional[str]\n",
    "    warehouse: Optional[str]\n",
    "    database: Optional[str]\n",
    "    schema: Optional[str]\n",
    "    password: Optional[str]  # use password_env in YAML (recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "192b6973-5d03-48ac-be61-fd57080373b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "def load_snowflake_creds_from_yaml(\n",
    "    yaml_path: str | Path,\n",
    "    key_path: str = \"snowflake\"\n",
    ") -> SnowflakeCreds:\n",
    "    \"\"\"\n",
    "    Reads Snowflake credentials from a YAML file and returns a SnowflakeCreds object.\n",
    "    \"\"\"\n",
    "    yaml_path = Path(yaml_path)\n",
    "    cfg = yaml.safe_load(yaml_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    node = cfg\n",
    "    for key in key_path.split(\".\"):\n",
    "        if key not in node:\n",
    "            raise KeyError(f\"Missing config path: {key_path}\")\n",
    "        node = node[key]\n",
    "\n",
    "    def req(k: str) -> str:\n",
    "        v = node.get(k)\n",
    "        if not v or not isinstance(v, str):\n",
    "            raise ValueError(f\"Missing/invalid '{key_path}.{k}' in YAML\")\n",
    "        return v\n",
    "\n",
    "    account = req(\"account\")\n",
    "    user = req(\"user\")\n",
    "\n",
    "    password = None\n",
    "    password_env = node.get(\"password_env\")\n",
    "    if password_env:\n",
    "        password = os.environ.get(password_env)\n",
    "        if not password:\n",
    "            raise ValueError(\n",
    "                f\"Env var '{password_env}' is not set (needed for Snowflake password).\"\n",
    "            )\n",
    "    else:\n",
    "        password = node.get(\"password\")\n",
    "\n",
    "    return SnowflakeCreds(\n",
    "        account=account,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        role=node.get(\"role\"),\n",
    "        warehouse=node.get(\"warehouse\"),\n",
    "        database=node.get(\"database\"),\n",
    "        schema=node.get(\"schema\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f76278b9-2bc1-4b53-af4e-bc097f14e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=load_snowflake_creds_from_yaml(config_path, key_path=\"export.snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a824fccc-f147-405a-84cf-2dce9d5b4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) SnowSQL helpers (PUT + COPY)\n",
    "# -----------------------------\n",
    "def _snowsql_base_cmd(creds: SnowflakeCreds, snowsql_path: str = \"snowsql\") -> list[str]:\n",
    "    cmd = [\n",
    "        snowsql_path,\n",
    "        \"-a\", creds.account,\n",
    "        \"-u\", creds.user,\n",
    "        \"-o\", \"exit_on_error=true\",\n",
    "        \"-o\", \"friendly=false\",\n",
    "        \"-o\", \"quiet=true\",\n",
    "    ]\n",
    "    return cmd\n",
    "\n",
    "\n",
    "def _run_snowsql(creds: SnowflakeCreds, sql: str, *, snowsql_path: str = \"snowsql\", timeout_sec: int = 300) -> str:\n",
    "    env = os.environ.copy()\n",
    "    if creds.password:\n",
    "        env[\"SNOWSQL_PWD\"] = creds.password\n",
    "\n",
    "    cmd = _snowsql_base_cmd(creds, snowsql_path=snowsql_path) + [\"-q\", sql]\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        cmd,\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=timeout_sec,\n",
    "    )\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            \"SnowSQL command failed.\\n\"\n",
    "            f\"Command: {shlex.join(cmd)}\\n\"\n",
    "            f\"STDOUT:\\n{proc.stdout}\\n\"\n",
    "            f\"STDERR:\\n{proc.stderr}\"\n",
    "        )\n",
    "    return proc.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90aecbf8-b1a1-4912-adff-95711c944feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_file_to_stage(\n",
    "    creds: SnowflakeCreds,\n",
    "    local_path: Path,\n",
    "    stage_fqn: str,\n",
    "    *,\n",
    "    overwrite: bool = True,\n",
    "    auto_compress: bool = False,\n",
    "    parallel: int = 4,\n",
    "    snowsql_path: str = \"snowsql\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Uploads a local file into an INTERNAL stage via PUT.\n",
    "    Note: PUT *requires* a local file path (we keep it temp and delete after).\n",
    "    \"\"\"\n",
    "    if not stage_fqn.startswith(\"@\"):\n",
    "        raise ValueError(\"stage_fqn must start with '@' (e.g. @DB.SCHEMA.LANDING_STAGE)\")\n",
    "\n",
    "    session = []\n",
    "    if creds.role:\n",
    "        session.append(f\"USE ROLE {creds.role}\")\n",
    "    if creds.warehouse:\n",
    "        session.append(f\"USE WAREHOUSE {creds.warehouse}\")\n",
    "    if creds.database:\n",
    "        session.append(f\"USE DATABASE {creds.database}\")\n",
    "    if creds.schema:\n",
    "        session.append(f\"USE SCHEMA {creds.schema}\")\n",
    "\n",
    "    put_sql = (\n",
    "        f\"PUT 'file://{local_path.as_posix()}' {stage_fqn} \"\n",
    "        f\"OVERWRITE={'TRUE' if overwrite else 'FALSE'} \"\n",
    "        f\"AUTO_COMPRESS={'TRUE' if auto_compress else 'FALSE'} \"\n",
    "        f\"PARALLEL={int(parallel)}\"\n",
    "    )\n",
    "    full_sql = \"; \".join(session + [put_sql]) + \";\"\n",
    "    return _run_snowsql(creds, full_sql, snowsql_path=snowsql_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52ced254-42ae-45d7-9b41-74021847476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_stage_to_raw_table(\n",
    "    creds: SnowflakeCreds,\n",
    "    stage_fqn: str,\n",
    "    stage_file_name: str,\n",
    "    raw_table_fqn: str,\n",
    "    file_format_fqn: str,\n",
    "    *,\n",
    "    on_error: str = \"ABORT_STATEMENT\",\n",
    "    force: bool = True,\n",
    "    snowsql_path: str = \"snowsql\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Loads a specific staged file into a RAW table using COPY INTO.\n",
    "    stage_file_name is the basename that was PUT (e.g. 'booking_2025_01_part_001.csv.gz').\n",
    "    \"\"\"\n",
    "    session = []\n",
    "    if creds.role:\n",
    "        session.append(f\"USE ROLE {creds.role}\")\n",
    "    if creds.warehouse:\n",
    "        session.append(f\"USE WAREHOUSE {creds.warehouse}\")\n",
    "    if creds.database:\n",
    "        session.append(f\"USE DATABASE {creds.database}\")\n",
    "    if creds.schema:\n",
    "        session.append(f\"USE SCHEMA {creds.schema}\")\n",
    "\n",
    "    copy_sql = f\"\"\"\n",
    "    COPY INTO {raw_table_fqn}\n",
    "    FROM {stage_fqn}/{stage_file_name}\n",
    "    FILE_FORMAT = (FORMAT_NAME = '{file_format_fqn}')\n",
    "    ON_ERROR = {on_error}\n",
    "    FORCE = {'TRUE' if force else 'FALSE'};\n",
    "    \"\"\".strip()\n",
    "\n",
    "    full_sql = \"; \".join(session + [copy_sql])\n",
    "    return _run_snowsql(creds, full_sql, snowsql_path=snowsql_path, timeout_sec=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e143d02-a8de-48b5-b9ac-a1370dbd9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) End-to-end: Postgres -> Stage -> RAW (temp files deleted)\n",
    "# -----------------------------\n",
    "def postgres_query_to_snowflake_raw(\n",
    "    pg_conn,\n",
    "    *,\n",
    "    sql: str,\n",
    "    creds: SnowflakeCreds,\n",
    "    stage_fqn: str,             # e.g. \"@HOTEL_ANALYTICS.RAW.LANDING_STAGE\"\n",
    "    raw_table_fqn: str,         # e.g. \"HOTEL_ANALYTICS.RAW.BOOKING\"\n",
    "    file_format_fqn: str,       # e.g. \"HOTEL_ANALYTICS.RAW.CSV_FMT\"\n",
    "    target_filename: str,       # e.g. \"booking_2025_01_part_001.csv.gz\"\n",
    "    delimiter: str = \",\",\n",
    "    header: bool = True,\n",
    "    null_as: str = \"\",\n",
    "    snowsql_path: str = \"snowsql\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    1) Export query to TEMP csv.gz\n",
    "    2) PUT temp file to stage as target_filename\n",
    "    3) COPY INTO RAW table\n",
    "    4) Delete temp file + temp dir\n",
    "    \"\"\"\n",
    "    tmp_file = None\n",
    "    try:\n",
    "        # 1) Export to temp file\n",
    "        tmp_file = export_query_to_temp_csv_gz(pg_conn, sql, delimiter=delimiter, header=header, null_as=null_as)\n",
    "\n",
    "        # Rename to the desired final file name (so stage has predictable names)\n",
    "        final_path = tmp_file.parent / target_filename\n",
    "        tmp_file.rename(final_path)\n",
    "\n",
    "        # 2) PUT to stage\n",
    "        put_file_to_stage(\n",
    "            creds,\n",
    "            local_path=final_path,\n",
    "            stage_fqn=stage_fqn,\n",
    "            snowsql_path=snowsql_path,\n",
    "            overwrite=True,\n",
    "            auto_compress=False,  # file already gz\n",
    "        )\n",
    "\n",
    "        # 3) COPY INTO RAW\n",
    "        copy_stage_to_raw_table(\n",
    "            creds,\n",
    "            stage_fqn=stage_fqn,\n",
    "            stage_file_name=target_filename,\n",
    "            raw_table_fqn=raw_table_fqn,\n",
    "            file_format_fqn=file_format_fqn,\n",
    "            snowsql_path=snowsql_path,\n",
    "        )\n",
    "\n",
    "    finally:\n",
    "        # 4) Cleanup temp files (your “no local storage” requirement)\n",
    "        if tmp_file is not None:\n",
    "            tmp_dir = tmp_file.parent\n",
    "            # delete all files we created in that temp dir\n",
    "            for p in tmp_dir.glob(\"*\"):\n",
    "                try:\n",
    "                    p.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            try:\n",
    "                tmp_dir.rmdir()\n",
    "            except Exception:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc125bd5-b980-4f14-b0fe-886f93bdc0f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SnowSQL command failed.\nCommand: snowsql -a BAJLLOB-CN38560 -u ALEKYAKASTURY -o exit_on_error=true -o friendly=false -o quiet=true -q 'USE ROLE ACCOUNTADMIN; USE DATABASE HOTEL_ANALYTICS; USE SCHEMA RAW; COPY INTO HOTEL_ANALYTICS.RAW.HOTEL\n    FROM @HOTEL_ANALYTICS.RAW.LANDING_STAGE/extract.csv.gz\n    FILE_FORMAT = (FORMAT_NAME = '\"'\"'HOTEL_ANALYTICS.RAW.CSV_FMT'\"'\"')\n    ON_ERROR = ABORT_STATEMENT\n    FORCE = TRUE;'\nSTDOUT:\n\nSTDERR:\n001757 (42601): SQL compilation error:\nTable 'HOTEL_ANALYTICS.RAW.HOTEL' does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m postgres_query_to_snowflake_raw(\n\u001b[0;32m      2\u001b[0m     conn,\n\u001b[0;32m      3\u001b[0m     sql\u001b[38;5;241m=\u001b[39mfinal_query,\n\u001b[0;32m      4\u001b[0m     creds\u001b[38;5;241m=\u001b[39mcreds,\n\u001b[0;32m      5\u001b[0m     stage_fqn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@HOTEL_ANALYTICS.RAW.LANDING_STAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     raw_table_fqn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHOTEL_ANALYTICS.RAW.HOTEL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     file_format_fqn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHOTEL_ANALYTICS.RAW.CSV_FMT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     target_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextract.csv.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     snowsql_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnowsql\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# or r\"C:\\path\\to\\snowsql.exe\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n",
      "Cell \u001b[1;32mIn[40], line 44\u001b[0m, in \u001b[0;36mpostgres_query_to_snowflake_raw\u001b[1;34m(pg_conn, sql, creds, stage_fqn, raw_table_fqn, file_format_fqn, target_filename, delimiter, header, null_as, snowsql_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m     put_file_to_stage(\n\u001b[0;32m     35\u001b[0m         creds,\n\u001b[0;32m     36\u001b[0m         local_path\u001b[38;5;241m=\u001b[39mfinal_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m         auto_compress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# file already gz\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# 3) COPY INTO RAW\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     copy_stage_to_raw_table(\n\u001b[0;32m     45\u001b[0m         creds,\n\u001b[0;32m     46\u001b[0m         stage_fqn\u001b[38;5;241m=\u001b[39mstage_fqn,\n\u001b[0;32m     47\u001b[0m         stage_file_name\u001b[38;5;241m=\u001b[39mtarget_filename,\n\u001b[0;32m     48\u001b[0m         raw_table_fqn\u001b[38;5;241m=\u001b[39mraw_table_fqn,\n\u001b[0;32m     49\u001b[0m         file_format_fqn\u001b[38;5;241m=\u001b[39mfile_format_fqn,\n\u001b[0;32m     50\u001b[0m         snowsql_path\u001b[38;5;241m=\u001b[39msnowsql_path,\n\u001b[0;32m     51\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# 4) Cleanup temp files (your “no local storage” requirement)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[39], line 35\u001b[0m, in \u001b[0;36mcopy_stage_to_raw_table\u001b[1;34m(creds, stage_fqn, stage_file_name, raw_table_fqn, file_format_fqn, on_error, force, snowsql_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m copy_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124mCOPY INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_table_fqn\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124mFROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_fqn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_file_name\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124mFORCE = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRUE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mforce\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFALSE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     34\u001b[0m full_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(session \u001b[38;5;241m+\u001b[39m [copy_sql])\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _run_snowsql(creds, full_sql, snowsql_path\u001b[38;5;241m=\u001b[39msnowsql_path, timeout_sec\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 31\u001b[0m, in \u001b[0;36m_run_snowsql\u001b[1;34m(creds, sql, snowsql_path, timeout_sec)\u001b[0m\n\u001b[0;32m     23\u001b[0m proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m     24\u001b[0m     cmd,\n\u001b[0;32m     25\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_sec,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnowSQL command failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommand: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshlex\u001b[38;5;241m.\u001b[39mjoin(cmd)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTDOUT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mproc\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTDERR:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mproc\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m     )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout\n",
      "\u001b[1;31mRuntimeError\u001b[0m: SnowSQL command failed.\nCommand: snowsql -a BAJLLOB-CN38560 -u ALEKYAKASTURY -o exit_on_error=true -o friendly=false -o quiet=true -q 'USE ROLE ACCOUNTADMIN; USE DATABASE HOTEL_ANALYTICS; USE SCHEMA RAW; COPY INTO HOTEL_ANALYTICS.RAW.HOTEL\n    FROM @HOTEL_ANALYTICS.RAW.LANDING_STAGE/extract.csv.gz\n    FILE_FORMAT = (FORMAT_NAME = '\"'\"'HOTEL_ANALYTICS.RAW.CSV_FMT'\"'\"')\n    ON_ERROR = ABORT_STATEMENT\n    FORCE = TRUE;'\nSTDOUT:\n\nSTDERR:\n001757 (42601): SQL compilation error:\nTable 'HOTEL_ANALYTICS.RAW.HOTEL' does not exist\n"
     ]
    }
   ],
   "source": [
    "    postgres_query_to_snowflake_raw(\n",
    "        conn,\n",
    "        sql=final_query,\n",
    "        creds=creds,\n",
    "        stage_fqn=\"@HOTEL_ANALYTICS.RAW.LANDING_STAGE\",\n",
    "        raw_table_fqn=\"HOTEL_ANALYTICS.RAW.HOTEL\",\n",
    "        file_format_fqn=\"HOTEL_ANALYTICS.RAW.CSV_FMT\",\n",
    "        target_filename=\"extract.csv.gz\",\n",
    "        snowsql_path=\"snowsql\",  # or r\"C:\\path\\to\\snowsql.exe\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2803d9c-984a-4cfa-a329-f28ec1125573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af0ba8-0832-4480-8f3b-a47fc798417f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
